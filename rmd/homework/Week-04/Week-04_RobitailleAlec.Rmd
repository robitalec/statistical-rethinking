---
author: "Alec L. Robitaille"
output: github_document
editor_options: 
  chunk_output_type: console
---

# Homework: Week 4 
2021-08-30 [updated: `r Sys.Date()`]

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rethinking)
options(digits = 2, scipen = 999)
```

### Setup

```{r}
# Packages
library(ggdag)
library(dagitty)
library(data.table)
library(ggplot2)
library(tidybayes)
```



## Question 1
> Consider three fictional Polynesian islands. On each there is a Royal
Ornithologist charged by the king with surveying the birb population. They have
each found the following proportions of 5 important birb species:

```{r}
# Data
birds <- matrix(
	c(0.2, 0.2, 0.2, 0.2, 0.2,
		0.8, 0.1, 0.05, 0.025, 0.025,
		0.05, 0.15, 0.7, 0.05, 0.05),
	nrow = 3, ncol = 5, byrow = TRUE
)
dimnames(birds) <- list(as.character(1:3), LETTERS[1:5])
birds
```

> First, compute the entropy of each island’s birb distribution. Interpret these
entropy values

```{r}
DT <- melt(data.table(birds, keep.rownames = 'island'), id.vars = 'island',
					 variable.name = 'id', value.name = 'proportion')

# Entropy
entropy <- function(p) -sum(p * log(p))
DT[, .(entropy = entropy(proportion)), by = island]
```

The information entropy describes the uncertainty in a distribution of 
probabilities given the average log-probability of an event (from Statistical
Rethinking 7.2). Island 1 has the highest entropy, with the flat probability of 0.2 across 5 bird species. Island 2 has the lowest entropy, including species A with the highest overall proportion 0.8.


> Second, use each island’s birb distribution to predict the other two. This
means to compute the K-L Divergence of each island from the others, treating
each island as if it were a statistical model of the other islands. You should
end up with 6 different K-L Divergence values. Which island predicts the others
best? Why?


```{r}
divergence <- function(p, q) sum(p * (log(p) - log(q)))
z <- CJ(p = DT$island, q = DT$island, unique = TRUE)[, row_id := .I]
z[, div := divergence(DT[island == p, proportion], 
											DT[island == q, proportion]),
	by = row_id]

z[p != q]
```


`divergence(p, q)` = "Average difference in log probability between the target (p) and the model (q)". 


Model 1 predicts target 3 best (lowest divergence at 0.63) and target 2 best (lowest divergence at 0.87) because it has the highest entropy. Model 3 predicts target 1 best (lowest divergence at 0.64) because it has higher entropy than model 2. 


## Question 2 

> Recall the marriage, age, and happiness collider bias example from Chapter
6. Run models m6.9 and m6.10 again. Compare these two models using WAIC (or LOO,
they will produce identical results). Which model is expected to make better
predictions? Which model provides the correct causal inference about the
influence of age on happiness? Can you explain why the answers to these two
questions disagree?

### Data
```{r}
library(rethinking)
d <- sim_happiness(seed = 1977, N_years = 1e3)

d2 <- d[d$age > 17,]
d2$A <- (d2$age - 18) / (65 - 18)
d2$mid <- d2$married + 1

precis(d2)
```

### Models

```{r}
m6.9 <- quap(
	alist(
		happiness ~ dnorm(mu, sigma),
		mu <- a[mid] + bA * A,
		a[mid] ~ dnorm(0, 1),
		bA ~ dnorm(0, 2),
		sigma ~ dexp(1)
	), data = d2
)

precis(m6.9, depth = 2)
```
