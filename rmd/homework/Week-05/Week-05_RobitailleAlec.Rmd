---
author: "Alec L. Robitaille"
output: github_document
editor_options: 
  chunk_output_type: console
---

# Homework: Week 5
2021-09-03 [updated: `r Sys.Date()`]

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rethinking)
options(digits = 2, scipen = 999)
```

### Setup

```{r}
# Packages
library(ggdag)
library(dagitty)
library(data.table)
library(ggplot2)
library(tidybayes)

# Functions
dag_plot <- function(dag) {
	stat <- node_status(dag, FALSE)
	stat$data$status[is.na(stat$data$status)] <- 'intermediate'
	ggplot(stat, aes(x = x, y = y, xend = xend, yend = yend)) +
	  geom_dag_point(aes(color = status), alpha = 0.5, size = 15) +
	  geom_dag_edges() +
		labs(color = '') + 
	  geom_dag_text(color = 'black') +
		scale_color_manual(values = list('exposure' = '#35608DFF',
																		 'outcome' = '#22A884FF',
																		 'intermediate' = 'grey50')) + 
	  theme_void()
}
```



## Question 1

> Consider the `data(Wines2012)` data table. These data are expert ratings of 20
different French and American wines by 9 different French and American judges.




### Data
> Your goal is to model score, the subjective rating assigned by each judge to
each wine. I recommend standardizing it. In this first problem, consider only
variation among judges and wines. Construct index variables of judge and wine
and then use these index variables to construct a linear regression model.

```{r}
library(rethinking)
library(cmdstanr)
library(data.table)

data("Wines2012")

DT <- data.table(Wines2012)

DT[, scale_score := scale(score)]
DT[, index_judge := .GRP, judge]
DT[, index_wine := .GRP, wine]
```

### Prior predictive simulation
```{r, eval = FALSE}
n <- 100

DT <- data.table(
	index_judge_low = -2,
	index_judge_high = 2,
	index_wine_low = -2,
	index_wine_high = 2
)[rep(seq.int(.N), each = n / .N)]
DT[, alpha := rnorm(.N, 0, 0.2)]
DT[, beta_judge := rnorm(.N, 0, 0.5)]
DT[, beta_wine := rnorm(.N, 0, 0.5)]
DT[, sigma := rexp(.N, 1)]

m <- melt(DT, measure.vars = c('index_judge_low', 'index_judge_high',
													'index_wine_low', 'index_wine_high'))


DT[, mu_for_judge_low := alpha + beta_judge * index_judge + beta_wine * index_wine]

mod <- m$sample(list(index_judge = as.integer(c(-2, 2)), index_wine = as.integer(c(-2, 2))))
posterior::as_draws_df(mod$draws()) -> mdrw


# TODO: this doesnt make sense - an index doesnt have this range
DT <- data.table(
	index_judge_low = -2,
	index_judge_high = 2,
	index_wine_low = -2,
	index_wine_high = 2
)[rep(seq.int(.N), each = n / .N)]
#[, combination_id := .I][rep(seq.int(.N), each = n / .N)]

# index_judge <- c(-2, 2)
DT[, alpha := rnorm(.N, 0, 0.2)]
DT[, beta_judge := rnorm(.N, 0, 0.5)]
DT[, beta_wine := rnorm(.N, 0, 0.5)]
DT[, sigma := rexp(.N, 1)]
DT[, mu := alpha + beta_judge * index_judge + beta_wine * index_wine]


# TODO how to auto flex
ggplot(DT) + 
	geom_segment(aes(
		x = index_judge_low,
		xend = index_judge_high,
		y = alpha + beta_judge * index_judge_low + beta_wine * index_wine_low,
		yend = alpha + beta_judge * index_judge_high + beta_wine * index_wine_low
	))

ggplot(DT) + 
	geom_segment(aes(
		x = index_wine_low,
		xend = index_wine_high,
		y = alpha + beta_judge * index_judge_low + beta_wine * index_wine_low,
		yend = alpha + beta_judge * index_judge_low + beta_wine * index_wine_high
	))

mu_function <- function(alpha, beta_judge, beta_wine, sigma) {
	alpha + beta_judge * index_judge + beta_wine * index_wine
}

DT[, mu := quote()]

DT[, rep(seq.int(.N), each = n / .N)]
library(ggplot2)

ggplot(DT) + 
	geom_point(aes(index_judge, mu)) +
	facet_wrap(~index_wine)

ggplot(DT) + 
	geom_point(aes(index_judge, mu)) +
	facet_wrap(~index_wine)


ggplot() + 
	xlim(c(-2, 2)) +
	geom_function(fun = function(x) ~  alpha + beta_judge * x + beta_wine * index_wine)

beta_judge <- rnorm(n, 0, 0.5)
beta_wine <- rnorm(n, 0, 0.5)
sigma <- rexp(n, 1)
mu <- alpha + beta_judge * index_judge + beta_wine * index_wine
df <- data.frame(x = rep(), y = mu)
plot()
scale_score <- dnorm(mu, sigma)
```

### Model
```{r, eval = FALSE}
data_m1 <- as.list(DT[, .(scale_score, index_judge, index_wine)])
m1 <- ulam(
	alist(
		scale_score ~ dnorm(mu, sigma),
		mu <- alpha + beta_judge * index_judge + beta_wine * index_wine,
		alpha ~ dnorm(0, 0.2),
		beta_judge ~ dnorm(0, 0.5),
		beta_wine ~ dnorm(0, 0.5),
		sigma ~ dexp(1)
	),
	data = data_m1,
	chains = 1
)
writeLines(m1@model)

m1_stan <- c(	'
data{
    vector[180] scale_score;
    int index_wine[180];
    int index_judge[180];
}
parameters{
    real alpha;
    real beta_judge;
    real beta_wine;
    real<lower=0> sigma;
}
model{
    vector[180] mu;
    sigma ~ exponential( 1 );
    beta_wine ~ normal( 0 , 0.5 );
    beta_judge ~ normal( 0 , 0.5 );
    alpha ~ normal( 0 , 0.2 );
    for ( i in 1:180 ) {
        mu[i] = alpha + beta_judge * index_judge[i] + beta_wine * index_wine[i];
    }
    scale_score ~ normal( mu , sigma );
}'
)

m1_stan_file <- write_stan_file(m1_stan)
m1_stan_model <- cmdstan_model(m1_stan_file)
m1_stan_model_sample <- m1_stan_model$sample(data_m1, chains = 1)



```



Justify your priors. You should end up with 9 judge parameters and 20 wine
parameters. Use ulam instead of quap to build this model, and be sure to check
the chains for convergence. If youâ€™d rather build the model directly in Stan or
PyMC3, go ahead. I just want you to use Hamiltonian Monte Carlo instead of
quadratic approximation. 

How do you interpret the variation among individual
judges and individual wines? Do you notice any patterns, just by plotting the
differences? Which judges gave the highest/lowest ratings? Which wines were
rated worst/ best on average?
