[["index.html", "Learning bayesian data analysis with Statistical Rethinking 1 Overview 1.1 Links", " Learning bayesian data analysis with Statistical Rethinking Alec L. Robitaille 2021-08-18 [updated: 2021-08-26] 1 Overview 1.1 Links Link: https://github.com/rmcelreath/statrethinking_winter2019 1.1.1 Adaptations Stan+R: https://vincentarelbundock.github.io/rethinking2/ tidy+rethinking: https://david-salazar.github.io/2020/04/19/statistical-rethinking-week-1/ brms+tidy: https://bookdown.org/content/4857/ "],["homework-week-1.html", "2 Homework: Week 1 2.1 Variables 2.2 Joint model 2.3 Question 1 2.4 Question 2 2.5 Question 3", " 2 Homework: Week 1 2021-08-18 [updated: 2021-08-26] 2.1 Variables N: fixed by experimenter p: Prior probability W: A probability distribution given the data. 2.2 Joint model W ~ Binomial(N , p) p ~ Uniform(0, 1) W is distributed binomially with N observations and probability p on each p is distributed uniformally at 1 2.3 Question 1 Suppose the globe tossing data (Chapter 2) had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as in the book. # Size of grid for grid approximation gridsize &lt;- 1000 # Prior grid prior_grid &lt;- seq(0, 1, length.out = gridsize) # Prior probability (all 1) prior_prob &lt;- rep(1, gridsize) # Data probability # given 4/15, using binomial distribution data_prob &lt;- dbinom(8, 15, prob = prior_grid) # Calculate the posterior numerator by multiplying prior and data probability posterior_num &lt;- prior_prob * data_prob # Standardize by sum of posterior numerator posterior &lt;- posterior_num / sum(posterior_num) # Save for later posterior_1 &lt;- posterior plot(posterior, type = &#39;l&#39;) # Sample from posterior samples &lt;- sample(prior_grid, size = gridsize, prob = posterior, replace = TRUE) mean(samples) ## [1] 0.53 PI(samples, .99) ## 1% 100% ## 0.23 0.82 2.4 Question 2 Start over in 1, but now use a prior that is zero below p = 0.5 and a constant above p = 0.5. This corresponds to prior information that a majority of the Earth’s surface is water. What difference does the better prior make? # Size of grid for grid approximation gridsize &lt;- 1000 # Prior grid prior_grid &lt;- seq(0, 1, length.out = gridsize) # Prior probability (all 1 above 0.5, all 0 below) prior_prob &lt;- c(rep(0, gridsize / 2), rep(1, gridsize / 2)) # Data probability # given 4/15, using binomial distribution data_prob &lt;- dbinom(4, 15, prob = prior_grid) # Calculate the posterior numerator by multiplying prior and data probability posterior_num &lt;- prior_prob * data_prob # Standardize by sum of posterior numerator posterior &lt;- posterior_num / sum(posterior_num) # Save for later posterior_2 &lt;- posterior plot(posterior, type = &#39;l&#39;) # Sample from posterior samples &lt;- sample(prior_grid, size = gridsize, prob = posterior, replace = TRUE) mean(samples) ## [1] 0.55 PI(samples, .99) ## 1% 100% ## 0.50 0.72 Narrower curve, higher max, all zeroes before 0.5 2.5 Question 3 For the posterior distribution from 2, compute 89% percentile and HPDI intervals. Compare the widths of these intervals. Which is wider? Why? If you had only the information in the interval, what might you misunderstand about the shape of the posterior distribution? library(rethinking) library(ggplot2) library(data.table) # Calculate Percentile Interval at 89% percent_interval &lt;- PI(posterior, prob = 0.89) percent_interval ## 5% 94% ## 0.0000 0.0073 # Calculate Highest Posterior Density at 89% highest_post_dens &lt;- HPDI(posterior, prob = 0.89) highest_post_dens ## |0.89 0.89| ## 0.0000 0.0025 "],["homework-week-2.html", "3 Homework: Week 2 3.1 Question 1 3.2 Question 2 3.3 Homework: Question 3", " 3 Homework: Week 2 2021-08-24 [updated: 2021-08-26] 3.1 Question 1 The weights listed below were recorded in the !Kung census, but heights were not recorded for these individuals. Provide predicted heights and 89% compatibility intervals for each of these individuals. That is, fill in the table below, using model-based predictions. Individual, weight, expected height, 89% interval 1, 45,,, 2, 40,,, 3, 65,,, 4, 31,,, 5, 53,,, Model: \\(h_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma)\\) \\(\\mu_{i} = \\alpha + \\beta(x_{i} - \\bar{x})\\) \\(\\alpha \\sim \\text{Normal}(178, 20)\\) \\(\\beta \\sim \\text{Log-Normal}(0, 1)\\) \\(\\sigma \\sim \\text{Uniform}(0, 50)\\) library(rethinking) library(data.table) library(ggplot2) library(tidybayes) theme_set(theme_bw()) data(Howell1) d &lt;- Howell1[Howell1$age &gt;= 18,] m &lt;- quap( alist( height ~ dnorm(mu, sigma), mu &lt;- a + b * (weight - mean(d$weight)), a ~ dnorm(178, 20), b ~ dnorm(0, 1), sigma ~ dunif(0, 50) ), data = d ) precis(m) ## mean sd 5.5% 94.5% ## a 154.6 0.270 154.17 155.03 ## b 0.9 0.042 0.84 0.97 ## sigma 5.1 0.191 4.77 5.38 Simulate: # Set weights to simulate for weights &lt;- data.table(weight = c(45, 40, 65, 31, 54), id = as.character(seq(1, 5))) simmed &lt;- sim(m, list(weight = weights$weight), n = 1e3) # Tidy DT &lt;- melt(as.data.table(simmed), measure.vars = paste0(&#39;V&#39;, 1:5), value.name = &#39;height&#39;, variable.name = &#39;id&#39;) DT[, id := gsub(&#39;V&#39;, &#39;&#39;, id)] DT[weights, weight := weight, on = &#39;id&#39;] # Plot ggplot(DT, aes(height)) + stat_halfeye(.width = .89) + facet_wrap(~id) 3.2 Question 2 Model the relationship between height (cm) and the natural logarithm of weight (log-kg): log(weight). Use the entire Howell1 data frame, all 544 rows, adults and non-adults. Use any model type from Chapter 4 that you think useful: an ordinary linear regression, a polynomial or a spline. Plot the posterior predictions against the raw data library(rethinking) library(data.table) library(ggplot2) library(tidybayes) theme_set(theme_bw()) data(Howell1) d &lt;- Howell1 d$logweight &lt;- log(d$weight) m1 &lt;- quap( alist( height ~ dnorm(mu, sigma), mu &lt;- a + b * (logweight - mean(d$logweight)), a ~ dnorm(178, 20), b ~ dnorm(0, 1), sigma ~ dunif(0, 50) ), data = d ) sim_x &lt;- log(1:60) simmed &lt;- sim(m1, list(logweight = sim_x)) # Tidy DT &lt;- melt(as.data.table(simmed), value.name = &#39;height&#39;, variable.name = &#39;x&#39;) ## Warning in melt.data.table(as.data.table(simmed), value.name = &quot;height&quot;, : ## id.vars and measure.vars are internally guessed when both are &#39;NULL&#39;. All ## non-numeric/integer/logical type columns are considered id.vars, which in this ## case are columns []. Consider providing at least one of &#39;id&#39; or &#39;measure&#39; vars in ## future. DT[data.table(sim_x, x = paste0(&#39;V&#39;, 1:60)), logweight := sim_x, on = &#39;x&#39;] DT[, meanheight := mean(height), by = logweight] DT[, low := PI(height)[1], by = logweight] DT[, high := PI(height)[2], by = logweight] # Plot ggplot(DT) + geom_ribbon(aes(x = exp(logweight), ymin = low, ymax = high), fill = &#39;grey&#39;) + geom_point(aes(exp(logweight), height), data = d, color = &#39;lightblue&#39;, alpha = 0.8) + geom_line(aes(exp(logweight), meanheight)) Using the dlnorm, prior of a Log normal distribution on beta m2 &lt;- quap( alist( height ~ dnorm(mu, sigma), mu &lt;- a + b * (logweight - mean(d$logweight)), a ~ dnorm(178, 20), b ~ dlnorm(0, 1), sigma ~ dunif(0, 50) ), data = d ) sim_x &lt;- log(1:60) simmed &lt;- sim(m2, list(logweight = sim_x)) # Tidy DT &lt;- melt(as.data.table(simmed), value.name = &#39;height&#39;, variable.name = &#39;x&#39;) ## Warning in melt.data.table(as.data.table(simmed), value.name = &quot;height&quot;, : ## id.vars and measure.vars are internally guessed when both are &#39;NULL&#39;. All ## non-numeric/integer/logical type columns are considered id.vars, which in this ## case are columns []. Consider providing at least one of &#39;id&#39; or &#39;measure&#39; vars in ## future. DT[data.table(sim_x, x = paste0(&#39;V&#39;, 1:60)), logweight := sim_x, on = &#39;x&#39;] DT[, meanheight := mean(height), by = logweight] DT[, low := PI(height)[1], by = logweight] DT[, high := PI(height)[2], by = logweight] # Plot ggplot(DT) + geom_ribbon(aes(x = exp(logweight), ymin = low, ymax = high), fill = &#39;grey&#39;) + geom_point(aes(exp(logweight), height), data = d, color = &#39;lightblue&#39;, alpha = 0.8) + geom_line(aes(exp(logweight), meanheight)) 3.3 Homework: Question 3 Set up: library(rethinking) library(data.table) library(ggplot2) library(tidybayes) theme_set(theme_bw()) data(Howell1) d &lt;- Howell1 d$weight_s &lt;- scale(d$weight) d$weight_s2 &lt;- scale(d$weight) ^ 2 m &lt;- quap( alist( height ~ dnorm(mu, sigma), mu ~ a + b1 * weight_s + b2 * weight_s2, a ~ dnorm(178, 20), b1 ~ dlnorm(0, 1), b2 ~ dnorm(0, 1), sigma ~ dunif(0, 50) ), data = d ) n &lt;- 20 sim_x &lt;- seq(min(d$weight_s), max(d$weight_s), length.out = n) linked &lt;- link( m, data = list(weight_s = sim_x, weight_s2 = sim_x ^ 2), post = extract.prior(m) )[1:50,] plot(NULL, xlim = range(sim_x), ylim = range(linked) + c(-10, 10)) apply(linked, 1, FUN = function(x) lines(sim_x, x)) ## NULL m &lt;- quap( alist( height ~ dnorm(mu, sigma), mu ~ a + b1 * weight_s + b2 * weight_s2, a ~ dnorm(178, 20), b1 ~ dlnorm(0, 1), b2 ~ dnorm(0, 10), sigma ~ dunif(0, 50) ), data = d ) n &lt;- 20 sim_x &lt;- seq(min(d$weight_s), max(d$weight_s), length.out = n) linked &lt;- link( m, data = list(weight_s = sim_x, weight_s2 = sim_x ^ 2), post = extract.prior(m) )[1:50,] plot(NULL, xlim = range(sim_x), ylim = range(linked) + c(-10, 10)) apply(linked, 1, FUN = function(x) lines(sim_x, x)) ## NULL "],["homework-week-3.html", "4 Homework: Week 3 4.1 Overview 4.2 Question 1 4.3 Question 2 4.4 Question 3", " 4 Homework: Week 3 2021-08-25 [updated: 2021-08-26] 4.1 Overview All three problems below are based on the same data. The data in data(foxes) are 116 foxes from 30 different urban groups in England. These foxes are like street gangs. Group size varies from 2 to 8 individuals. Each group maintains its own (almost exclusive) urban territory. Some territories are larger than others. The area variable encodes this information. Some territories also have more avgfood than others. We want to model the weight of each fox. For the problems below, assume this DAG 4.1.1 Setup DAG library(ggdag) library(dagitty) library(data.table) library(ggplot2) dag_plot &lt;- function(dag) { stat &lt;- node_status(dag, FALSE) stat$data$status[is.na(stat$data$status)] &lt;- &#39;intermediate&#39; ggplot(stat, aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = status), alpha = 0.5, size = 15) + geom_dag_edges() + labs(color = &#39;&#39;) + geom_dag_text(color = &#39;black&#39;) + scale_color_manual(values = list(&#39;exposure&#39; = &#39;#35608DFF&#39;, &#39;outcome&#39; = &#39;#22A884FF&#39;, &#39;intermediate&#39; = &#39;grey50&#39;)) + theme_void() } dag &lt;- dagify( weight ~ groupsize + avgfood, groupsize ~ avgfood, avgfood ~ area, exposure = &#39;area&#39;, outcome = &#39;weight&#39; ) dag_plot(dag) 4.2 Question 1 Use a model to infer the total causal influence of area on weight. Would increasing the area available to each fox make it heavier (healthier)? You might want to standardize the variables. Regardless, use prior predictive simulation to show that your model’s prior predictions stay within the possible outcome range. 4.2.1 Workings AREA ON WEIGHT scale(weight) ~ dnorm(mu, sigma) mu &lt;- a + b * (scale(area)) a: intercept when weight and area are scaled, the expected intercept is 0 therefore a ~ dnorm(0, 0.5) b: beta, rate of change given one unit of increase in area b ~ dnorm(0, 1) sigma: standard deviation uniform prior sigma ~ dunif(0, 50) 4.2.2 Model library(rethinking) data(foxes) foxes$scale_area &lt;- scale(foxes$area) foxes$scale_weight &lt;- scale(foxes$weight) m1 &lt;- quap( alist( scale_weight ~ dnorm(mu, sigma), mu &lt;- a + bArea * scale_area, a ~ dnorm(0, 0.05), bArea ~ dnorm(0, 0.5), sigma ~ dunif(0, 50) ), data = foxes ) 4.2.3 Prior predictive simulation plot_link &lt;- function(DT, n) { data.table(DT)[sample(.N, n), plot(data.table(x = rep(c(-2, 2), .N), y = c(V1, V2)), type = &#39;l&#39;)] } prior &lt;- extract.prior(m1) l &lt;- link(m1, post = prior, data = list(scale_area = c(-2, 2))) plot_link(l, 20) ## NULL 4.2.4 Paths Interest: Area on Weight Paths Area -&gt; Avgfood -&gt; Weight Area -&gt; Avgfood -&gt; Groupsize -&gt; Weight There are pipes between Avgfood and between Avgfood and Groupsize. 4.2.5 Interpretation Would increasing the area available to each fox make it heavier (healthier)? bArea has a mean of 0.02, with compatibility intervals around 0. Therefore the model does not indicate an influence of area on the weight. precis(m1) ## mean sd 5.5% 94.5% ## a 0.000000076 0.044 -0.07 0.07 ## bArea 0.018830745 0.091 -0.13 0.16 ## sigma 0.995491796 0.065 0.89 1.10 post &lt;- extract.samples(m1) s &lt;- sim(m1, data = list(scale_area = c(-2, 2)), post = post) library(tidybayes) ggplot(data.table(s), aes(V1)) + stat_halfeye(.width = .89) # TODO: come back to this 4.3 Question 2 Now infer the causal impact of adding food to a territory. Would this make foxes heavier? Which covariates do you need to adjust for to estimate the total causal influence of food? Food on weight Paths: Food -&gt; Weight, Food -&gt; Groupsize -&gt; Weight Since Groupsize is a pipe between Food and Weight, it needs to be adjusted for to estimate the total causal influence of food. 4.3.1 Model foxes$scale_avgfood &lt;- scale(foxes$avgfood) foxes$scale_groupsize &lt;- scale(foxes$groupsize) m2 &lt;- quap( alist( scale_weight ~ dnorm(mu, sigma), mu &lt;- a + bGroupsize * scale_groupsize + bAvgfood * scale_avgfood, a ~ dnorm(0, 0.05), bGroupsize ~ dnorm(0, 0.5), bAvgfood ~ dnorm(0, 0.5), sigma ~ dunif(0, 50) ), data = foxes ) m2_withoutGroupsize &lt;- quap( alist( scale_weight ~ dnorm(mu, sigma), mu &lt;- a + bAvgfood * scale_avgfood, a ~ dnorm(0, 0.05), bAvgfood ~ dnorm(0, 0.5), sigma ~ dunif(0, 50) ), data = foxes ) prior &lt;- extract.prior(m2) precis(prior) ## mean sd 5.5% 94.5% histogram ## a 0.00088 0.05 -0.079 0.082 ▁▂▇▇▃▁▁ ## bGroupsize 0.02243 0.49 -0.771 0.797 ▁▁▂▇▇▃▁▁ ## bAvgfood -0.00693 0.50 -0.835 0.752 ▁▁▂▇▇▂▁▁ ## sigma 25.41485 14.05 3.702 46.754 ▅▇▇▇▅▇▇▇▇▅ 4.3.2 Interpretation precis(m2) ## mean sd 5.5% 94.5% ## a -0.0000000061 0.043 -0.069 0.069 ## bGroupsize -0.5724942193 0.180 -0.860 -0.285 ## bAvgfood 0.4762340795 0.180 0.189 0.763 ## sigma 0.9458960704 0.062 0.846 1.046 When adjusting for the group size, avgfood has a positive relationship with weight and group size has a negative relationship with weight. If group size is excluded from the model (as shown below), avgfood does not show a relationship to weight. precis(m2_withoutGroupsize) ## mean sd 5.5% 94.5% ## a 0.0000000089 0.044 -0.07 0.07 ## bAvgfood -0.0242047984 0.091 -0.17 0.12 ## sigma 0.9953693131 0.065 0.89 1.10 4.4 Question 3 Now infer the causal impact of group size. Which covariates do you need to adjust for? Looking at the posterior distribution of the resulting model, what do you think explains these data? That is, can you explain the estimates for all three problems? How do they go together? Group size on weight Paths: Groupsize -&gt; Weight, Groupsize &lt;- Avgfood -&gt; Weight Avgfood is a fork between groupsize and weight. m2 &lt;- quap( alist( scale_weight ~ dnorm(mu, sigma), mu &lt;- a + bGroupsize * scale_groupsize + bAvgfood * scale_avgfood, a ~ dnorm(0, 0.05), bGroupsize ~ dnorm(0, 0.5), bAvgfood ~ dnorm(0, 0.5), sigma ~ dunif(0, 50) ), data = foxes ) m2_withoutGroupsize &lt;- quap( alist( scale_weight ~ dnorm(mu, sigma), mu &lt;- a + bAvgfood * scale_avgfood, a ~ dnorm(0, 0.05), bAvgfood ~ dnorm(0, 0.5), sigma ~ dunif(0, 50) ), data = foxes ) prior &lt;- extract.prior(m2) precis(prior) ## mean sd 5.5% 94.5% histogram ## a -0.00058 0.05 -0.081 0.079 ▁▁▂▇▇▂▁▁▁ ## bGroupsize 0.01417 0.50 -0.798 0.813 ▁▂▇▇▃▁▁ ## bAvgfood -0.00757 0.48 -0.749 0.772 ▁▁▂▇▇▂▁▁ ## sigma 25.50335 14.58 3.081 47.348 ▇▇▅▇▇▇▇▇▇▇ "],["lecture-01.html", "5 Lecture 01 5.1 Hypotheses - Process Models - Statistical Models 5.2 Small world / large world 5.3 Example: four marbles 5.4 Building a model", " 5 Lecture 01 5.0.1 Popper generate meaningful (not null) hypotheses and predictions and falsify those 5.0.2 Approach A framework for developing + using statistical golems Bayesian data analysis uses probability to describe uncertainty “count all the ways data can happen, according to assumptions and the assumptions with more ways consistent with the data are more plausible” Multilevel models Models within models Avoids averaging … Model comparison Compare meaningful (not null) models Caution: overfitting 5.1 Hypotheses - Process Models - Statistical Models Any statistical model M can correspond to multiple process models Any hypothesis H may correspond to multiple process models Any statistical model may correspond to multiple hypothesis Untitled 5.2 Small world / large world Small world: models have assumptions, bayesian models fit optimally Large world: real world, no guarantee of optimality 5.3 Example: four marbles Setup 4 marbles, either black or white, with replacement Possibilities (5) therefore: WWWW, BWWW, BBWW, BBBW, BBBB Observation: BWB Calculate Given 3 observations, there are 4 choices, for a total of 64 possibilities Given we observed both a white and a black marble, possibilities WWWW and BBBB are not valid At each branch, there are 3 possibilities it can be white and 1 possibility it can be black Bayesian is additive, at each branch just sum the possibility BWWW: 3 = 1 * 3 * 1 BBWW: 8 = 2 * 2 * 2 BBBW: 9 = 3 * 1 * 3 Using new information New information is directly integrated into the old information, therefore just multiply it through So if we take another measure of B, multiply the property through BWWW: 3 * 1 = 3 BBWW: 8 * 2 = 16 BBBW: 9 * 3 = 27 Using other information Factory says B measures are rare, but minimum of 1 per bag Factory info WWWW 0 since we observed a B BWWW: 3 BBWW: 2 BBBW: 1 BBBB 0 since we observed a W Multiply it through BWWW: 3 * 3 = 9 BBWW: 16 * 2 = 32 BBBW: 1 * 27 = 27 Counts get huge - therefore we normalize them giving us probabilities (0-1) Probability theory is just normalized counting 5.4 Building a model Design the model Condition on the data Evaluate, critique the model (Restart) "],["lecture-02.html", "6 Lecture 02 6.1 Joint prior distribution 6.2 Example: inflatable world 6.3 Grid approximation", " 6 Lecture 02 6.1 Joint prior distribution The joint prior distribution is the prior probability of distribution + parameters 6.2 Example: inflatable world 6.2.1 Design the model p: water proportion 1-p: land proportion 6.2.2 Condition Bayes updating: converts priors to posteriors Adds all data at once All posteriors are the prior for next observation Sample size is embodied in the posterior 6.2.3 Evaluate Golem must be supervised Did it malfunction? Does the answer make sense? … 6.2.4 Define parameters N: fixed by experimentor W: a probability distribution, in this case a binomial distribution WLWWLWWLW dbinom(6, size = 9, prob = 0.5) p: prior probability distribution, in this case uniformed 6.2.5 Joint model W ~ Binomial(N, p) p ~ Uniform(0, 1) (W is distributed binomially with probability p on each measure, p is uniform at 1) 6.2.6 Posterior Posterior = (probability observed variables * prior) / normalizing constant (If priors are uniform, they don’t affect the shape of the posterior. They may influence the shape though) 6.3 Grid approximation Grid approximation: consider only a finite discrete set of solutions For example, 1000 solutions Generate a sequence of solutions seq_sol &lt;- seq(0, 1, length.out = 1000) Prior = uniform 1 across sequence of solutions prior &lt;- rep(1, seq_sol) Probability of data = binomial prob_data &lt;- dbinom(6, size = 9, prob = seq_sol) Posterior numerator = posterior_num &lt;- prior * prob_data Posterior standardized = posterior_numerator / sum(posterior_num) 6.3.1 Sampling from the posterior Approximate the posterior, then you can sample from the posteriors sample(p, prob = posterior, 1e4, replace = TRUE) Summarize above/below some value Percentile interval Highest posterior density interval … Predictive checks rbinorm(1e4, size = 0, prob = samples) … "],["lecture-03.html", "7 Lecture 03 7.1 Regressions 7.2 Normal distributions 7.3 Prior predictive distributions 7.4 Quadratic approximate 7.5 Centering variables", " 7 Lecture 03 7.1 Regressions Model of mean and variance of some normally distributed measure Mean as additive combination of weighted vairables Typical assumed constant variable (???) The line returned is the mean - but with bayesian we want to see the distribution of lines, ranked by plausability The model endorses the line, but the line doesn’t necessarily fit the data In regressions there will always be more certainty at the means and bowtie towards the limits of the data Regression models don’t have arrows like DAGs - they just measure associations. 7.2 Normal distributions Normal distributions arise when repeated fluctuations tend to cancel near 0 The gaussian distribution is the most conservative distribution to use for a prior, it is the best option if no additional scientific information is available 7.3 Prior predictive distributions Simulate from the joint posterior distribution and evaluate Setup model with quap prior &lt;- extract.prior(model) link(model, post = prior, data = seq) where the seq is a sequence of your x variable (eg for standardize -2, 2 Plot lines These are all the possibility give the prior, not the data If the lines show such a limited relationship that you’d expect that the true relationship is outside of these, expand the priors. If alternatively they are widely implausible, tighten the priors. 7.4 Quadratic approximate In a multidimensional space, QUAP uses gradient climbing to find peaks Maximum likelihood estimation = QUAP with flat priors Function in rethinking is rethinking::quap 7.5 Centering variables x - mean(x) Should be default behaviour when doing a regression "],["lecture-04.html", "8 Lecture 04 8.1 Standardizing variables 8.2 Plotting uncertainty - sample from posterior 8.3 Polynomials 8.4 Splines 8.5 Basis splines", " 8 Lecture 04 8.1 Standardizing variables (x - mean(x)) / sd(x) or scale(x) Result = mean of 0, sd of 1 Helps software fit Value = 1 is equal to 1 SD 8.2 Plotting uncertainty - sample from posterior (if multivariate normal) Approximate posterior from mean, standard deviation Sample from multivariate normal distribution of parameters Use sampels to generate predictions that integrate over uncertainty extract_samples returns a, b, sigma, … and you can plot each 8.3 Polynomials Polynomials have bad behaviour especially at the boundaries of the data They don’t fit locally, and are not actually flexibly. Eg. a polynomial of 3rd degree will necessarily have two turns - this has to happen irrespective of the data 8.4 Splines Locally wiggly functions, combined by interpolation Geocentric - describing relationships - not exploring them 8.5 Basis splines Bayesian B-splines = P-splines Similar to linear models but with synthetic variables \\(\\mu = \\alpha + w_{1} \\beta_{1} + + w_{2} \\beta_{2} + + w_{3} \\beta_{3} + + w_{4} \\beta_{4} + ...\\) Knots are often picked at equal intervals in data, though strategies vary At each knot, the knot’s function is at 100%, moving away from it, the neighboring functions turn on Parameters always have more uncertainty than predictions Caution: overfitting 8.5.1 Recipe Choose knots - points where spline pivots Choose degree of basis functions - how wiggly, polynomial Find posterior distribution of weights "],["lecture-05.html", "9 Lecture 05 9.1 Multiple regression models 9.2 Directed acyclic graphs (DAG) 9.3 Example: Age, marriage, divorce 9.4 Plotting multivariate posteriors 9.5 Reveal masked associations 9.6 Categorical variables", " 9 Lecture 05 9.1 Multiple regression models Why? Spurious associations Determining the value of some predictor given other predictors eg. divorce rate given marriage rate and median age at marriage. Once we know marriage rate, what is the value in knowing median age? 9.2 Directed acyclic graphs (DAG) Directed: arrows, indicating causal implications Acyclic: no loops Unlike statistical models, DAGs have causal implications eg. Median age → marriage rate → divorce rate, Median age → divorce rate 9.3 Example: Age, marriage, divorce \\(D_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma)\\) \\(\\mu_{i} = \\alpha + \\beta_{M}M_{i} + \\beta_{A}A_{i}\\) (M)arriage rate (A)ge at marriage (D)ivorce rate 9.3.1 Priors Standardize to z-scores \\(\\alpha\\) = expected value for response when all values are 0. since they are all standardized the response should be 0. Without peaking at the data, this could be hard to guess. But after standardization, it is much simpler. Slopes - use prior predictive simulation. Harder. 9.3.2 Prior predictive simulation See https://www.notion.so/Statistical-Rethinking-79b2bb4d7c664277a8889122c7a03a9f#4b1e14ff04394a6d881331eda4185ec0 9.3.3 Interpretation Once we know median age at marriage, there is little additional value in knowing marriage rate. Once we know marriage rate, there is still value in knowing median age at marriage. If we don’t know median, it is still useful to know marriage rate, since median age at marriage is related to marriage rate. However, we don’t want to try and influence eg. policy on marriage rate, since it isn’t causal on divorce rate. 9.4 Plotting multivariate posteriors Regress predictor on other predictors Compute predictor residuals Regress outcome on residuals Side note: never analyze the residuals. 9.4.1 Posterior predictive checks Compute implied predictions for observed cases Again, regressions will always do well in the area around the mean 9.5 Reveal masked associations Sometimes association between outcome and predictor is masked by another variable This tends to arise when 2 predictors associated with the outcome have opposite effects on it 9.6 Categorical variables Two approaches: Use dummy/indicator variables Use index variables Index variables are much better 9.6.1 Dummy variable “Stand in” variable Eg. male/female column, translated to 0, 1, 0, 0, 1 where 0 female, 1 male Model: \\(h_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma)\\) \\(\\mu_{i} = \\alpha + \\beta_{M}M_{i}\\) In the case of dummy variables, alpha is the mean when M = 0 (female) and beta M is the change in mean when M = 1 (male). Result is 2 intercepts = where alpha alone is for female and alpha + beta M is intercept for males Problem: for k categories, need k-1 dummy variables and need priors for each. also, priors aren’t balanced because of alpha vs beta "],["lecture-06.html", "10 Lecture 06 10.1 Index variable 10.2 Four elemental confounds", " 10 Lecture 06 10.1 Index variable (For unordered, categorical variables) Starts at 1, counts up Same prior can be given to all Extends easily &gt; 2 eg. m &lt;- quap( alist( height ~ dnorm(mu, sigma), mu &lt;- a[sex], a[sex] ~ dnorm(178, 20), sigma ~ dunif(0, 50) ), data = d ) a[sex] and a for each sex and prior for each. directly in precis(m) too. Then you can directly calculate the difference between groups in the posteriors, no need to rerun the model post &lt;- extract.samples(m) post$diff &lt;- post$a[, 1] - post$a[, 2] precis(post) # mean # sigma 27 # a[1] 134 # a[2] 142 # diff -7.7 10.2 Four elemental confounds When inferring the relationships between X and Y… Confounds are not determined by model selection, so we use DAGs. Arrows indicate causation, and statistical information can flow either way. 10.2.1 Notes Regression models don’t have arrows like DAGs - they just measure associations. You can’t tell the difference between the fork and the path given the data alone. Remember DAGs are small world constructs. 10.2.2 The fork X ← Z → Y Z is a common cause of X and Y. Including Z will remove the relationship between X and Y. 10.2.3 The path X → Z → Y Z is along the path of X and Y, mediating the relationship. For example, the influence of treatment on plant height, where treatment has an influence on fungus. T → F → H Since the treatment influences the fungus (a post treatment measure), if we include both the treatment and the fungus, we will see no relationship of treatment on height, only fungus. (once we know fungus, what does treatment tell us - nothing). In this case, the model with both treatment and fungus tells us the relationship between them, but to properly consider the influence of treatment we need to omit fungus Therefore, understanding the relationship between T and F is important, but for determining causality of T on H, we need to omit it from that model. 10.2.4 The collider X → Z ← Y Z is common result of X and Y. X and Y are independent, if you condition on Z. 10.2.5 Steps List all paths connecting X (treatment) and Y (outcome) Classify each path as either open or closed. All paths are open unless they contain a collider. Classify each path as backdoor/frontdoor. Backdoor paths have an arrow entering X. Condition on variables in backdoor paths to close them. Untitled "],["lecture-07.html", "11 Lecture 07 11.1 Four elemental confounds (continued) 11.2 Over fitting 11.3 Measuring model fit 11.4 Obtaining the regular features", " 11 Lecture 07 11.1 Four elemental confounds (continued) 11.1.1 Unobserved variables Careful about unmeasured variables. They can create confounds, without being directly measured. Eg. (Haunted DAG). G on C. G → P → C, G → C. But unobserved variable U creates a collider: G → P ← U → C. So including P allows the collider to distort the influence on G on C. 11.2 Over fitting Ockham’s razor: “plurality should never be posited without necessity” This isn’t sufficient, because we are usually comparing between models that are more complicated but fit the data better, and models that are less complicated but fit worse. Two major hazards: too simple, not learning enough from data (under fitting) and too complex, learning too much from data (over fitting) Goal = to learn from regular features from the sample, those that will generalize to other samples 11.3 Measuring model fit 11.3.1 R squared Common, not great \\(R_{2} = 1 - \\frac{var(residuals)}{var(outcome)}\\) “Proportion of variance explained” You can get R squared = 1 with a parameter for each data point - perfect fit. This is obviously nonsense. Therefore there’s a trap of picking models solely on their R squared because increase the parameters and you will increase the R squared. 11.4 Obtaining the regular features Regularizing priors Cross validation Information criteria "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
