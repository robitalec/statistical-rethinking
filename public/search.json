[{"path":"index.html","id":"overview","chapter":"Overview","heading":"Overview","text":"Alec L. Robitaille","code":""},{"path":"index.html","id":"approach","chapter":"Overview","heading":"Approach","text":"Chapters 1-4, use quap rethinking package, following\nalong book solutions. Chapters 5-10, use cmdstanr \nstantargets benefit incredible targets package. wrapped\nbookdown. Thanks Richard McElreath book providing\nlectures available online. thanks package Stan developers.See full references packages.","code":""},{"path":"index.html","id":"links","chapter":"Overview","heading":"Links","text":"Course: https://github.com/rmcelreath/statrethinking_winter2019targets: https://books.ropensci.org/targets/cmdstanr: https://mc-stan.org/cmdstanr/stantargets: https://docs.ropensci.org/stantargets/bookdown: https://bookdown.org/home/","code":""},{"path":"index.html","id":"adaptations","chapter":"Overview","heading":"Adaptations","text":"Stan+R: https://vincentarelbundock.github.io/rethinking2/tidy+rethinking: https://david-salazar.github.io/2020/04/19/statistical-rethinking-week-1/brms+tidy: https://bookdown.org/content/4857/Julia+Turing: https://github.com/StatisticalRethinkingJulia/TuringModels.jl","code":""},{"path":"index.html","id":"etc","chapter":"Overview","heading":"Etc","text":"https://chi-feng.github.io/mcmc-demo","code":""},{"path":"index.html","id":"seed","chapter":"Overview","heading":"Seed","text":"","code":"\nset.seed(42)"},{"path":"week-1.html","id":"week-1","chapter":"1 Week 1","heading":"1 Week 1","text":"2021-08-18 [updated: 2021-11-17]","code":""},{"path":"week-1.html","id":"variables","chapter":"1 Week 1","heading":"1.1 Variables","text":"N: fixed experimenterp: Prior probabilityW: probability distribution given data.","code":""},{"path":"week-1.html","id":"joint-model","chapter":"1 Week 1","heading":"1.2 Joint model","text":"W ~ Binomial(N , p)p ~ Uniform(0, 1)W distributed binomially N observations probability p eachp distributed uniformally 1","code":""},{"path":"week-1.html","id":"question-1","chapter":"1 Week 1","heading":"1.3 Question 1","text":"Suppose globe tossing data (Chapter 2) turned 8 water 15\ntosses. Construct posterior distribution, using grid approximation. Use \nflat prior book.","code":"\n# Size of grid for grid approximation\ngridsize <- 1000\n\n# Prior grid\nprior_grid <- seq(0, 1, length.out = gridsize)\n\n# Prior probability (all 1)\nprior_prob <- rep(1, gridsize)\n\n# Data probability\n#  given 4/15, using binomial distribution\ndata_prob <- dbinom(8, 15, prob = prior_grid)\n\n# Calculate the posterior numerator by multiplying prior and data probability\nposterior_num <- prior_prob * data_prob\n# Standardize by sum of posterior numerator\nposterior <- posterior_num / sum(posterior_num)\n\n# Save for later\nposterior_1 <- posterior\n\nplot(posterior, type = 'l')\n# Sample from posterior\nsamples <- sample(prior_grid, size = gridsize, prob = posterior, replace = TRUE)\nmean(samples)## [1] 0.54\nPI(samples, .99)##   1% 100% \n## 0.27 0.80"},{"path":"week-1.html","id":"question-2","chapter":"1 Week 1","heading":"1.4 Question 2","text":"Start 1, now use prior zero p = 0.5 constant\np = 0.5. corresponds prior information majority \nEarth’s surface water. difference better prior make?Narrower curve, higher max, zeroes 0.5","code":"\n# Size of grid for grid approximation\ngridsize <- 1000\n\n# Prior grid\nprior_grid <- seq(0, 1, length.out = gridsize)\n\n# Prior probability (all 1 above 0.5, all 0 below)\nprior_prob <- c(rep(0, gridsize / 2), rep(1, gridsize / 2))\n\n# Data probability\n#  given 4/15, using binomial distribution\ndata_prob <- dbinom(4, 15, prob = prior_grid)\n\n# Calculate the posterior numerator by multiplying prior and data probability\nposterior_num <- prior_prob * data_prob\n# Standardize by sum of posterior numerator\nposterior <- posterior_num / sum(posterior_num)\n\n# Save for later\nposterior_2 <- posterior\n\nplot(posterior, type = 'l')\n# Sample from posterior\nsamples <- sample(prior_grid, size = gridsize, prob = posterior, replace = TRUE)\nmean(samples)## [1] 0.55\nPI(samples, .99)##   1% 100% \n## 0.50 0.72"},{"path":"week-1.html","id":"question-3","chapter":"1 Week 1","heading":"1.5 Question 3","text":"posterior distribution 2, compute 89% percentile HPDI\nintervals. Compare widths intervals. wider? ? \ninformation interval, might misunderstand \nshape posterior distribution?","code":"\n# Calculate Percentile Interval at 89%\npercent_interval <- PI(posterior, prob = 0.89)\npercent_interval##     5%    94% \n## 0.0000 0.0073\n# Calculate Highest Posterior Density at 89%\nhighest_post_dens <- HPDI(posterior, prob = 0.89)\nhighest_post_dens##  |0.89  0.89| \n## 0.0000 0.0025"},{"path":"week-2.html","id":"week-2","chapter":"2 Week 2","heading":"2 Week 2","text":"2021-08-24 [updated: 2021-11-17]","code":""},{"path":"week-2.html","id":"question-1-1","chapter":"2 Week 2","heading":"2.1 Question 1","text":"weights listed recorded !Kung census, heights \nrecorded individuals. Provide predicted heights 89% compatibility\nintervals individuals. , fill table , using\nmodel-based predictions.Model:\\(h_{} \\sim \\text{Normal}(\\mu_{}, \\sigma)\\)\\(\\mu_{} = \\alpha + \\beta(x_{} - \\bar{x})\\)\\(\\alpha \\sim \\text{Normal}(178, 20)\\)\\(\\beta \\sim \\text{Log-Normal}(0, 1)\\)\\(\\sigma \\sim \\text{Uniform}(0, 50)\\)Simulate:","code":"Individual, weight, expected height, 89% interval\n1, 45,,,\n2, 40,,,\n3, 65,,,\n4, 31,,,\n5, 53,,,\ntheme_set(theme_bw())\n\ndata(Howell1)\n\nd <- Howell1[Howell1$age >= 18,]\n\nm <- quap(\n    alist(\n        height ~ dnorm(mu, sigma),\n        mu <- a + b * (weight - mean(weight)),\n        a ~ dnorm(178, 20),\n        b ~ dnorm(0, 1),\n        sigma ~ dunif(0, 50)\n    ), \n    data = d\n)\n\nprecis(m)##        mean    sd   5.5%  94.5%\n## a     154.6 0.270 154.17 155.03\n## b       0.9 0.042   0.84   0.97\n## sigma   5.1 0.191   4.77   5.38\n# Set weights to simulate for\nweights <- data.table(weight = c(45, 40, 65, 31, 54),\n                                            id = as.character(seq(1, 5)))\nsimmed <- sim(m, list(weight = weights$weight), n = 1e3)\n\n# Tidy\nDT <- melt(as.data.table(simmed), measure.vars = paste0('V', 1:5),\n                     value.name = 'height', variable.name = 'id')\nDT[, id := gsub('V', '', id)]\nDT[weights, weight := weight, on = 'id']\n\n# Plot\nggplot(DT, aes(height)) +\n    stat_halfeye(.width = .89) + \n    facet_wrap(~id)"},{"path":"week-2.html","id":"question-2-1","chapter":"2 Week 2","heading":"2.2 Question 2","text":"Model relationship height (cm) natural logarithm weight\n(log-kg): log(weight). Use entire Howell1 data frame, 544 rows, adults\nnon-adults. Use model type Chapter 4 think useful: \nordinary linear regression, polynomial spline. Plot posterior\npredictions raw dataUsing dlnorm, prior Log normal distribution beta","code":"\ntheme_set(theme_bw())\n\ndata(Howell1)\nd <- Howell1\nd$logweight <- log(d$weight)\n\nm1 <- quap(\n    alist(\n        height ~ dnorm(mu, sigma),\n        mu <- a + b * (logweight - mean(logweight)),\n        a ~ dnorm(178, 20),\n        b ~ dnorm(0, 1),\n        sigma ~ dunif(0, 50)\n    ), \n    data = d\n)\nsim_x <- log(1:60)\nsimmed <- sim(m1, list(logweight = sim_x))\n\n# Tidy\nDT <- melt(as.data.table(simmed), value.name = 'height', variable.name = 'x')## Warning in melt.data.table(as.data.table(simmed), value.name = \"height\", :\n## id.vars and measure.vars are internally guessed when both are 'NULL'. All\n## non-numeric/integer/logical type columns are considered id.vars, which in this\n## case are columns []. Consider providing at least one of 'id' or 'measure' vars\n## in future.\nDT[data.table(sim_x, x = paste0('V', 1:60)),\n     logweight := sim_x,\n     on = 'x']\nDT[, meanheight := mean(height), by = logweight]\nDT[, low := PI(height)[1], by = logweight]\nDT[, high := PI(height)[2], by = logweight]\n\n# Plot\nggplot(DT) + \n    geom_ribbon(aes(x = exp(logweight), ymin = low, ymax = high), fill = 'grey') + \n    geom_point(aes(exp(logweight), height), data = d, color = 'lightblue', alpha = 0.8) + \n    geom_line(aes(exp(logweight), meanheight))\nm2 <- quap(\n    alist(\n        height ~ dnorm(mu, sigma),\n        mu <- a + b * (logweight - mean(logweight)),\n        a ~ dnorm(178, 20),\n        b ~ dlnorm(0, 1),\n        sigma ~ dunif(0, 50)\n    ), \n    data = d\n)\nsim_x <- log(1:60)\nsimmed <- sim(m2, list(logweight = sim_x))\n\n# Tidy\nDT <- melt(as.data.table(simmed), value.name = 'height', variable.name = 'x')## Warning in melt.data.table(as.data.table(simmed), value.name = \"height\", :\n## id.vars and measure.vars are internally guessed when both are 'NULL'. All\n## non-numeric/integer/logical type columns are considered id.vars, which in this\n## case are columns []. Consider providing at least one of 'id' or 'measure' vars\n## in future.\nDT[data.table(sim_x, x = paste0('V', 1:60)),\n     logweight := sim_x,\n     on = 'x']\nDT[, meanheight := mean(height), by = logweight]\nDT[, low := PI(height)[1], by = logweight]\nDT[, high := PI(height)[2], by = logweight]\n\n# Plot\nggplot(DT) + \n    geom_ribbon(aes(x = exp(logweight), ymin = low, ymax = high), fill = 'grey') + \n    geom_point(aes(exp(logweight), height), data = d, color = 'lightblue', alpha = 0.8) + \n    geom_line(aes(exp(logweight), meanheight))"},{"path":"week-2.html","id":"question-3-1","chapter":"2 Week 2","heading":"2.3 Question 3","text":"Set :","code":"\ntheme_set(theme_bw())\n\ndata(Howell1)\nd <- Howell1\nd$weight_s <- scale(d$weight)\nd$weight_s2 <- scale(d$weight) ^ 2\n\nm <- quap(\n    alist(\n        height ~ dnorm(mu, sigma),\n        mu ~ a + b1 * weight_s + b2 * weight_s2,\n        a ~ dnorm(178, 20),\n        b1 ~ dlnorm(0, 1),\n        b2 ~ dnorm(0, 1),\n        sigma ~ dunif(0, 50)\n    ),\n    data = d\n)\n\n\nn <- 20\nsim_x <- seq(min(d$weight_s), max(d$weight_s), length.out = n)\nlinked <- link(\n    m,\n    data = list(weight_s = sim_x, weight_s2 = sim_x ^ 2),\n    post = extract.prior(m)\n)[1:50,]\nplot(NULL, xlim = range(sim_x), ylim = range(linked) + c(-10, 10))\napply(linked, 1, FUN = function(x) lines(sim_x, x))## NULL\nm <- quap(\n    alist(\n        height ~ dnorm(mu, sigma),\n        mu ~ a + b1 * weight_s + b2 * weight_s2,\n        a ~ dnorm(178, 20),\n        b1 ~ dlnorm(0, 1),\n        b2 ~ dnorm(0, 10),\n        sigma ~ dunif(0, 50)\n    ),\n    data = d\n)\n\n\nn <- 20\nsim_x <- seq(min(d$weight_s), max(d$weight_s), length.out = n)\nlinked <- link(\n    m,\n    data = list(weight_s = sim_x, weight_s2 = sim_x ^ 2),\n    post = extract.prior(m)\n)[1:50,]\nplot(NULL, xlim = range(sim_x), ylim = range(linked) + c(-10, 10))\napply(linked, 1, FUN = function(x) lines(sim_x, x))## NULL"},{"path":"week-3.html","id":"week-3","chapter":"3 Week 3","heading":"3 Week 3","text":"2021-08-25 [updated: 2021-11-17]","code":""},{"path":"week-3.html","id":"overview-1","chapter":"3 Week 3","heading":"3.1 Overview","text":"three problems based data. data data(foxes)\n116 foxes 30 different urban groups England. foxes like\nstreet gangs. Group size varies 2 8 individuals. group maintains\n(almost exclusive) urban territory. territories larger \nothers. area variable encodes information. territories also \navgfood others. want model weight fox. \nproblems , assume DAG","code":""},{"path":"week-3.html","id":"setup","chapter":"3 Week 3","heading":"3.1.1 Setup","text":"","code":"\n# DAG\ndag <- dagify(\n    weight ~ groupsize + avgfood,\n    groupsize ~ avgfood,\n    avgfood ~ area,\n    exposure = 'area',\n    outcome = 'weight'\n)\n\ndag_plot(dag)"},{"path":"week-3.html","id":"question-1-2","chapter":"3 Week 3","heading":"3.2 Question 1","text":"Use model infer total causal influence area weight. \nincreasing area available fox make heavier (healthier)? might\nwant standardize variables. Regardless, use prior predictive simulation\nshow model’s prior predictions stay within possible outcome\nrange.","code":""},{"path":"week-3.html","id":"workings","chapter":"3 Week 3","heading":"3.2.1 Workings","text":"Area weightscale(weight) ~ dnorm(mu, sigma)\nmu <- + b * (scale(area))\n: intercept\nweight area scaled, expected intercept 0\ntherefore\n~ dnorm(0, 0.5)b: beta, rate change given one unit increase area\nb ~ dnorm(0, 1)sigma: standard deviation\nuniform prior\nsigma ~ dunif(0, 50)","code":""},{"path":"week-3.html","id":"model","chapter":"3 Week 3","heading":"3.2.2 Model","text":"","code":"\ndata(foxes)\n\nfoxes$scale_area <- scale(foxes$area)\nfoxes$scale_weight <- scale(foxes$weight)\n\nm1 <- quap(\n    alist(\n        scale_weight ~ dnorm(mu, sigma),\n        mu <- a + bArea * scale_area,\n        a ~ dnorm(0, 0.05),\n        bArea ~ dnorm(0, 0.5),\n        sigma ~ dunif(0, 50)\n    ), \n    data = foxes\n)"},{"path":"week-3.html","id":"prior-predictive-simulation","chapter":"3 Week 3","heading":"3.2.3 Prior predictive simulation","text":"","code":"\nprior <- extract.prior(m1)\nl <- link(m1, post = prior, data = list(scale_area = c(-2, 2)))\nplot_link(l, 20)## NULL"},{"path":"week-3.html","id":"paths","chapter":"3 Week 3","heading":"3.2.4 Paths","text":"Interest: Area WeightPathsArea -> Avgfood -> WeightArea -> Avgfood -> Groupsize -> WeightAvgfood Groupsize pipes Area Weight. \nbackdoors colliders.","code":""},{"path":"week-3.html","id":"interpretation","chapter":"3 Week 3","heading":"3.2.5 Interpretation","text":"increasing area available fox make heavier (healthier)?bArea mean 0.02, compatibility intervals around 0.\nTherefore model indicate total causal influence \narea weight.","code":"\nprecis(m1)##            mean    sd  5.5% 94.5%\n## a     -0.000001 0.044 -0.07  0.07\n## bArea  0.018844 0.091 -0.13  0.16\n## sigma  0.995497 0.065  0.89  1.10\npost <- extract.samples(m1)\ns <- sim(m1, data = list(scale_area = c(-2, 2)), post = post)\n\nggplot(data.table(s), aes(V1)) +\n    stat_halfeye(.width = .89)"},{"path":"week-3.html","id":"question-2-2","chapter":"3 Week 3","heading":"3.3 Question 2","text":"Now infer causal impact adding food territory. make foxes\nheavier? covariates need adjust estimate total causal\ninfluence food?","code":""},{"path":"week-3.html","id":"paths-1","chapter":"3 Week 3","heading":"3.3.1 Paths","text":"Interest: Food weightPaths:Food -> WeightFood -> Groupsize -> WeightGroupsize pipe Area Weight. backdoors colliders.","code":""},{"path":"week-3.html","id":"model-1","chapter":"3 Week 3","heading":"3.3.2 Model","text":"","code":"\nfoxes$scale_avgfood <- scale(foxes$avgfood)\n\nm2 <- quap(\n    alist(\n        scale_weight ~ dnorm(mu, sigma),\n        mu <- a + bFood * scale_avgfood,\n        a ~ dnorm(0, 0.05),\n        bFood ~ dnorm(0, 0.5),\n        sigma ~ dunif(0, 50)\n    ), \n    data = foxes\n)\n\nprecis(m2)##             mean    sd  5.5% 94.5%\n## a      0.0000018 0.044 -0.07  0.07\n## bFood -0.0241757 0.091 -0.17  0.12\n## sigma  0.9953723 0.065  0.89  1.10\npost <- extract.samples(m2)\ns <- sim(m2, data = list(scale_avgfood = c(-2, 2)), post = post)\n\nggplot(data.table(s), aes(V1)) +\n    stat_halfeye(.width = .89)"},{"path":"week-3.html","id":"interpretation-1","chapter":"3 Week 3","heading":"3.3.3 Interpretation","text":"make foxes heavier? covariates need adjust \nestimate total causal influence food?bFood mean -0.02, compatibility intervals around 0.\nmodel indicate total causal influence area weight.\ncovariates needed estiamte total causal influence food\nbackdoors colliders.","code":""},{"path":"week-3.html","id":"question-3-2","chapter":"3 Week 3","heading":"3.4 Question 3","text":"Now infer causal impact group size. covariates need \nadjust ? Looking posterior distribution resulting model, \nthink explains data? , can explain estimates \nthree problems? go together?","code":""},{"path":"week-3.html","id":"paths-2","chapter":"3 Week 3","heading":"3.4.1 Paths","text":"Interest: Group size weightPaths:Groupsize -> WeightGroupsize <- Avgfood -> WeightAvgfood collider Groupsize Weight. backdoor\nGroupsize path closed.","code":"\nfoxes$scale_groupsize <- scale(foxes$groupsize)\n\nm3 <- quap(\n    alist(\n        scale_weight ~ dnorm(mu, sigma),\n        mu <- a + bGroupsize * scale_groupsize + bFood * scale_avgfood,\n        a ~ dnorm(0, 0.05),\n        bGroupsize ~ dnorm(0, 0.5),\n        bFood ~ dnorm(0, 0.5),\n        sigma ~ dunif(0, 50)\n    ), \n    data = foxes\n)\n\nprecis(m3)##                   mean    sd   5.5%  94.5%\n## a           0.00000062 0.043 -0.069  0.069\n## bGroupsize -0.57250071 0.180 -0.860 -0.285\n## bFood       0.47624205 0.180  0.189  0.763\n## sigma       0.94590281 0.062  0.846  1.046\nDT <- melt(data.table(extract.samples(m3))[, .(bGroupsize, bFood)])## Warning in melt.data.table(data.table(extract.samples(m3))[, .(bGroupsize, :\n## id.vars and measure.vars are internally guessed when both are 'NULL'. All\n## non-numeric/integer/logical type columns are considered id.vars, which in this\n## case are columns []. Consider providing at least one of 'id' or 'measure' vars\n## in future.\nggplot(DT) + \n    geom_density(aes(value, fill = variable), alpha = 0.6) + \n    theme_bw() + scale_fill_viridis_d(begin = 0.3, end = 0.8)"},{"path":"week-3.html","id":"interpretation-2","chapter":"3 Week 3","heading":"3.4.2 Interpretation","text":"covariates need adjust ? Looking posterior\ndistribution resulting model, think explains data? \n, can explain estimates three problems? go\ntogether?Avgfood covariate needs included since collider \nGroupsize Weight. mean compatibility intervals bFood positive,\nmean compatibility intervals bGroupsize negative. \nindicates food’s positive relationship weight buffered interacting\nnegative relationship group size. Increased food leads increased\nbody weight, food also results larger groups, decreases \nfood availability.","code":""},{"path":"week-4.html","id":"week-4","chapter":"4 Week 4","heading":"4 Week 4","text":"2021-08-30 [updated: 2021-11-17]","code":""},{"path":"week-4.html","id":"question-1-3","chapter":"4 Week 4","heading":"4.1 Question 1","text":"Consider three fictional Polynesian islands. Royal\nOrnithologist charged king surveying birb population. \nfound following proportions 5 important birb species:First, compute entropy island’s birb distribution. Interpret \nentropy valuesThe information entropy describes uncertainty distribution \nprobabilities given average log-probability event (Statistical\nRethinking 7.2). Island 1 highest entropy, flat probability 0.2 across 5 bird species. Island 2 lowest entropy, including species highest overall proportion 0.8.Second, use island’s birb distribution predict two. \nmeans compute K-L Divergence island others, treating\nisland statistical model islands. \nend 6 different K-L Divergence values. island predicts others\nbest? ?divergence(p, q) = “Average difference log probability target (p) model (q)”.Model 1 predicts target 3 best (lowest divergence 0.63) target 2 best (lowest divergence 0.87) highest entropy. Model 3 predicts target 1 best (lowest divergence 0.64) higher entropy model 2.","code":"\n# Data\nbirds <- matrix(\n    c(0.2, 0.2, 0.2, 0.2, 0.2,\n        0.8, 0.1, 0.05, 0.025, 0.025,\n        0.05, 0.15, 0.7, 0.05, 0.05),\n    nrow = 3, ncol = 5, byrow = TRUE\n)\ndimnames(birds) <- list(as.character(1:3), LETTERS[1:5])\nbirds##      A    B    C     D     E\n## 1 0.20 0.20 0.20 0.200 0.200\n## 2 0.80 0.10 0.05 0.025 0.025\n## 3 0.05 0.15 0.70 0.050 0.050\nDT <- melt(data.table(birds, keep.rownames = 'island'), id.vars = 'island',\n                     variable.name = 'id', value.name = 'proportion')\n\n# Entropy\nentropy <- function(p) -sum(p * log(p))\nDT[, .(entropy = entropy(proportion)), by = island]##    island entropy\n## 1:      1    1.61\n## 2:      2    0.74\n## 3:      3    0.98\ndivergence <- function(p, q) sum(p * (log(p) - log(q)))\nz <- CJ(p = DT$island, q = DT$island, unique = TRUE)[, row_id := .I]\nz[, div := divergence(DT[island == p, proportion], \n                                            DT[island == q, proportion]),\n    by = row_id]\n\nz[p != q]##    p q row_id  div\n## 1: 1 2      2 0.97\n## 2: 1 3      3 0.64\n## 3: 2 1      4 0.87\n## 4: 2 3      6 2.01\n## 5: 3 1      7 0.63\n## 6: 3 2      8 1.84"},{"path":"week-4.html","id":"question-2-3","chapter":"4 Week 4","heading":"4.2 Question 2","text":"Recall marriage, age, happiness collider bias example Chapter\n6. Run models m6.9 m6.10 .","code":""},{"path":"week-4.html","id":"dag","chapter":"4 Week 4","heading":"4.2.1 DAG","text":"","code":"\ndag <- dagify(\n    marriage ~ happiness,\n    marriage ~ age,\n    exposure = 'age',\n    outcome = 'happiness'\n)\n\ndag_plot(dag)"},{"path":"week-4.html","id":"data","chapter":"4 Week 4","heading":"4.2.2 Data","text":"","code":"\nd <- sim_happiness(seed = 1977, N_years = 1e3)\n\nd2 <- d[d$age > 17,]\nd2$A <- (d2$age - 18) / (65 - 18)\nd2$mid <- d2$married + 1\n\nprecis(d2)##                          mean    sd   5.5% 94.5%  histogram\n## age       41.5000000000000000 13.86 20.000 63.00 ▃▇▇▇▇▇▇▇▇▇\n## married    0.4072916666666667  0.49  0.000  1.00 ▇▁▁▁▁▁▁▁▁▅\n## happiness -0.0000000000000001  1.21 -1.789  1.79   ▇▅▇▅▅▇▅▇\n## A          0.5000000000000000  0.29  0.043  0.96 ▇▇▇▅▇▇▅▇▇▇\n## mid        1.4072916666666666  0.49  1.000  2.00 ▇▁▁▁▁▁▁▁▁▅"},{"path":"week-4.html","id":"models","chapter":"4 Week 4","heading":"4.2.3 Models","text":"","code":"\nm6.9 <- quap(\n    alist(\n        happiness ~ dnorm(mu, sigma),\n        mu <- a[mid] + bA * A,\n        a[mid] ~ dnorm(0, 1),\n        bA ~ dnorm(0, 2),\n        sigma ~ dexp(1)\n    ), data = d2\n)\n\nprecis(m6.9, depth = 2)##        mean    sd  5.5% 94.5%\n## a[1]  -0.24 0.063 -0.34 -0.13\n## a[2]   1.26 0.085  1.12  1.39\n## bA    -0.75 0.113 -0.93 -0.57\n## sigma  0.99 0.023  0.95  1.03\nm6.10 <- quap(\n    alist(\n        happiness ~ dnorm(mu, sigma),\n        mu <- a + bA * A,\n        a ~ dnorm(0, 1),\n        bA ~ dnorm(0, 2),\n        sigma ~ dexp(1)\n    ), data = d2\n)\n\nprecis(m6.10, depth = 2)##              mean    sd  5.5% 94.5%\n## a      0.00000016 0.077 -0.12  0.12\n## bA    -0.00000027 0.132 -0.21  0.21\n## sigma  1.21318761 0.028  1.17  1.26"},{"path":"week-4.html","id":"interpretation-3","chapter":"4 Week 4","heading":"4.2.4 Interpretation","text":"Compare two models using WAIC (LOO, produce identical\nresults). model expected make better predictions? model\nprovides correct causal inference influence age happiness?\nCan explain answers two questions disagree?Model m6.9 includes marriage m6.10 . causal influence age\nhappiness confounded marriage marriage collider \nage happiness. Conditioning marriage opens path age \nhappiness, making age happiness independent. Therefore, despite WAIC \nm6.9 lower, tell us anything causation \nvariables.","code":"\nrethinking::compare(m6.9, m6.10)##       WAIC SE dWAIC dSE pWAIC weight\n## m6.9  2714 38     0  NA   3.7      1\n## m6.10 3102 28   388  35   2.3      0"},{"path":"week-4.html","id":"question-3-3","chapter":"4 Week 4","heading":"4.3 Question 3","text":"Reconsider urban fox analysis last week’s homework. Use WAIC LOO\nbased model comparison five different models, using weight \noutcome, containing sets predictor variables:avgfood + groupsize + areaavgfood + groupsizegroupsize + areaavgfoodarea","code":""},{"path":"week-4.html","id":"data-1","chapter":"4 Week 4","heading":"4.3.1 Data","text":"","code":"\ndata(foxes)"},{"path":"week-4.html","id":"models-1","chapter":"4 Week 4","heading":"4.3.2 Models","text":"","code":"\nfoxes$scale_area <- scale(foxes$area)\nfoxes$scale_weight <- scale(foxes$weight)\nfoxes$scale_avgfood <- scale(foxes$avgfood)\nfoxes$scale_groupsize <- scale(foxes$groupsize)\n\nm1 <- quap(\n    alist(\n        scale_weight ~ dnorm(mu, sigma),\n        mu <- a + bFood * scale_avgfood + bGroup * scale_groupsize + bArea * scale_area,\n        a ~ dnorm(0, 0.2),\n        bFood ~ dnorm(0, 0.5),\n        bGroup ~ dnorm(0, 0.5),\n        bArea ~ dnorm(0, 0.5),\n        sigma ~ dunif(0, 50)\n    ), \n    data = foxes\n)\n\nm2 <- quap(\n    alist(\n        scale_weight ~ dnorm(mu, sigma),\n        mu <- a + bFood * scale_avgfood + bGroup * scale_groupsize,\n        a ~ dnorm(0, 0.2),\n        bFood ~ dnorm(0, 0.5),\n        bGroup ~ dnorm(0, 0.5),\n        sigma ~ dunif(0, 50)\n    ), \n    data = foxes\n)\n\nm3 <- quap(\n    alist(\n        scale_weight ~ dnorm(mu, sigma),\n        mu <- a + bGroup * scale_groupsize + bArea * scale_area,\n        a ~ dnorm(0, 0.2),\n        bArea ~ dnorm(0, 0.5),\n        bGroup ~ dnorm(0, 0.5),\n        sigma ~ dunif(0, 50)\n    ), \n    data = foxes\n)\n\nm4 <- quap(\n    alist(\n        scale_weight ~ dnorm(mu, sigma),\n        mu <- a + bFood * scale_avgfood,\n        a ~ dnorm(0, 0.2),\n        bFood ~ dnorm(0, 0.5),\n        sigma ~ dunif(0, 50)\n    ), \n    data = foxes\n)\n\nm5 <- quap(\n    alist(\n        scale_weight ~ dnorm(mu, sigma),\n        mu <- a + bArea * scale_area,\n        a ~ dnorm(0, 0.2),\n        bArea ~ dnorm(0, 0.5),\n        sigma ~ dunif(0, 50)\n    ), \n    data = foxes\n)"},{"path":"week-4.html","id":"dag-1","chapter":"4 Week 4","heading":"4.3.3 DAG","text":"","code":"\ndag <- dagify(\n    weight ~ groupsize + avgfood,\n    groupsize ~ avgfood,\n    avgfood ~ area,\n    exposure = 'area',\n    outcome = 'weight'\n)\n\ndag_plot(dag)"},{"path":"week-4.html","id":"interpretation-4","chapter":"4 Week 4","heading":"4.3.4 Interpretation","text":"Can explain relative differences WAIC scores, using fox DAG\nlast week’s homework? sure pay attention standard error \nscore differences (dSE).weight ~ avgfood + groupsize + areaweight ~ avgfood + groupsizeweight ~ groupsize + areaweight ~ avgfoodweight ~ areaWeight outcome models. Looking DAG, see \npotential back door avgfood group size, colliders. Avgfood \npath area weight, groupsize avgfood weight. \npaths variable confounds shown DAG:Model 2: Weight ~ groupsize + avgfood (determine causal effect groupsize weight, including avgfood open collider)Model 4: Weight ~ avgfood (determine causal effect avgfood, without groupsize confusing relationship since ’s pipe)Model 5: Weight ~ area (avgfood groupsize excluded)Model 1 includes parameters , expected, highest model\nfit. dSE column returned compare function indicates standard\nerror difference models, @dSE slot showing \ncombinations models. Models 4 5 barely differ, likely \nstrong influence area average food. Including area avgfood \nlike conditioning intermediate treatment effect. Models 4 5 \ndifferent models 1, 2, 3. Models 1, 2, 3 groupsize \nWAIC coeftab, well DAG, indicate models \ninference.","code":"\ncompare_models <- rethinking::compare(m1, m2, m3, m4, m5)\ncompare_models##    WAIC SE dWAIC dSE pWAIC weight\n## m1  323 16  0.00  NA   4.6 0.4568\n## m2  324 16  0.97 3.6   3.7 0.2811\n## m3  324 16  1.15 2.9   3.8 0.2575\n## m4  333 14 10.56 7.1   2.4 0.0023\n## m5  334 14 10.66 7.2   2.6 0.0022\ncompare_models@dSE##     m1  m2  m3   m4   m5\n## m1  NA 3.6 2.9 7.13 7.19\n## m2 3.6  NA 5.8 6.55 6.79\n## m3 2.9 5.8  NA 6.54 6.59\n## m4 7.1 6.5 6.5   NA 0.84\n## m5 7.2 6.8 6.6 0.84   NA\n# Filled points: in-sample deviance\n# Open points: WAIC \n# Dark lines: standard error of WAIC\n# Light lines with triangles: standard error of difference in WAIC between each model and top model\nplot(compare_models)\ncoeftab(m1, m2, m3, m4, m5)##        m1      m2      m3      m4      m5     \n## a            0       0       0       0       0\n## bFood     0.30    0.48      NA   -0.02      NA\n## bGroup   -0.64   -0.57   -0.48      NA      NA\n## bArea     0.28      NA    0.41      NA    0.02\n## sigma     0.93    0.95    0.95    1.00    1.00\n## nobs       116     116     116     116     116"},{"path":"week-5.html","id":"week-5","chapter":"5 Week 5","heading":"5 Week 5","text":"2021-09-03 [updated: 2021-11-17]","code":""},{"path":"week-5.html","id":"question-1-4","chapter":"5 Week 5","heading":"5.1 Question 1","text":"Consider data(Wines2012) data table. data expert ratings 20\ndifferent French American wines 9 different French American judges.","code":""},{"path":"week-5.html","id":"data-2","chapter":"5 Week 5","heading":"5.1.1 Data","text":"goal model score, subjective rating assigned judge \nwine. recommend standardizing . first problem, consider \nvariation among judges wines. Construct index variables judge wine\nuse index variables construct linear regression model.","code":"\nDT <- data_wines()\n\nn_index_judge <- DT[, uniqueN(index_judge)]\nn_index_wine <- DT[, uniqueN(index_wine)]\nn_rows <- DT[, .N]"},{"path":"week-5.html","id":"prior-predictive-simulation-1","chapter":"5 Week 5","heading":"5.1.2 Prior predictive simulation","text":"Justify priors. end 9 judge parameters 20 wine\nparameters.Given parameters scaled, prior predictive plots show scaled wine scores mostly -2 2. Since prior knowledge relationship (positive/negative slopes), satisfied \nrelatively conservative prior.","code":"\nwriteLines(readLines(tar_read(stan_file_w05_q1_prior)))## data {\n##   int<lower=0> N;\n##   int<lower=0> N_judges;\n##   int<lower=0> N_wines;\n## }\n## parameters{\n##   real alpha;\n##   vector[N_judges] beta_judge;\n##   vector[N_wines] beta_wine;\n##   real<lower=0> sigma;\n## }\n## model{\n##   sigma ~ exponential(1);\n##   beta_wine ~ normal(0, 0.5);\n##   beta_judge ~ normal(0, 0.5);\n##   alpha ~ normal(0, 0.2);\n## }\nq1_prior_draws <- tar_read(stan_mcmc_w05_q1_prior)$draws()\n\nmcmc_areas(q1_prior_draws, regex_pars = 'judge')\nmcmc_areas(q1_prior_draws, regex_pars = 'wine')"},{"path":"week-5.html","id":"model-2","chapter":"5 Week 5","heading":"5.1.3 Model","text":"Use ulam instead quap build model, sure check\nchains convergence. ’d rather build model directly Stan \nPyMC3, go ahead. just want use Hamiltonian Monte Carlo instead \nquadratic approximation.interpret variation among individual judges individual\nwines? notice patterns, just plotting differences? \njudges gave highest/lowest ratings? wines rated worst/ best \naverage?Accounting judges, wines scored similar distributions. Wine 19\nhowever particularly poorly scored. Judges, however, much variable\nscoring three individuals (judges 1, 2, 5) entirely almost entirely\nscoring lower judges. worst scoring judge judge 5.","code":"\nwriteLines(readLines(tar_read(stan_file_w05_q1)))## data {\n##   int<lower=0> N;\n##   int<lower=0> N_judges;\n##   int<lower=0> N_wines;\n##   int index_judge[N];\n##   int index_wine[N];\n##   vector[N] scale_score;\n## }\n## parameters{\n##   real alpha;\n##   vector[N_judges] beta_judge;\n##   vector[N_wines] beta_wine;\n##   real<lower=0> sigma;\n## }\n## model{\n##   alpha ~ normal(0, 0.2);\n##   beta_wine ~ normal(0, 0.5);\n##   beta_judge ~ normal(0, 0.5);\n##   sigma ~ exponential(1);\n## \n##   vector[N] mu;\n##   mu = beta_wine[index_wine] + beta_judge[index_judge];\n##   scale_score ~ normal(mu, sigma);\n## }\nq1_draws <- tar_read(stan_mcmc_w05_q1)$draws()\nsetDT(q1_draws)\n# Judges\nprecis(q1_draws, depth = 2)[3:11,]##                mean   sd   5.5%  94.5%       histogram\n## beta_judge[1] -0.54 0.19 -0.861 -0.233 ▁▁▁▁▂▃▇▇▇▅▂▁▁▁▁\n## beta_judge[2] -0.34 0.19 -0.644 -0.038  ▁▁▁▁▂▃▇▇▇▅▂▁▁▁\n## beta_judge[3]  0.80 0.20  0.490  1.109        ▁▁▃▇▇▂▁▁\n## beta_judge[4]  0.13 0.20 -0.179  0.452        ▁▁▁▃▇▅▂▁\n## beta_judge[5] -0.65 0.19 -0.963 -0.343  ▁▁▁▁▂▃▇▇▇▃▂▁▁▁\n## beta_judge[6]  0.48 0.20  0.162  0.792        ▁▁▁▅▇▃▁▁\n## beta_judge[7] -0.28 0.20 -0.592  0.035  ▁▁▁▁▃▅▇▇▇▃▂▁▁▁\n## beta_judge[8]  0.21 0.20 -0.106  0.527       ▁▁▂▇▇▂▁▁▁\n## beta_judge[9]  0.21 0.19 -0.089  0.519       ▁▁▂▇▇▃▁▁▁\nmelt(q1_draws, measure.vars = patterns('beta_judge'))[, .(mean_score = mean(value)), variable][order(-mean_score)]##         variable mean_score\n## 1: beta_judge[3]       0.80\n## 2: beta_judge[6]       0.48\n## 3: beta_judge[9]       0.21\n## 4: beta_judge[8]       0.21\n## 5: beta_judge[4]       0.13\n## 6: beta_judge[7]      -0.28\n## 7: beta_judge[2]      -0.34\n## 8: beta_judge[1]      -0.54\n## 9: beta_judge[5]      -0.65\nmcmc_areas(q1_draws, regex_pars = 'judge')\n# Wines\nprecis(q1_draws, depth = 2)[12:31,]##                  mean   sd   5.5%  94.5%   histogram\n## beta_wine[1]   0.1175 0.25 -0.282  0.526  ▁▁▂▅▇▇▂▁▁▁\n## beta_wine[2]   0.2271 0.26 -0.187  0.630  ▁▁▁▃▇▇▅▂▁▁\n## beta_wine[3]  -0.1017 0.26 -0.514  0.314  ▁▁▂▅▇▅▂▁▁▁\n## beta_wine[4]   0.2406 0.26 -0.174  0.648  ▁▁▁▃▇▇▅▂▁▁\n## beta_wine[5]   0.0668 0.26 -0.351  0.477 ▁▁▁▂▇▇▅▂▁▁▁\n## beta_wine[6]  -0.0117 0.26 -0.431  0.407  ▁▁▁▃▇▇▃▁▁▁\n## beta_wine[7]  -0.0901 0.26 -0.515  0.302  ▁▁▂▅▇▇▂▁▁▁\n## beta_wine[8]  -0.1886 0.26 -0.608  0.232  ▁▁▁▃▇▇▃▁▁▁\n## beta_wine[9]  -0.1196 0.24 -0.516  0.271   ▁▁▂▅▇▅▂▁▁\n## beta_wine[10] -0.1401 0.26 -0.554  0.273 ▁▁▁▁▂▇▇▅▂▁▁\n## beta_wine[11]  0.0858 0.26 -0.329  0.509 ▁▁▁▂▅▇▅▂▁▁▁\n## beta_wine[12]  0.4641 0.26  0.035  0.869 ▁▁▁▂▅▇▅▂▁▁▁\n## beta_wine[13] -0.3145 0.26 -0.716  0.091 ▁▁▁▂▅▇▅▂▁▁▁\n## beta_wine[14]  0.2234 0.27 -0.200  0.642  ▁▁▁▃▇▇▅▂▁▁\n## beta_wine[15]  0.0963 0.26 -0.311  0.515 ▁▁▁▂▅▇▅▂▁▁▁\n## beta_wine[16] -0.0234 0.26 -0.424  0.389 ▁▁▁▁▅▇▇▃▁▁▁\n## beta_wine[17]  0.0051 0.26 -0.420  0.432  ▁▁▁▃▇▇▃▁▁▁\n## beta_wine[18] -0.1674 0.26 -0.591  0.261  ▁▁▁▃▇▇▅▂▁▁\n## beta_wine[19] -0.7246 0.26 -1.137 -0.303  ▁▁▁▂▇▇▅▂▁▁\n## beta_wine[20]  0.3215 0.25 -0.068  0.717  ▁▁▁▂▅▇▇▂▁▁\nmelt(q1_draws, measure.vars = patterns('beta_wine'))[, .(mean_score = mean(value)), variable][order(-mean_score)]##          variable mean_score\n##  1: beta_wine[12]     0.4641\n##  2: beta_wine[20]     0.3215\n##  3:  beta_wine[4]     0.2406\n##  4:  beta_wine[2]     0.2271\n##  5: beta_wine[14]     0.2234\n##  6:  beta_wine[1]     0.1175\n##  7: beta_wine[15]     0.0963\n##  8: beta_wine[11]     0.0858\n##  9:  beta_wine[5]     0.0668\n## 10: beta_wine[17]     0.0051\n## 11:  beta_wine[6]    -0.0117\n## 12: beta_wine[16]    -0.0234\n## 13:  beta_wine[7]    -0.0901\n## 14:  beta_wine[3]    -0.1017\n## 15:  beta_wine[9]    -0.1196\n## 16: beta_wine[10]    -0.1401\n## 17: beta_wine[18]    -0.1674\n## 18:  beta_wine[8]    -0.1886\n## 19: beta_wine[13]    -0.3145\n## 20: beta_wine[19]    -0.7246\nmcmc_areas(q1_draws, regex_pars = 'wine')"},{"path":"week-5.html","id":"question-2-4","chapter":"5 Week 5","heading":"5.2 Question 2","text":"Now consider three features wines judges: (1) flight: Whether wine red white. (2) wine.amer: Indicator variable American wines. (3) judge.amer: Indicator variable American judges. Use indicator index variables model influence features scores. Omit individual judge wine index variables Problem\n1. include interaction effects yet. use ulam, justify priors,\nsure check chains.","code":""},{"path":"week-5.html","id":"data-3","chapter":"5 Week 5","heading":"5.2.1 Data","text":"","code":"\nDT <- data_wines()\n\nn_index_flight <- DT[, uniqueN(flight)]\nn_index_wine_american <- DT[, uniqueN(index_wine_american)]\nn_index_judge_american <- DT[, uniqueN(index_judge_american)]\nn_rows <- DT[, .N]\n\nq2_data <- c(\n    as.list(DT[, .(scale_score = as.numeric(scale_score),\n                                 index_flight,\n                                 index_wine_american,\n                                 index_judge_american)]),\n    N_flights = n_index_flight,\n    N_wine_american = n_index_wine_american,\n    N_judge_american = n_index_judge_american,\n    N = n_rows\n)"},{"path":"week-5.html","id":"priors","chapter":"5 Week 5","heading":"5.2.2 Priors","text":"","code":"\nwriteLines(readLines(tar_read(stan_file_w05_q2_prior)))## data {\n##   int<lower=0> N;\n##   int<lower=0> N_flights;\n##   int<lower=0> N_wine_american;\n##   int<lower=0> N_judge_american;\n## }\n## parameters{\n##   real alpha;\n##   vector[N_flights] beta_flights;\n##   vector[N_wine_american] beta_wine_american;\n##   vector[N_judge_american] beta_judge_american;\n##   real<lower=0> sigma;\n## }\n## model{\n##   alpha ~ normal(0, 0.2);\n##   beta_flights ~ normal(0, 0.5);\n##   beta_wine_american ~ normal(0, 0.5);\n##   beta_judge_american ~ normal(0, 0.5);\n##   sigma ~ exponential(1);\n## }\nq2_prior_draws <- tar_read(stan_mcmc_w05_q2_prior)$draws()\n\nmcmc_areas(q2_prior_draws, regex_pars = 'flights')\nmcmc_areas(q2_prior_draws, regex_pars = 'judge')\nmcmc_areas(q2_prior_draws, regex_pars = 'wine')"},{"path":"week-5.html","id":"model-3","chapter":"5 Week 5","heading":"5.2.3 Model","text":"","code":"\nwriteLines(readLines(tar_read(stan_file_w05_q2)))## data {\n##   int<lower=0> N;\n##   int<lower=0> N_flights;\n##   int<lower=0> N_wine_american;\n##   int<lower=0> N_judge_american;\n## \n##   int index_flight[N];\n##   int index_wine_american[N];\n##   int index_judge_american[N];\n## \n##   vector[N] scale_score;\n## }\n## parameters{\n##   real alpha;\n##   vector[N_flights] beta_flights;\n##   vector[N_wine_american] beta_wine_american;\n##   vector[N_judge_american] beta_judge_american;\n##   real<lower=0> sigma;\n## }\n## model{\n##   alpha ~ normal(0, 0.2);\n##   beta_flights ~ normal(0, 0.5);\n##   beta_wine_american ~ normal(0, 0.5);\n##   beta_judge_american ~ normal(0, 0.5);\n##   sigma ~ exponential(1);\n## \n##   vector[N] mu;\n##   mu = beta_flights[index_flight] + beta_wine_american[index_wine_american] + beta_judge_american[index_judge_american];\n##   scale_score ~ normal(mu, sigma);\n## }\nq2 <- tar_read(stan_mcmc_w05_q2)\nq2_draws <- q2$draws()\n\nq2$summary()##  [90m# A tibble: 9 × 10 [39m\n##   variable            mean   median     sd    mad      q5     q95  rhat ess_bulk\n##    [3m [90m<chr> [39m [23m               [3m [90m<dbl> [39m [23m     [3m [90m<dbl> [39m [23m   [3m [90m<dbl> [39m [23m   [3m [90m<dbl> [39m [23m    [3m [90m<dbl> [39m [23m    [3m [90m<dbl> [39m [23m  [3m [90m<dbl> [39m [23m     [3m [90m<dbl> [39m [23m\n##  [90m1 [39m lp__            - [31m9 [39m [31m. [39m [31m24 [39m [90me [39m+1 - [31m9 [39m [31m. [39m [31m20 [39m [90me [39m+1 2.04   1.92   - [31m96 [39m [31m. [39m [31m2 [39m   - [31m89 [39m [31m. [39m [31m7 [39m    1.00     [4m1 [24m746.\n##  [90m2 [39m alpha            8.61 [90me [39m [31m-4 [39m - [31m1 [39m [31m. [39m [31m84 [39m [90me [39m [31m-4 [39m 0.198  0.196   - [31m0 [39m [31m. [39m [31m312 [39m   0.336  1.00     [4m3 [24m101.\n##  [90m3 [39m beta_flights[…  - [31m2 [39m [31m. [39m [31m0 [39m [31m4 [39m [90me [39m [31m-3 [39m - [31m9 [39m [31m. [39m [31m90 [39m [90me [39m [31m-4 [39m 0.297  0.295   - [31m0 [39m [31m. [39m [31m501 [39m   0.484  1.00     [4m1 [24m876.\n##  [90m4 [39m beta_flights[…  - [31m5 [39m [31m. [39m [31m52 [39m [90me [39m [31m-3 [39m - [31m1 [39m [31m. [39m [31m0 [39m [31m4 [39m [90me [39m [31m-2 [39m 0.298  0.303   - [31m0 [39m [31m. [39m [31m483 [39m   0.489  1.00     [4m1 [24m929.\n##  [90m5 [39m beta_wine_ame…  - [31m8 [39m [31m. [39m [31m69 [39m [90me [39m [31m-2 [39m - [31m8 [39m [31m. [39m [31m85 [39m [90me [39m [31m-2 [39m 0.303  0.302   - [31m0 [39m [31m. [39m [31m575 [39m   0.403  1.00     [4m1 [24m730.\n##  [90m6 [39m beta_wine_ame…   9.67 [90me [39m [31m-2 [39m  1.02 [90me [39m [31m-1 [39m 0.301  0.302   - [31m0 [39m [31m. [39m [31m406 [39m   0.589  1.00     [4m1 [24m847.\n##  [90m7 [39m beta_judge_am…  - [31m1 [39m [31m. [39m [31m16 [39m [90me [39m [31m-1 [39m - [31m1 [39m [31m. [39m [31m14 [39m [90me [39m [31m-1 [39m 0.296  0.295   - [31m0 [39m [31m. [39m [31m590 [39m   0.361  1.00     [4m1 [24m864.\n##  [90m8 [39m beta_judge_am…   1.21 [90me [39m [31m-1 [39m  1.20 [90me [39m [31m-1 [39m 0.291  0.290   - [31m0 [39m [31m. [39m [31m347 [39m   0.602  1.00     [4m1 [24m762.\n##  [90m9 [39m sigma            1.00 [90me [39m+0  9.99 [90me [39m [31m-1 [39m 0.053 [4m4 [24m 0.052 [4m9 [24m   0.918   1.09   1.00     [4m3 [24m052.\n##  [90m# … with 1 more variable: ess_tail <dbl> [39m\nmcmc_trace(q2_draws)\n# Recall: \nDT[, .N, .(flight, index_flight)]##    flight index_flight  N\n## 1:  white            1 90\n## 2:    red            2 90\nDT[, .N, .(judge.amer, index_judge_american)]##    judge.amer index_judge_american   N\n## 1:          0                    1  80\n## 2:          1                    2 100\nDT[, .N, .(wine.amer, index_wine_american)]##    wine.amer index_wine_american   N\n## 1:         1                   1 108\n## 2:         0                   2  72\nlabs <- c(\n    'beta_flights[1]' = 'White Wine',\n    'beta_flights[2]' = 'Red Wine',\n    'beta_wine_american[1]' = 'French Wine',\n    'beta_wine_american[2]' = 'American Wine',\n    'beta_judge_american[1]' = 'American Judge',\n    'beta_judge_american[2]' = 'French Judge'\n)\nmcmc_areas(q2_draws, regex_pars = 'flight') + scale_y_discrete(labels = labs)## Scale for 'y' is already present. Adding another scale for 'y', which will\n## replace the existing scale.\nmcmc_areas(q2_draws, regex_pars = 'judge') + scale_y_discrete(labels = labs)## Scale for 'y' is already present. Adding another scale for 'y', which will\n## replace the existing scale.\nmcmc_areas(q2_draws, regex_pars = 'wine') + scale_y_discrete(labels = labs)## Scale for 'y' is already present. Adding another scale for 'y', which will\n## replace the existing scale."},{"path":"week-5.html","id":"question-3-4","chapter":"5 Week 5","heading":"5.3 Question 3","text":"Now consider two-way interactions among three features. end \nthree different interaction terms model. easier \nbuild, use indicator variables. use ulam, justify priors, \nsure check chains. Explain interaction means. sure \ninterpret model’s predictions outcome scale (mu, expected score),\nscale individual parameters. can use link help ,\njust use knowledge linear model instead. conclude\nfeatures scores? Can relate results model(s)\nindividual judge wine inferences Problem 1?","code":""},{"path":"week-5.html","id":"data-4","chapter":"5 Week 5","heading":"5.3.1 Data","text":"","code":"\nDT <- data_wines()\nn_index_interactions <- DT[, uniqueN(index_interactions)]\n\nq3_data <- c(\n    as.list(DT[, .(scale_score = as.numeric(scale_score),\n                                 index_interactions)]),\n    N_interactions = n_index_interactions,\n    N = n_rows\n)"},{"path":"week-5.html","id":"model-4","chapter":"5 Week 5","heading":"5.3.2 Model","text":"","code":"\nwriteLines(readLines(tar_read(stan_file_w05_q3)))## data {\n##   int<lower=0> N;\n##   int<lower=0> N_interactions;\n## \n##   int index_interactions[N];\n## \n##   vector[N] scale_score;\n## }\n## parameters{\n##   real alpha;\n##   vector[N_interactions] beta_interactions;\n##   real<lower=0> sigma;\n## }\n## model{\n##   alpha ~ normal(0, 0.2);\n##   beta_interactions ~ normal(0, 0.25);\n##   sigma ~ exponential(1);\n## \n##   vector[N] mu;\n##   mu = alpha + beta_interactions[index_interactions];\n## \n##   scale_score ~ normal(mu, sigma);\n## }\nq3 <- tar_read(stan_mcmc_w05_q3)\nq3_draws <- q3$draws()\n\nq3$summary()##  [90m# A tibble: 11 × 10 [39m\n##    variable          mean   median     sd    mad      q5      q95  rhat ess_bulk\n##     [3m [90m<chr> [39m [23m             [3m [90m<dbl> [39m [23m     [3m [90m<dbl> [39m [23m   [3m [90m<dbl> [39m [23m   [3m [90m<dbl> [39m [23m    [3m [90m<dbl> [39m [23m     [3m [90m<dbl> [39m [23m  [3m [90m<dbl> [39m [23m     [3m [90m<dbl> [39m [23m\n##  [90m 1 [39m lp__          - [31m9 [39m [31m. [39m [31m25 [39m [90me [39m+1 - [31m9 [39m [31m. [39m [31m22 [39m [90me [39m+1 2.23   2.07   - [31m96 [39m [31m. [39m [31m7 [39m   - [31m89 [39m [31m. [39m [31m5 [39m     1.00     [4m1 [24m640.\n##  [90m 2 [39m alpha          1.86 [90me [39m [31m-3 [39m  1.15 [90me [39m [31m-3 [39m 0.101  0.099 [4m1 [24m  - [31m0 [39m [31m. [39m [31m164 [39m   0.168   1.00     [4m3 [24m164.\n##  [90m 3 [39m beta_interac…  2.58 [90me [39m [31m-2 [39m  2.68 [90me [39m [31m-2 [39m 0.176  0.174   - [31m0 [39m [31m. [39m [31m266 [39m   0.317   1.00     [4m5 [24m526.\n##  [90m 4 [39m beta_interac… - [31m1 [39m [31m. [39m [31m91 [39m [90me [39m [31m-1 [39m - [31m1 [39m [31m. [39m [31m90 [39m [90me [39m [31m-1 [39m 0.185  0.183   - [31m0 [39m [31m. [39m [31m503 [39m   0.113   1.00     [4m6 [24m850.\n##  [90m 5 [39m beta_interac…  2.91 [90me [39m [31m-2 [39m  2.83 [90me [39m [31m-2 [39m 0.158  0.161   - [31m0 [39m [31m. [39m [31m228 [39m   0.289   1.00     [4m4 [24m941.\n##  [90m 6 [39m beta_interac…  1.10 [90me [39m [31m-1 [39m  1.08 [90me [39m [31m-1 [39m 0.177  0.179   - [31m0 [39m [31m. [39m [31m183 [39m   0.402   1.00     [4m5 [24m471.\n##  [90m 7 [39m beta_interac…  1.26 [90me [39m [31m-1 [39m  1.27 [90me [39m [31m-1 [39m 0.179  0.178   - [31m0 [39m [31m. [39m [31m174 [39m   0.421   1.00     [4m6 [24m705.\n##  [90m 8 [39m beta_interac… - [31m2 [39m [31m. [39m [31m52 [39m [90me [39m [31m-1 [39m - [31m2 [39m [31m. [39m [31m54 [39m [90me [39m [31m-1 [39m 0.166  0.167   - [31m0 [39m [31m. [39m [31m524 [39m   0.020 [4m8 [24m  1.00     [4m6 [24m633.\n##  [90m 9 [39m beta_interac…  1.76 [90me [39m [31m-1 [39m  1.77 [90me [39m [31m-1 [39m 0.174  0.173   - [31m0 [39m [31m. [39m [31m105 [39m   0.461   1.00     [4m6 [24m188.\n##  [90m10 [39m beta_interac… - [31m1 [39m [31m. [39m [31m45 [39m [90me [39m [31m-2 [39m - [31m1 [39m [31m. [39m [31m48 [39m [90me [39m [31m-2 [39m 0.164  0.165   - [31m0 [39m [31m. [39m [31m283 [39m   0.261   1.00     [4m4 [24m721.\n##  [90m11 [39m sigma          9.91 [90me [39m [31m-1 [39m  9.90 [90me [39m [31m-1 [39m 0.052 [4m0 [24m 0.053 [4m5 [24m   0.909   1.08    1.00     [4m8 [24m835.\n##  [90m# … with 1 more variable: ess_tail <dbl> [39m\nmcmc_trace(q3_draws)\nDT[, judge_char := ifelse(judge.amer == 0, 'French Judge', 'American Judge')]\nDT[, wine_char := ifelse(wine.amer == 0, 'French Wine', 'American Wine')]\n\nlabs <- DT[, .(.GRP, paste(.BY, collapse = ', ')), by = .(wine_char, judge_char, as.character(flight))][, .(GRP, V2)]\nlabs <- setNames(labs$V2, paste0('beta_interactions[', labs$GRP, ']'))\nmcmc_areas(q3_draws, regex_pars = 'interaction') + scale_y_discrete(labels = labs)## Scale for 'y' is already present. Adding another scale for 'y', which will\n## replace the existing scale."},{"path":"week-6.html","id":"week-6","chapter":"6 Week 6","heading":"6 Week 6","text":"2021-09-06 [updated: 2021-11-17]","code":""},{"path":"week-6.html","id":"question-1-5","chapter":"6 Week 6","heading":"6.1 Question 1","text":"data data(NWOGrants) outcomes scientific funding applications\nNetherlands Organization Scientific Research (NWO) 2010–2012\n(see van der Lee Ellemers doi:10.1073/pnas.1510159112). data \nsimilar structure UCBAdmit data discussed Chapter 11. want \nconsider similar question: total indirect causal effects\ngender grant awards? Consider mediation path (pipe) dis-\ncipline. Draw corresponding DAG use one binomial GLMs \nanswer question.","code":""},{"path":"week-6.html","id":"data-5","chapter":"6 Week 6","heading":"6.1.1 Data","text":"Discipline: factor 9 levelsGender: factor 2 levels data (…)Applications: countAwards: count","code":"\nDT <- data_grants()\n\nprecis(DT)##                   mean     sd 5.5% 94.5%     histogram\n## discipline         NaN     NA   NA    NA              \n## gender             NaN     NA   NA    NA              \n## applications     156.8 119.52 37.0   410     ▅▇▅▅▃▂▁▁▃\n## awards            25.9  15.95  8.5    48 ▂▅▅▇▂▇▅▂▁▅▁▁▂\n## index_gender       1.5   0.51  1.0     2    ▇▁▁▁▁▁▁▁▁▇\n## index_discipline   5.0   2.66  1.0     9      ▇▃▃▃▃▃▃▃\nsummary(DT)##                discipline gender  applications     awards    index_gender\n##  Chemical sciences  :2    f:9    Min.   :  9   Min.   : 2   Min.   :1.0  \n##  Earth/life sciences:2    m:9    1st Qu.: 70   1st Qu.:14   1st Qu.:1.0  \n##  Humanities         :2           Median :130   Median :24   Median :1.5  \n##  Interdisciplinary  :2           Mean   :157   Mean   :26   Mean   :1.5  \n##  Medical sciences   :2           3rd Qu.:220   3rd Qu.:33   3rd Qu.:2.0  \n##  Physical sciences  :2           Max.   :425   Max.   :65   Max.   :2.0  \n##  (Other)            :6                                                   \n##  index_discipline\n##  Min.   :1       \n##  1st Qu.:3       \n##  Median :5       \n##  Mean   :5       \n##  3rd Qu.:7       \n##  Max.   :9       \n## \nq1_data <- c(\n    as.list(DT[, .(awards, applications, index_gender, index_discipline)]),\n    N = DT[, .N],\n    N_gender = DT[, uniqueN(index_gender)],\n    N_discipline = DT[, uniqueN(index_discipline)]\n)"},{"path":"week-6.html","id":"dag-2","chapter":"6 Week 6","heading":"6.1.2 DAG","text":"","code":"\ndag <- dagify(\n    awards ~ index_gender + index_discipline,\n    index_discipline ~ index_gender,\n    exposure = 'index_gender',\n    outcome = 'awards'\n)\n\ndag_plot(dag)"},{"path":"week-6.html","id":"priors-1","chapter":"6 Week 6","heading":"6.1.3 Priors","text":"","code":"\nwriteLines(readLines(tar_read(stan_b_file_w06_q1_prior)))## data {\n##   int<lower=0> N;\n##   int<lower=0> N_gender;\n##   int<lower=0> N_discipline;\n## }\n## parameters{\n##   real alpha;\n##   vector[N_gender] beta_gender;\n##   vector[N_discipline] beta_discipline;\n##   real<lower=0> sigma;\n##   real<lower=0,upper=1> theta;\n## }\n## model{\n##   alpha ~ normal(0, 0.2);\n##   beta_gender ~ normal(0, 0.25);\n##   beta_discipline ~ normal(0, 0.25);\n##   sigma ~ exponential(1);\n##   theta ~ beta(1, 1);\n## }\nq1_prior_draws <- tar_read(stan_b_mcmc_w06_q1_prior)$draws()\n\nmcmc_areas(q1_prior_draws, regex_pars = 'theta')\nmcmc_areas(q1_prior_draws, regex_pars = 'sigma')\nmcmc_areas(q1_prior_draws, regex_pars = 'beta_gender')\nmcmc_areas(q1_prior_draws, regex_pars = 'beta_discipline')"},{"path":"week-6.html","id":"model-5","chapter":"6 Week 6","heading":"6.1.4 Model","text":"disciplineWhat causal interpretation? NWO’s goal equalize rates \nfunding genders, type intervention effective?Investigate departmental levels, since included relative differences small.","code":"\nwriteLines(readLines(tar_read(stan_b_file_w06_q1)))## data {\n##   int<lower=0> N;\n##   int<lower=0> N_gender;\n##   int awards[N];\n##   int applications [N];\n##   int index_gender[N];\n## }\n## parameters{\n##   vector[N_gender] alpha;\n## }\n## model{\n##   vector[N] p;\n##   alpha ~ normal(-1, 1);\n## \n##   p = inv_logit(alpha[index_gender]);\n##   awards ~ binomial(applications, p);\n## }\nq1_draws <- tar_read(stan_b_mcmc_w06_q1)$draws()\n\nmcmc_areas(q1_draws, regex_pars = 'alpha')\nq1_draws$dif_alpha <- (inv.logit(q1_draws$`alpha[1]`) -\n    inv.logit(q1_draws$`alpha[2]`)) * 100\nmcmc_areas(q1_draws, regex_pars = 'dif_alpha')\nwriteLines(readLines(tar_read(stan_b_file_w06_q1_discipline)))## data {\n##   int<lower=0> N;\n##   int<lower=0> N_gender;\n##   int<lower=0> N_discipline;\n##   int awards[N];\n##   int applications [N];\n##   int index_gender[N];\n##   int index_discipline[N];\n## }\n## parameters{\n##   vector[N_gender] alpha;\n##   vector[N_discipline] beta;\n## }\n## model{\n##   vector[N] p;\n##   alpha ~ normal(-1, 1);\n##   beta ~ normal(0, 0.25);\n## \n##   p = inv_logit(alpha[index_gender] + beta[index_discipline]);\n##   awards ~ binomial(applications, p);\n## }\nq1_discipline_draws <- tar_read(stan_b_mcmc_w06_q1_discipline)$draws()\n\nmcmc_areas(q1_discipline_draws, regex_pars = 'alpha')\nmcmc_areas(q1_discipline_draws, regex_pars = 'beta')\n# Need to account for base rates to look at absolute rate\n# q1_draws$dif_alpha <- (inv.logit(q1_draws$`alpha[1]`) -\n#   inv.logit(q1_draws$`alpha[2]`)) * 100\n# mcmc_areas(q1_draws, regex_pars = 'dif_alpha')\n\n# We can look at relative rates though\nq1_discipline_draws$dif_alpha_rel <- q1_discipline_draws$`alpha[1]` - q1_discipline_draws$`alpha[2]`\nmcmc_areas(q1_discipline_draws, regex_pars = 'dif_alpha_rel')"},{"path":"week-7.html","id":"week-7","chapter":"7 Week 7","heading":"7 Week 7","text":"2021-09-07 [updated: 2021-11-17]","code":""},{"path":"week-7.html","id":"data-6","chapter":"7 Week 7","heading":"7.0.1 Data","text":"Response: 1-7 integer, “morally permissible action taken () ”. Categorical, ordered, distances categories metric known.Logit = log-odds, cumulative logit = log-cumulative-odds. constrained 0-1.Log-cumulative-odds response 7 infinity since log(1/(1-1)) = infinity.\nGiven , need K-1 = 6 intercepts.","code":"\nDT <- data_trolley()\nprecis(DT)##              mean    sd 5.5% 94.5%      histogram\n## case          NaN    NA   NA    NA               \n## response     4.20  1.91    1     7   ▃▂▁▃▁▇▁▅▁▅▁▅\n## order       16.50  9.29    2    31        ▇▅▇▇▅▇▂\n## id            NaN    NA   NA    NA               \n## age         37.49 14.23   18    61  ▂▇▅▇▅▇▅▅▃▅▂▁▁\n## male         0.57  0.49    0     1     ▅▁▁▁▁▁▁▁▁▇\n## edu           NaN    NA   NA    NA               \n## action       0.43  0.50    0     1     ▇▁▁▁▁▁▁▁▁▅\n## intention    0.47  0.50    0     1     ▇▁▁▁▁▁▁▁▁▇\n## contact      0.20  0.40    0     1     ▇▁▁▁▁▁▁▁▁▂\n## story         NaN    NA   NA    NA               \n## action2      0.63  0.48    0     1     ▃▁▁▁▁▁▁▁▁▇\n## education    5.73  1.35    3     8 ▁▁▁▁▁▂▁▅▁▇▁▂▁▂\n## individual 166.00 95.56   19   313        ▇▇▇▇▇▇▅"},{"path":"week-7.html","id":"model-ordered-categorical-outcome","chapter":"7 Week 7","heading":"7.0.2 Model: ordered categorical outcome","text":"Probability data: \\(R_{} \\sim \\text{Ordered-logit}(\\phi_{}, K)\\)Linear model: \\(\\phi_{} = 0\\)Prior intercept: \\(K_{k} \\sim \\text{Normal}(0, 1.5)\\)Interactions","code":"\nwriteLines(readLines(tar_read(stan_c_file_w07_model1)))## data {\n##   int N;\n##   int K;\n##   int response[N];\n##   int action[N];\n##   int intention[N];\n##   int contact[N];\n## }\n## parameters {\n##  // Cut points are the positions of responses along cumulative odds\n##   ordered[K] cutpoints;\n##   real beta_action;\n##   real beta_intention;\n##   real beta_contact;\n## }\n## model {\n##   vector[N] phi;\n## \n##  for (i in 1:N) {\n##      phi[i] = beta_action * action[i] + beta_contact * contact[i] + beta_intention * intention[i];\n##      response[i] ~ ordered_logistic(phi[i], cutpoints);\n##  }\n## \n##   cutpoints ~ normal(0, 1.5);\n##   beta_action ~ normal(0, 0.5);\n##   beta_contact ~ normal(0, 0.5);\n##   beta_intention ~ normal(0, 0.5);\n## }\nmodel1_draws <- tar_read(stan_c_mcmc_w07_model1)$draws()\n\nmcmc_areas(model1_draws, regex_pars = 'beta')\nwriteLines(readLines(tar_read(stan_c_file_w07_model1_interactions)))## data {\n##   int N;\n##   int K;\n##   int response[N];\n##   int action[N];\n##   int intention[N];\n##   int contact[N];\n## }\n## parameters {\n##  // Cut points are the positions of responses along cumulative odds\n##   ordered[K] cutpoints;\n##   real beta_action;\n##   real beta_intention;\n##   real beta_contact;\n##   real beta_intention_contact;\n##   real beta_intention_action;\n## }\n## model {\n##   vector[N] phi;\n## \n##  for (i in 1:N) {\n##      phi[i] = beta_action * action[i] + beta_contact * contact[i] + beta_intention * intention[i] + beta_intention_contact * intention[i] * contact[i]  + beta_intention_action * intention[i] * action[i]\n##      ;\n##      response[i] ~ ordered_logistic(phi[i], cutpoints);\n##  }\n## \n##   cutpoints ~ normal(0, 1.5);\n##   beta_action ~ normal(0, 0.5);\n##   beta_contact ~ normal(0, 0.5);\n##   beta_intention ~ normal(0, 0.5);\n## }\nmodel1_interactions_draws <- tar_read(stan_c_mcmc_w07_model1_interactions)$draws()\n\nmcmc_areas(model1_interactions_draws, regex_pars = 'beta')\nmcmc_areas(model1_interactions_draws, regex_pars = 'cut', transformations = inv.logit) + xlim(0, 1)## Scale for 'x' is already present. Adding another scale for 'x', which will\n## replace the existing scale."},{"path":"week-7.html","id":"model-ordered-categorical-predictors","chapter":"7 Week 7","heading":"7.0.3 Model: ordered categorical predictors","text":"Probability data: \\(R_{} \\sim \\text{Ordered-logit}(\\phi_{}, K)\\)Linear model: \\(\\phi_{} = \\beta_{E} \\sum_{j=0}^{E_{}-1}\\delta_{j} + \\beta{A_{}}A_{} + \\beta{I_{}}I_{} + \\beta{C_{}}C_{}\\)Prior intercept: \\(K_{k} \\sim \\text{Normal}(0, 1.5)\\)\nPrior \\(\\beta\\): \\(\\beta_{}, \\beta_{}, \\beta_{C}, \\beta_{E}K_{k} \\sim \\text{Normal}(0, 1)\\)\nPrior \\(\\delta\\) vector: \\(\\delta \\sim \\text{Dirichlet}(\\alpha)\\)Dirichlet distribution = multivariate extension beta distribution. Probabilities zero one, sum one. parameterized \npseudo-counts observations.intercept takes first category, add 0 sequence ofMake sure reorder education levels","code":"\nwriteLines(readLines(tar_read(stan_c_file_w07_model2)))## data {\n##   int N;\n##   int K;\n##   int N_edu;\n##   int response[N];\n##   int action[N];\n##   int intention[N];\n##   int contact[N];\n##   int education[N];\n##   vector[N_edu - 1] alpha;\n## }\n## parameters {\n##  // Cut points are the positions of responses along cumulative odds\n##   ordered[K] cutpoints;\n##   real beta_action;\n##   real beta_intention;\n##   real beta_contact;\n##   real beta_education;\n## \n##   // Vector N reals that sum to 1\n##   simplex[7] delta;\n## }\n## model {\n##   vector[N] phi;\n##   vector[N_edu] delta_j;\n## \n##   delta ~ dirichlet(alpha);\n##   delta_j = append_row(0, delta);\n## \n##  for (i in 1:N) {\n##     // add beta education  * sum delta j, up to current i's education\n##     phi[i] = beta_education * sum(delta_j[1:education[i]]) +\n##       beta_action * action[i] +\n##       beta_contact * contact[i] +\n##       beta_intention * intention[i];\n##     response[i] ~ ordered_logistic(phi[i], cutpoints);\n##  }\n## \n##   cutpoints ~ normal(0, 1.5);\n##   beta_action ~ normal(0, 1);\n##   beta_contact ~ normal(0, 1);\n##   beta_intention ~ normal(0, 1);\n##   beta_education ~ normal(0, 1);\n## }\nmodel2_draws <- tar_read(stan_c_mcmc_w07_model2)$draws()\n\nmcmc_areas(model2_draws, regex_pars = 'beta')\nmcmc_areas(model2_draws, regex_pars = 'delta')\nmcmc_areas(model2_draws, regex_pars = 'cut', transformations = inv.logit) + xlim(0, 1)## Scale for 'x' is already present. Adding another scale for 'x', which will\n## replace the existing scale."},{"path":"week-7.html","id":"question-1-6","chapter":"7 Week 7","heading":"7.1 Question 1","text":"Trolley data — data(Trolley) — saw education level (modeled \nordered category) associated responses. association causal? One\nplausible confound education also associated age, \ncausal process: People older finish school begin\n. Reconsider Trolley data light. Draw DAG represents\nhypothetical causal relationships among response, education, age.statistical model models need evaluate causal influence \neducation responses? Fit models trolley data. \nconclude causal relationships among three variables?","code":"\ndag <- dagify(\n  response ~ education + age + action + intention + contact,\n  education ~ age,\n  contact ~ action,\n  exposure = 'education',\n  outcome = 'response'\n)\n\ndag_plot(dag)\nadjustmentSets(dag, exposure = 'education', outcome = 'response', effect = 'total' )## { age }\nwriteLines(readLines(tar_read(stan_c_file_w07_model2_age)))## data {\n##   int N;\n##   int K;\n##   int N_edu;\n##   int response[N];\n##   int action[N];\n##   int intention[N];\n##   int contact[N];\n##   int education[N];\n##   real age[N];\n##   vector[N_edu - 1] alpha;\n## }\n## parameters {\n##  // Cut points are the positions of responses along cumulative odds\n##   ordered[K] cutpoints;\n##   real beta_action;\n##   real beta_intention;\n##   real beta_contact;\n##   real beta_education;\n##   real beta_age;\n## \n##   // Vector N reals that sum to 1\n##   simplex[7] delta;\n## }\n## model {\n##   vector[N] phi;\n##   vector[N_edu] delta_j;\n## \n##   delta ~ dirichlet(alpha);\n##   delta_j = append_row(0, delta);\n## \n##  for (i in 1:N) {\n##     // add beta education  * sum delta j, up to current i's education\n##     phi[i] = beta_education * sum(delta_j[1:education[i]]) +\n##       beta_action * action[i] +\n##       beta_contact * contact[i] +\n##       beta_age * age[i] +\n##       beta_intention * intention[i];\n##     response[i] ~ ordered_logistic(phi[i], cutpoints);\n##  }\n## \n##   cutpoints ~ normal(0, 1.5);\n##   beta_action ~ normal(0, 1);\n##   beta_contact ~ normal(0, 1);\n##   beta_intention ~ normal(0, 1);\n##   beta_education ~ normal(0, 1);\n##   beta_age ~ normal(0, 1);\n## }\nmodel2_age_draws <- tar_read(stan_c_mcmc_w07_model2_age)$draws()\n\nmcmc_areas(model2_age_draws, regex_pars = 'beta')\nmcmc_areas(model2_age_draws, regex_pars = 'delta')\nmcmc_areas(model2_age_draws, regex_pars = 'cut', transformations = inv.logit) + xlim(0, 1)## Scale for 'x' is already present. Adding another scale for 'x', which will\n## replace the existing scale."},{"path":"week-7.html","id":"question-2-5","chapter":"7 Week 7","heading":"7.2 Question 2","text":"Consider one variable Trolley data: Gender. Suppose gender\nmight influence education well response directly. Draw DAG now \nincludes response, education, age, gender. Using DAG, \npossible inferences Problem 1 con founded gender? ,\ndefine additional models need infer causal influence education\nresponse. conclude?","code":"\ndag <- dagify(\n  response ~ education + age + gender + action + intention + contact,\n  education ~ age,\n  education ~ gender,\n  contact ~ action,\n  exposure = 'education',\n  outcome = 'response'\n)\n\ndag_plot(dag)\nadjustmentSets(dag, exposure = 'education', outcome = 'response', effect = 'total' )## { age, gender }\nwriteLines(readLines(tar_read(stan_c_file_w07_model2_gender)))## data {\n##   int N;\n##   int K;\n##   int N_edu;\n##   int response[N];\n##   int action[N];\n##   int intention[N];\n##   int contact[N];\n##   int education[N];\n##   real age[N];\n##   int gender[N];\n##   vector[N_edu - 1] alpha;\n## }\n## parameters {\n##  // Cut points are the positions of responses along cumulative odds\n##   ordered[K] cutpoints;\n##   real beta_action;\n##   real beta_intention;\n##   real beta_contact;\n##   real beta_education;\n##   real beta_age;\n##   real beta_gender;\n## \n##   // Vector N reals that sum to 1\n##   simplex[7] delta;\n## }\n## model {\n##   vector[N] phi;\n##   vector[N_edu] delta_j;\n## \n##   delta ~ dirichlet(alpha);\n##   delta_j = append_row(0, delta);\n## \n##  for (i in 1:N) {\n##     // add beta education  * sum delta j, up to current i's education\n##     phi[i] = beta_education * sum(delta_j[1:education[i]]) +\n##       beta_action * action[i] +\n##       beta_contact * contact[i] +\n##       beta_age * age[i] +\n##       beta_gender * gender[i] +\n##       beta_intention * intention[i];\n##     response[i] ~ ordered_logistic(phi[i], cutpoints);\n##  }\n## \n##   cutpoints ~ normal(0, 1.5);\n##   beta_action ~ normal(0, 1);\n##   beta_contact ~ normal(0, 1);\n##   beta_intention ~ normal(0, 1);\n##   beta_education ~ normal(0, 1);\n##   beta_age ~ normal(0, 1);\n##   beta_gender ~ normal(0, 1);\n## }\nmodel2_gender_draws <- tar_read(stan_c_mcmc_w07_model2_gender)$draws()\n\nmcmc_areas(model2_gender_draws, regex_pars = 'beta')\nmcmc_areas(model2_gender_draws, regex_pars = 'delta')\nmcmc_areas(model2_gender_draws, regex_pars = 'cut', transformations = inv.logit) + xlim(0, 1)## Scale for 'x' is already present. Adding another scale for 'x', which will\n## replace the existing scale."},{"path":"week-8.html","id":"week-8","chapter":"8 Week 8","heading":"8 Week 8","text":"2021-09-08 [updated: 2021-11-17]","code":""},{"path":"week-8.html","id":"question-1-7","chapter":"8 Week 8","heading":"8.1 Question 1","text":"","code":""},{"path":"week-8.html","id":"data-7","chapter":"8 Week 8","heading":"8.1.1 Data","text":"Negative influence predation somewhat balanced size tank.","code":"\nDT <- data_reedfrogs()\nprecis(DT)##           mean    sd  5.5% 94.5%     histogram\n## density  23.33 10.38 10.00    35 ▇▁▁▁▁▁▁▇▁▁▁▁▇\n## pred       NaN    NA    NA    NA              \n## size       NaN    NA    NA    NA              \n## surv     16.31  9.88  4.58    33       ▂▇▂▁▃▁▃\n## propsurv  0.72  0.27  0.29     1     ▁▁▃▁▁▂▁▅▇\nwriteLines(readLines(tar_read(stan_d_file_w08_model_frogs_1)))## data {\n##  int N;\n##  int survival[N];\n##  int density[N];\n##  int tank[N];\n## }\n## parameters {\n##  real sigma;\n##  real alpha[N];\n##  real alpha_bar;\n## }\n## transformed parameters {\n##  vector[N] p;\n## \n##  for (i in 1:N) {\n##      p[i] = inv_logit(alpha[i]);\n##  }\n## }\n## model {\n##  alpha_bar ~ normal(0, 0.25);\n##  alpha ~ normal(alpha_bar, sigma);\n##  sigma ~ exponential(1);\n##  for (i in 1:N) {\n##      survival[i] ~ binomial(density[i], p[i]);\n##  }\n## }\nmodel_frogs_1_draws <- tar_read(stan_d_mcmc_w08_model_frogs_1)$draws()\n\nmcmc_areas(model_frogs_1_draws, regex_pars = 'alpha')\nmcmc_areas(model_frogs_1_draws, regex_pars = 'p\\\\[')\nwriteLines(readLines(tar_read(stan_d_file_w08_model_frogs_2)))## data {\n##  int N;\n##  int survival[N];\n##  int density[N];\n##  int tank[N];\n##  int predation[N];\n## }\n## parameters {\n##  real sigma;\n##  real alpha[N];\n##  real alpha_bar;\n##  real beta_predation;\n## }\n## transformed parameters {\n##  vector[N] p;\n## \n##  for (i in 1:N) {\n##      p[i] = inv_logit(alpha[i] + beta_predation * predation[i]);\n##  }\n## }\n## model {\n##  alpha_bar ~ normal(0, 0.25);\n##  alpha ~ normal(alpha_bar, sigma);\n##  beta_predation ~ normal(0, 0.5);\n##  sigma ~ exponential(1);\n##  for (i in 1:N) {\n##      survival[i] ~ binomial(density[i], p[i]);\n##  }\n## }\nmodel_frogs_2_draws <- tar_read(stan_d_mcmc_w08_model_frogs_2)$draws()\nmcmc_areas(model_frogs_2_draws, regex_pars = 'predation')\nwriteLines(readLines(tar_read(stan_d_file_w08_model_frogs_3)))## data {\n##  int N;\n##  int survival[N];\n##  int density[N];\n##  int tank[N];\n##  int size[N];\n## }\n## parameters {\n##  real sigma;\n##  real alpha[N];\n##  real alpha_bar;\n##  real beta_size;\n## }\n## transformed parameters {\n##  vector[N] p;\n## \n##  for (i in 1:N) {\n##      p[i] = inv_logit(alpha[i] + beta_size * size[i]);\n##  }\n## }\n## model {\n##  alpha_bar ~ normal(0, 0.25);\n##  alpha ~ normal(alpha_bar, sigma);\n##  beta_size ~ normal(0, 0.5);\n##  sigma ~ exponential(1);\n##  for (i in 1:N) {\n##      survival[i] ~ binomial(density[i], p[i]);\n##  }\n## }\nmodel_frogs_3_draws <- tar_read(stan_d_mcmc_w08_model_frogs_3)$draws()\nmcmc_areas(model_frogs_3_draws, regex_pars = 'size')\nwriteLines(readLines(tar_read(stan_d_file_w08_model_frogs_4)))## data {\n##  int N;\n##  int survival[N];\n##  int density[N];\n##  int tank[N];\n##  int size[N];\n##  int predation[N];\n## }\n## parameters {\n##  real sigma;\n##  real alpha[N];\n##  real alpha_bar;\n##  real beta_size;\n##  real beta_predation;\n##  real beta_interaction;\n## }\n## transformed parameters {\n##  vector[N] p;\n## \n##  for (i in 1:N) {\n##      p[i] = inv_logit(alpha[i] + beta_size * size[i] + beta_predation * predation[i] + beta_interaction * (size[i] * predation[i]));\n##  }\n## }\n## model {\n##  alpha_bar ~ normal(0, 0.25);\n##  alpha ~ normal(alpha_bar, sigma);\n##  beta_size ~ normal(0, 0.5);\n##  beta_predation ~ normal(0, 0.5);\n##  beta_interaction ~ normal(0, 0.25);\n##  sigma ~ exponential(1);\n##  for (i in 1:N) {\n##      survival[i] ~ binomial(density[i], p[i]);\n##  }\n## }\nmodel_frogs_4_draws <- tar_read(stan_d_mcmc_w08_model_frogs_4)$draws()\n\nmcmc_areas(model_frogs_4_draws, regex_pars = 'beta')"},{"path":"week-8.html","id":"question-2-6","chapter":"8 Week 8","heading":"8.2 Question 2","text":"1980, typical Bengali woman 5 children \nlifetime. year 2000, typical Bengali woman 2 3. ’re\ngoing look historical set data, contraception widely\navailable many families chose use . data reside \ndata(bangladesh) come 1988 Bangladesh Fertility Survey. row\none 1934 women. six variables, can focus two \npractice problem: (1) district: ID number administrative district\nwoman resided (2) use.contraception: indicator (0/1) whether \nwoman using contraception.Now, focus predicting use.contraception, clustered district_id. Fit\n(1) traditional fixed-effects model uses index variable \ndistrict (2) multilevel model varying intercepts district. Plot\npredicted proportions women district using contraception, \nfixed-effects model varying-effects model. , make plot\ndistrict ID horizontal axis expected proportion using\ncontraception vertical.Make one plot model, layer \nplot, prefer. models disagree? Can explain \npattern disagreement? particular, can explain extreme cases\ndisagreement, happen models reach\ndifferent inferences?Fixed effects modelMultilevel modelComparison","code":"\nDT <- data_bangladesh()\nprecis(DT)##                                     mean     sd  5.5%  94.5%    histogram\n## woman             967.500000000000000000 558.44 107.3 1827.7   ▇▇▇▇▇▇▇▇▇▅\n## district           29.252843846949328821  17.80   1.0   57.0 ▇▅▇▅▅▇▅▃▅▅▅▅\n## use.contraception   0.392450879007238906   0.49   0.0    1.0   ▇▁▁▁▁▁▁▁▁▅\n## living.children     2.652016546018614473   1.24   1.0    4.0       ▅▃▁▃▁▇\n## age.centered        0.002197880041365139   9.01 -12.6   16.4      ▅▇▇▅▃▃▂\n## urban               0.290589451913133401   0.45   0.0    1.0   ▇▁▁▁▁▁▁▁▁▃\n## id                967.500000000000000000 558.44 107.3 1827.7   ▇▇▇▇▇▇▇▇▇▅\n## contraception       0.392450879007238906   0.49   0.0    1.0   ▇▁▁▁▁▁▁▁▁▅\n## scale_age           0.000000000000000014   1.00  -1.4    1.8    ▁▇▇▅▇▃▃▂▁\nwriteLines(readLines(tar_read(stan_e_file_w08_model_bang_fixed)))## data {\n##  // Integers for number of rows, and number of districts\n##   int<lower=0> N;\n##   int<lower=0> N_district;\n## \n##   // District and contraception, expecting integers of length N\n##  int district[N];\n##  int contraception[N];\n## }\n## parameters {\n##  // Alpha vector matching length of number of districts\n##  vector[N_district] alpha;\n## }\n## model {\n##  // p vector matching length of number of districts\n##   vector[N] p;\n## \n##   // Alpha is distributed normally\n##  alpha ~ normal(0, 1.5);\n## \n##  // For each for in data, alpha for that row's district\n##   for (i in 1:N) {\n##      p[i] = inv_logit(alpha[district[i]]);\n##   }\n## \n##   // Contraception if distributed with bernoulli, p\n##   contraception ~ bernoulli(p);\n## }\nmodel_bang_fixed_draws <- tar_read(stan_e_mcmc_w08_model_bang_fixed)$draws()\n\nmcmc_areas(model_bang_fixed_draws, regex_pars = 'alpha', transformations = inv.logit)\nwriteLines(readLines(tar_read(stan_e_file_w08_model_bang_multi)))## data {\n##  // Integers for number of rows, and number of districts\n##   int<lower=0> N;\n##   int<lower=0> N_district;\n## \n##   // District and contraception, expecting integers of length N\n##  int district[N];\n##  int contraception[N];\n## }\n## parameters {\n##  // Alpha vector matching length of number of districts\n##  vector[N_district] alpha;\n##  real<lower=0> sigma;\n## \n##  // Hyper parameter alpha bar\n##  real alpha_bar;\n## }\n## model {\n##  // p vector matching length of number of districts\n##   vector[N] p;\n## \n##  // For each for in data, alpha for that row's district\n##   for (i in 1:N) {\n##      p[i] = inv_logit(alpha[district[i]]);\n##   }\n## \n##   // Hyper priors: alpha bar and sigma\n##  alpha_bar ~ normal(0, 1.5);\n##  sigma ~ exponential(1);\n## \n##  // Priors\n##   // Alpha is distributed normally\n##   alpha ~ normal(alpha_bar, sigma);\n## \n##   // Contraception if distributed with bernoulli, p\n##   contraception ~ bernoulli(p);\n## }\nmodel_bang_multi_draws <- tar_read(stan_e_mcmc_w08_model_bang_multi)$draws()\n\nmcmc_trace(model_bang_multi_draws)\nmcmc_areas(model_bang_multi_draws, regex_pars = 'alpha', transformations = inv.logit)\nmcmc_areas(model_bang_multi_draws, pars = c('alpha_bar', 'sigma'))\n(mcmc_areas(model_bang_fixed_draws, regex_pars = 'alpha', transformations = inv.logit) + labs(title = 'fixed')) + \n    (mcmc_areas(model_bang_multi_draws, regex_pars = 'alpha\\\\[', transformations = inv.logit) + labs(title = 'multilevel'))\nsetDT(model_bang_fixed_draws)\nsetDT(model_bang_multi_draws)\n\ncompare <- rbindlist(list(\n    melt(model_bang_fixed_draws, measure.vars = patterns('alpha'))[, model_type := 'fixed'],\n    melt(model_bang_multi_draws, measure.vars = patterns('alpha'))[, model_type := 'multilevel']\n), fill = TRUE)\n\ncompare[, variable := as.integer(gsub('alpha\\\\[|\\\\]', '', variable))]## Warning in eval(jsub, SDenv, parent.frame()): NAs introduced by coercion\nggplot(compare[, .(value = mean(value)), .(variable, model_type)]) + \n    geom_hline(yintercept = 0, alpha = 0.5) + \n    geom_point(aes(variable, value, color = model_type)) +\n    geom_line(aes(variable, value, group = model_type), alpha = 0.2) +\n    scale_color_viridis_d(begin = 0.3, end = 0.8) + \n    labs(x = 'district', y = 'alpha')## Warning: Removed 1 rows containing missing values (geom_point).## Warning: Removed 1 row(s) containing missing values (geom_path)."},{"path":"week-8.html","id":"question-3-5","chapter":"8 Week 8","heading":"8.3 Question 3","text":"Return Trolley data, data(Trolley), Chapter 12. Define fit \nvarying intercepts model data. mean add intercept\nparameter individual linear model. Cluster varying intercepts\nindividual participants, indicated unique values id\nvariable. Include action, intention, contact . Compare varying\nintercepts model model ignores individuals, using WAIC/LOO \nposterior predictions. impact individual variation data?","code":"\nDT <- data_trolley()\nprecis(DT)##              mean    sd 5.5% 94.5%      histogram\n## case          NaN    NA   NA    NA               \n## response     4.20  1.91    1     7   ▃▂▁▃▁▇▁▅▁▅▁▅\n## order       16.50  9.29    2    31        ▇▅▇▇▅▇▂\n## id            NaN    NA   NA    NA               \n## age         37.49 14.23   18    61  ▂▇▅▇▅▇▅▅▃▅▂▁▁\n## male         0.57  0.49    0     1     ▅▁▁▁▁▁▁▁▁▇\n## edu           NaN    NA   NA    NA               \n## action       0.43  0.50    0     1     ▇▁▁▁▁▁▁▁▁▅\n## intention    0.47  0.50    0     1     ▇▁▁▁▁▁▁▁▁▇\n## contact      0.20  0.40    0     1     ▇▁▁▁▁▁▁▁▁▂\n## story         NaN    NA   NA    NA               \n## action2      0.63  0.48    0     1     ▃▁▁▁▁▁▁▁▁▇\n## education    5.73  1.35    3     8 ▁▁▁▁▁▂▁▅▁▇▁▂▁▂\n## individual 166.00 95.56   19   313        ▇▇▇▇▇▇▅\nwriteLines(readLines(tar_read(stan_c_file_w08_model_trolley)))## data {\n##   int N;\n##   int K;\n##   int response[N];\n##   int action[N];\n##   int intention[N];\n##   int contact[N];\n## \n## }\n## parameters {\n##  // Cut points are the positions of responses along cumulative odds\n##   ordered[K] cutpoints;\n##   real beta_action;\n##   real beta_intention;\n##   real beta_contact;\n##   real beta_intention_contact;\n##   real beta_intention_action;\n## }\n## transformed parameters {\n##   vector[N] phi;\n## \n##  for (i in 1:N) {\n##      phi[i] = beta_action * action[i] + beta_contact * contact[i] + beta_intention * intention[i] + beta_intention_contact * intention[i] * contact[i]  + beta_intention_action * intention[i] * action[i];\n##  }\n## }\n## model {\n##  response ~ ordered_logistic(phi, cutpoints);\n##   cutpoints ~ normal(0, 1.5);\n##   beta_action ~ normal(0, 0.5);\n##   beta_contact ~ normal(0, 0.5);\n##   beta_intention ~ normal(0, 0.5);\n## }\n## generated quantities {\n##  vector[N] log_lik;\n##   for (i in 1:N) {\n##     log_lik[i] = ordered_logistic_lpmf(response[i] | phi[i], cutpoints);\n##   }\n## }\nmodel_trolley <- tar_read(stan_c_mcmc_w08_model_trolley)\nmodel_trolley_draws <- model_trolley$draws()\n\nmcmc_areas(model_trolley_draws, regex_pars = 'cutpoints')\nmcmc_areas(model_trolley_draws, regex_pars = 'beta')\noptions(cmdstanr_draws_format = \"draws_array\")\nmodel_trolley_loo <- model_trolley$loo()\noptions(cmdstanr_draws_format = \"draws_df\")\nwriteLines(readLines(tar_read(stan_c_file_w08_model_trolley_multi)))## data {\n##   int N;\n##   int K;\n##   int response[N];\n##   int action[N];\n##   int intention[N];\n##   int contact[N];\n##   int individual[N];\n##   int N_individual;\n## }\n## parameters {\n##  // Cut points are the positions of responses along cumulative odds\n##   ordered[K] cutpoints;\n##   real beta_action;\n##   real beta_intention;\n##   real beta_contact;\n##   real beta_intention_contact;\n##   real beta_intention_action;\n##   real alpha_bar;\n##  real sigma;\n##   vector[N_individual] alpha;\n## }\n## transformed parameters {\n##   vector[N] phi;\n## \n##  for (i in 1:N) {\n##      phi[i] = alpha[individual[i]] + beta_action * action[i] +\n##                       beta_contact * contact[i] + beta_intention * intention[i] +\n##                       beta_intention_contact * intention[i] * contact[i]  +\n##                       beta_intention_action * intention[i] * action[i];\n##  }\n## }\n## model {\n##  // Hyper parameter priors\n##  alpha_bar ~ normal(0, 1.5);\n##  sigma ~ exponential(1);\n## \n##  // Priors\n##  alpha ~ normal(alpha_bar, sigma);\n## \n##  response ~ ordered_logistic(phi, cutpoints);\n##   cutpoints ~ normal(0, 1.5);\n##   beta_action ~ normal(0, 0.5);\n##   beta_contact ~ normal(0, 0.5);\n##   beta_intention ~ normal(0, 0.5);\n## }\n## generated quantities {\n##  vector[N] log_lik;\n##   for (i in 1:N) {\n##     log_lik[i] = ordered_logistic_lpmf(response[i] | phi[i], cutpoints);\n##   }\n## }\nmodel_trolley_multi <- tar_read(stan_c_mcmc_w08_model_trolley_multi)\nmodel_trolley_multi_draws <- model_trolley_multi$draws()\n\nmcmc_areas(model_trolley_multi_draws, regex_pars = 'cutpoints')\nmcmc_areas(model_trolley_multi_draws, regex_pars = 'beta')\noptions(cmdstanr_draws_format = \"draws_array\")\nmodel_trolley_multi_loo <- model_trolley_multi$loo()\noptions(cmdstanr_draws_format = \"draws_df\")\ncol_patterns <- '^beta|^alpha|^sigma|^cutpoints'\nsetDT(model_trolley_draws)\nsetDT(model_trolley_multi_draws)\n\nprecis(model_trolley_draws[, .SD, .SDcols = patterns(col_patterns)])## 6 vector or matrix parameters hidden. Use depth=2 to show them.##                         mean    sd  5.5% 94.5%       histogram\n## beta_action            -0.46 0.055 -0.54 -0.37      ▁▁▁▃▇▇▂▁▁▁\n## beta_intention         -0.27 0.058 -0.36 -0.18      ▁▁▁▅▇▅▂▁▁▁\n## beta_contact           -0.32 0.069 -0.43 -0.21     ▁▁▁▂▅▇▇▃▁▁▁\n## beta_intention_contact -1.29 0.097 -1.44 -1.13 ▁▁▁▁▃▅▇▇▇▃▂▁▁▁▁\n## beta_intention_action  -0.46 0.081 -0.59 -0.34   ▁▁▁▁▃▅▇▇▅▂▁▁▁\nprecis(model_trolley_multi_draws[, .SD, .SDcols = patterns(col_patterns)])## 337 vector or matrix parameters hidden. Use depth=2 to show them.##                         mean    sd  5.5% 94.5%      histogram\n## beta_action            -0.63 0.056 -0.72 -0.54      ▁▁▂▇▇▅▁▁▁\n## beta_intention         -0.36 0.060 -0.45 -0.26     ▁▁▁▃▇▇▃▁▁▁\n## beta_contact           -0.42 0.072 -0.54 -0.30    ▁▁▁▂▇▇▇▃▁▁▁\n## beta_intention_contact -1.74 0.101 -1.91 -1.58      ▁▁▁▃▇▅▁▁▁\n## beta_intention_action  -0.60 0.082 -0.73 -0.46  ▁▁▁▁▂▅▇▇▅▂▁▁▁\n## alpha_bar               1.01 0.437  0.26  1.72  ▁▁▁▂▂▂▇▇▅▂▂▁▁\n## sigma                   1.92 0.082  1.80  2.06 ▁▁▁▁▃▇▇▅▃▁▁▁▁▁\ncompared <- loo_compare(model_trolley_loo, model_trolley_multi_loo)\nprint(compared, simplify = FALSE)##        elpd_diff se_diff  elpd_loo se_elpd_loo p_loo    se_p_loo looic   \n## model2      0.0       0.0 -15529.0     89.8       356.8      4.7  31058.0\n## model1  -2935.7      86.8 -18464.7     40.5        11.1      0.1  36929.5\n##        se_looic\n## model2    179.6\n## model1     81.1"},{"path":"week-9.html","id":"week-9","chapter":"9 Week 9","heading":"9 Week 9","text":"2021-09-14 [updated: 2021-11-17]","code":""},{"path":"week-9.html","id":"question-1-8","chapter":"9 Week 9","heading":"9.1 Question 1","text":"Revisit Bangladesh fertility data, data(bangladesh). Fit model \nvarying intercepts district_id varying slopes urban (0/1\nindicator variable) district_id. still predicting use.contraception.\nInspect correlation intercepts slopes. Can interpret\ncorrelation, terms tells pattern \ncontraceptive use sample? might help plot varying effect\nestimates intercepts slopes, district. can\nvisualize correlation maybe easily think means \nparticular correlation. Plotting predicted proportion women using\ncontraception, district, urban women one axis rural \n, might also help.","code":"\nprecis(DT)##              mean    sd 5.5% 94.5%      histogram\n## case          NaN    NA   NA    NA               \n## response     4.20  1.91    1     7   ▃▂▁▃▁▇▁▅▁▅▁▅\n## order       16.50  9.29    2    31        ▇▅▇▇▅▇▂\n## id            NaN    NA   NA    NA               \n## age         37.49 14.23   18    61  ▂▇▅▇▅▇▅▅▃▅▂▁▁\n## male         0.57  0.49    0     1     ▅▁▁▁▁▁▁▁▁▇\n## edu           NaN    NA   NA    NA               \n## action       0.43  0.50    0     1     ▇▁▁▁▁▁▁▁▁▅\n## intention    0.47  0.50    0     1     ▇▁▁▁▁▁▁▁▁▇\n## contact      0.20  0.40    0     1     ▇▁▁▁▁▁▁▁▁▂\n## story         NaN    NA   NA    NA               \n## action2      0.63  0.48    0     1     ▃▁▁▁▁▁▁▁▁▇\n## education    5.73  1.35    3     8 ▁▁▁▁▁▂▁▅▁▇▁▂▁▂\n## individual 166.00 95.56   19   313        ▇▇▇▇▇▇▅\nwriteLines(readLines(tar_read(stan_e_file_w09_model_q1)))## data {\n##  // Integers for number of rows, and number of districts\n##   int<lower=0> N;\n##   int<lower=0> N_district;\n## \n##   // District, contraception and urban, expecting integers of length N\n##  int district[N];\n##  int contraception[N];\n##  int urban[N];\n## }\n## parameters {\n##  // Alpha and beta urban vectors matching length of number of districts\n##  vector[N_district] alpha;\n## \n##  // Beta urban vector matching length of number of districts\n##  vector[N_district] beta;\n## \n##  // Hyper parameter alpha bar, beta bar\n##  real alpha_bar;\n##  real beta_bar;\n## \n##  // Correlation matrix, sigma\n##  // 2 represents the number of predictors\n##  corr_matrix[2] Rho;\n##  vector<lower=0>[2] sigma;\n## }\n## model {\n##  // p vector matching length of number of districts\n##   vector[N] p;\n## \n##   // Hyper priors: alpha bar, beta urban bar, sigma and Rho\n##  alpha_bar ~ normal(0, 1);\n##  beta_bar ~ normal(0, 0.5);\n##  sigma ~ exponential(1);\n##  Rho ~ lkj_corr(2);\n## \n##  // Multivariate normal\n##   {\n##    vector[2] YY[N_district];\n##    vector[2] MU;\n##    MU = [alpha_bar, beta_bar]';\n##    for (j in 1:N_district) {\n##      YY[j] = [alpha[j], beta[j]]';\n##    }\n##    YY ~ multi_normal(MU, quad_form_diag(Rho, sigma));\n##   }\n## \n##  // For each for in data, alpha and beta for that row's district\n##   for (i in 1:N) {\n##      p[i] = inv_logit(alpha[district[i]] + beta[district[i]] * urban[i]);\n##   }\n## \n##   // Contraception if distributed with bernoulli, p\n##   contraception ~ bernoulli(p);\n## }\nmodel_q1_draws <- tar_read(stan_e_mcmc_w09_model_q1)$draws()\n\nmcmc_areas(model_q1_draws, regex_pars = 'alpha')\nmcmc_areas(model_q1_draws, regex_pars = 'beta')\nmcmc_areas(model_q1_draws, regex_pars = 'bar')\nmcmc_areas(model_q1_draws, regex_pars = 'Rho')\nsetDT(model_q1_draws)\n\nprecis(model_q1_draws[, .SD, .SDcols = patterns('*bar')])##            mean   sd  5.5% 94.5%    histogram\n## alpha_bar -0.69 0.10 -0.85 -0.53    ▁▁▁▂▇▇▃▁▁\n## beta_bar   0.64 0.16  0.38  0.90 ▁▁▁▂▃▇▇▅▃▁▁▁\nprecis(model_q1_draws[, .SD, .SDcols = patterns('Rho|sigma')], depth = 3)##           mean    sd  5.5% 94.5%      histogram\n## Rho[1,1]  1.00 0.000  1.00  1.00              ▇\n## Rho[2,1] -0.65 0.168 -0.86 -0.34       ▂▇▃▁▁▁▁▁\n## Rho[1,2] -0.65 0.168 -0.86 -0.34       ▂▇▃▁▁▁▁▁\n## Rho[2,2]  1.00 0.000  1.00  1.00              ▇\n## sigma[1]  0.57 0.096  0.43  0.74 ▁▁▁▂▅▇▇▇▃▂▁▁▁▁\n## sigma[2]  0.77 0.207  0.46  1.10      ▁▁▃▇▅▂▁▁▁\nmcmc_hex(model_q1_draws, regex_pars = '*bar')"},{"path":"week-9.html","id":"question-2-7","chapter":"9 Week 9","heading":"9.2 Question 2","text":"Now consider predictor variables age.centered living.children, also\ncontained data(bangladesh). Suppose age influences contraceptive use\n(changing attitudes) number children (older people time \nkids). Number children may also directly influence contraceptive use.\nDraw DAG reflects hypothetical relationships. build models\nneeded evaluate DAG. need least two models. Retain district\nurban, Problem 1. conclude causal influence \nage children?","code":"\ndag <- dagify(\n  contraceptive_use ~ age + number_children,\n  number_children ~ age,\n  exposure = 'age',\n  outcome = 'contraceptive_use'\n)\n\ndag_plot(dag)\nadjustmentSets(dag, exposure = 'age', outcome = 'contraceptive_use', effect = 'total')##  {}\nadjustmentSets(dag, exposure = 'age', outcome = 'contraceptive_use', effect = 'direct')## { number_children }\nwriteLines(readLines(tar_read(stan_e_file_w09_model_q2_a)))## data {\n##  // Integers for number of rows, and number of districts\n##   int<lower=0> N;\n##   int<lower=0> N_district;\n## \n##   // District, contraception and urban, expecting integers of length N\n##  int district[N];\n##  int contraception[N];\n##  int urban[N];\n## \n##  // Also scale_age\n##  real scale_age[N];\n## }\n## parameters {\n##  // Alpha and beta urban vectors matching length of number of districts\n##  vector[N_district] alpha;\n## \n##  // Beta urban vector matching length of number of districts\n##  vector[N_district] beta;\n## \n##  // Hyper parameter alpha bar, beta bar\n##  real alpha_bar;\n##  real beta_bar;\n## \n##  // scale_age\n##  real beta_scale_age;\n## \n##  // Correlation matrix, sigma\n##  // 2 represents the number of predictors\n##  corr_matrix[2] Rho;\n##  vector<lower=0>[2] sigma;\n## }\n## model {\n##  // p vector matching length of number of districts\n##   vector[N] p;\n## \n##   // Hyper priors: alpha bar, beta urban bar, sigma and Rho\n##  alpha_bar ~ normal(0, 1);\n##  beta_bar ~ normal(0, 0.5);\n##  sigma ~ exponential(1);\n##  Rho ~ lkj_corr(2);\n## \n##  // Multivariate normal\n##   {\n##    vector[2] YY[N_district];\n##    vector[2] MU;\n##    MU = [alpha_bar, beta_bar]';\n##    for (j in 1:N_district) {\n##      YY[j] = [alpha[j], beta[j]]';\n##    }\n##    YY ~ multi_normal(MU, quad_form_diag(Rho, sigma));\n##   }\n## \n##   // Beta scale_age prior\n##  beta_scale_age ~ normal(0, 1.5);\n## \n##  // For each for in data, alpha and beta for that row's district\n##   for (i in 1:N) {\n##      p[i] = inv_logit(alpha[district[i]] + beta[district[i]] * urban[i] + beta_scale_age * scale_age[i]);\n##   }\n## \n##   // Contraception if distributed with bernoulli, p\n##   contraception ~ bernoulli(p);\n## }\nmodel_q2_a_draws <- tar_read(stan_e_mcmc_w09_model_q2_a)$draws()\nsetDT(model_q2_a_draws)\nwriteLines(readLines(tar_read(stan_e_file_w09_model_q2_b)))## data {\n##  // Integers for number of rows, and number of districts\n##   int<lower=0> N;\n##   int<lower=0> N_district;\n## \n##   // District, contraception and urban, expecting integers of length N\n##  int district[N];\n##  int contraception[N];\n##  int urban[N];\n## \n##  // Also scale_age and n_children\n##  real scale_age[N];\n##  int n_children[N];\n## }\n## parameters {\n##  // Alpha and beta urban vectors matching length of number of districts\n##  vector[N_district] alpha;\n## \n##  // Beta urban vector matching length of number of districts\n##  vector[N_district] beta;\n## \n##  // Hyper parameter alpha bar, beta bar\n##  real alpha_bar;\n##  real beta_bar;\n## \n##  // scale_age and n_children\n##  real beta_scale_age;\n##  real beta_children;\n## \n##  // Correlation matrix, sigma\n##  // 2 represents the number of predictors\n##  corr_matrix[2] Rho;\n##  vector<lower=0>[2] sigma;\n## }\n## model {\n##  // p vector matching length of number of districts\n##   vector[N] p;\n## \n##   // Hyper priors: alpha bar, beta urban bar, sigma and Rho\n##  alpha_bar ~ normal(0, 1);\n##  beta_bar ~ normal(0, 0.5);\n##  sigma ~ exponential(1);\n##  Rho ~ lkj_corr(2);\n## \n##  // Multivariate normal\n##   {\n##    vector[2] YY[N_district];\n##    vector[2] MU;\n##    MU = [alpha_bar, beta_bar]';\n##    for (j in 1:N_district) {\n##      YY[j] = [alpha[j], beta[j]]';\n##    }\n##    YY ~ multi_normal(MU, quad_form_diag(Rho, sigma));\n##   }\n## \n##   // Beta scale_age prior\n##  beta_scale_age ~ normal(0, 1.5);\n##  beta_children ~ normal(0, 1.5);\n## \n##  // For each for in data, alpha and beta for that row's district\n##   for (i in 1:N) {\n##      p[i] = inv_logit(alpha[district[i]] + beta[district[i]] * urban[i] + beta_scale_age * scale_age[i] + beta_children * n_children[i]);\n##   }\n## \n##   // Contraception if distributed with bernoulli, p\n##   contraception ~ bernoulli(p);\n## }\nmodel_q2_b_draws <- tar_read(stan_e_mcmc_w09_model_q2_b)$draws()\nsetDT(model_q2_b_draws)\nprecis(model_q2_a_draws[, .SD, .SDcols = patterns('beta')])## 60 vector or matrix parameters hidden. Use depth=2 to show them.##                 mean   sd   5.5% 94.5%      histogram\n## beta_bar       0.643 0.16 0.3844  0.90 ▁▁▁▁▃▇▇▅▃▁▁▁▁▁\n## beta_scale_age 0.084 0.05 0.0057  0.16       ▁▁▃▇▅▂▁▁\nprecis(model_q2_b_draws[, .SD, .SDcols = patterns('beta')])## 60 vector or matrix parameters hidden. Use depth=2 to show them.##                 mean    sd  5.5% 94.5%     histogram\n## beta_bar        0.68 0.163  0.42  0.93 ▁▁▁▁▃▅▇▇▃▂▁▁▁\n## beta_scale_age -0.26 0.071 -0.38 -0.15  ▁▁▁▁▂▅▇▇▃▁▁▁\n## beta_children   0.41 0.060  0.32  0.51     ▁▁▃▇▇▃▁▁▁"},{"path":"week-9.html","id":"question-3-6","chapter":"9 Week 9","heading":"9.3 Question 3","text":"Modify models Problem 2 contained children variable \nmodel variable now monotonic ordered category, like education \nweek ordered categories. Education example 8 categories.\nChildren fewer (one sample 8 children). modify\ncode appropriately. conclude causal influence \nadditional child use contraception?","code":"\nwriteLines(readLines(tar_read(stan_e_file_w09_model_q3)))## data {\n##  // Integers for number of rows, and number of districts\n##   int<lower=0> N;\n##   int<lower=0> N_district;\n## \n##   // K categories\n##   int K;\n##   vector[K] alpha_k;\n## \n##   // District, contraception and urban, expecting integers of length N\n##  int district[N];\n##  int contraception[N];\n##  int urban[N];\n## \n##  // Also scale_age, n_children\n##  real scale_age[N];\n##  int n_children[N];\n## }\n## parameters {\n##  // Alpha and beta urban vectors matching length of number of districts\n##  vector[N_district] alpha;\n## \n##  // Beta urban vector matching length of number of districts\n##  vector[N_district] beta;\n## \n##  // Hyper parameter alpha bar, beta bar\n##  real alpha_bar;\n##  real beta_bar;\n## \n##  // scale_age and n_children\n##  real beta_scale_age;\n##  real beta_children;\n## \n##  // Correlation matrix, sigma\n##  // 2 represents the number of predictors\n##  corr_matrix[2] Rho;\n##  vector<lower=0>[2] sigma;\n## \n##  simplex[3] delta;\n## }\n## model {\n##  // p vector matching length of number of districts\n##   vector[N] p;\n## \n##   // Hyper priors: alpha bar, beta urban bar, sigma and Rho\n##  alpha_bar ~ normal(0, 1);\n##  beta_bar ~ normal(0, 0.5);\n##  sigma ~ exponential(1);\n##  Rho ~ lkj_corr(2);\n## \n##  //\n##  vector[K] delta_shell;\n##  delta ~ dirichlet(alpha_k);\n##  delta_shell = append_row(0, delta);\n## \n##  // Multivariate normal\n##   {\n##    vector[2] YY[N_district];\n##    vector[2] MU;\n##    MU = [alpha_bar, beta_bar]';\n##    for (j in 1:N_district) {\n##      YY[j] = [alpha[j], beta[j]]';\n##    }\n##    YY ~ multi_normal(MU, quad_form_diag(Rho, sigma));\n##   }\n## \n##   // Beta scale_age prior\n##  beta_scale_age ~ normal(0, 1.5);\n##  beta_children ~ normal(0, 1.5);\n## \n##  // For each for in data, alpha and beta for that row's district\n##   for (i in 1:N) {\n##      p[i] = inv_logit(alpha[district[i]] + beta[district[i]] * urban[i] + beta_scale_age * scale_age[i] + beta_children * sum(delta_shell[1:n_children[i]]) * n_children[i]);\n##   }\n## \n##   // Contraception if distributed with bernoulli, p\n##   contraception ~ bernoulli(p);\n## }\nmodel_q3_draws <- tar_read(stan_e_mcmc_w09_model_q3)$draws()\nsetDT(model_q3_draws)\n\nprecis(model_q3_draws[, .SD, .SDcols = patterns('beta')])## 60 vector or matrix parameters hidden. Use depth=2 to show them.##                 mean    sd  5.5% 94.5%      histogram\n## beta_bar        0.69 0.160  0.44  0.94 ▁▁▁▁▂▅▇▇▅▂▁▁▁▁\n## beta_scale_age -0.29 0.072 -0.40 -0.17   ▁▁▁▁▃▇▇▅▂▁▁▁\n## beta_children   0.34 0.046  0.26  0.41       ▁▁▃▇▇▂▁▁\nprecis(model_q3_draws[, .SD, .SDcols = patterns('delta')], 3)##           mean    sd 5.5% 94.5%      histogram\n## delta[1] 0.842 0.079 0.70  0.95 ▁▁▁▁▁▁▁▁▂▃▇▇▅▁\n## delta[2] 0.102 0.069 0.02  0.23    ▅▇▅▂▁▁▁▁▁▁▁\n## delta[3] 0.056 0.039 0.01  0.13        ▇▅▁▁▁▁▁\nmcmc_areas(model_q3_draws, regex_pars = 'delta')"},{"path":"lecture-01.html","id":"lecture-01","chapter":"10 Lecture 01","heading":"10 Lecture 01","text":"","code":""},{"path":"lecture-01.html","id":"popper","chapter":"10 Lecture 01","heading":"10.0.1 Popper","text":"generate meaningful (null) hypotheses predictions falsify ","code":""},{"path":"lecture-01.html","id":"approach-1","chapter":"10 Lecture 01","heading":"10.0.2 Approach","text":"framework developing + using statistical golemsBayesian data analysis\nuses probability describe uncertainty\n“count ways data can happen, according assumptions assumptions ways consistent data plausible”\nuses probability describe uncertainty“count ways data can happen, according assumptions assumptions ways consistent data plausible”Multilevel models\nModels within models\nAvoids averaging\n…\nModels within modelsAvoids averaging…Model comparison\nCompare meaningful (null) models\nCaution: fitting\nCompare meaningful (null) modelsCaution: fitting","code":""},{"path":"lecture-01.html","id":"hypotheses---process-models---statistical-models","chapter":"10 Lecture 01","heading":"10.1 Hypotheses - Process Models - Statistical Models","text":"statistical model M can correspond multiple process modelsAny hypothesis H may correspond multiple process modelsAny statistical model may correspond multiple hypothesisUntitled","code":""},{"path":"lecture-01.html","id":"small-world-large-world","chapter":"10 Lecture 01","heading":"10.2 Small world / large world","text":"Small world: models assumptions, Bayesian models fit optimallyLarge world: real world, guarantee optimality","code":""},{"path":"lecture-01.html","id":"example-four-marbles","chapter":"10 Lecture 01","heading":"10.3 Example: four marbles","text":"Setup4 marbles, either black white, replacementPossibilities (5) therefore: WWWW, BWWW, BBWW, BBBW, BBBBObservation: BWBCalculateGiven 3 observations, 4 choices, total 64 possibilitiesGiven observed white black marble, possibilities WWWW BBBB validAt branch, 3 possibilities can white 1 possibility can blackBayesian additive, branch just sum possibilityBWWW: 3 = 1 * 3 * 1BBWW: 8 = 2 * 2 * 2BBBW: 9 = 3 * 1 * 3Using new informationNew information directly integrated old information, therefore just multiply throughSo take another measure B, multiply property throughBWWW: 3 * 1 = 3BBWW: 8 * 2 = 16BBBW: 9 * 3 = 27Using informationFactory says B measures rare, minimum 1 per bagWWWW 0 since observed BBWWW: 3BBWW: 2BBBW: 1BBBB 0 since observed WMultiply throughBWWW: 3 * 3 = 9BBWW: 16 * 2 = 32BBBW: 1 * 27 = 27Counts get huge - therefore normalize giving us probabilities (0-1)Probability theory just normalized counting","code":"           Factory info"},{"path":"lecture-01.html","id":"building-a-model","chapter":"10 Lecture 01","heading":"10.4 Building a model","text":"Design modelCondition dataEvaluate, critique model(Restart)","code":""},{"path":"lecture-02.html","id":"lecture-02","chapter":"11 Lecture 02","heading":"11 Lecture 02","text":"","code":""},{"path":"lecture-02.html","id":"joint-prior-distribution","chapter":"11 Lecture 02","heading":"11.1 Joint prior distribution","text":"joint prior distribution prior probability distribution + parameters","code":""},{"path":"lecture-02.html","id":"example-inflatable-world","chapter":"11 Lecture 02","heading":"11.2 Example: inflatable world","text":"","code":""},{"path":"lecture-02.html","id":"design-the-model","chapter":"11 Lecture 02","heading":"11.2.1 Design the model","text":"p: water proportion1-p: land proportion","code":""},{"path":"lecture-02.html","id":"condition","chapter":"11 Lecture 02","heading":"11.2.2 Condition","text":"Bayes updating: converts priors posteriorsAdds data onceAll posteriors prior next observationSample size embodied posterior","code":""},{"path":"lecture-02.html","id":"evaluate","chapter":"11 Lecture 02","heading":"11.2.3 Evaluate","text":"Golem must supervisedDid malfunction?answer make sense?…","code":""},{"path":"lecture-02.html","id":"define-parameters","chapter":"11 Lecture 02","heading":"11.2.4 Define parameters","text":"N: fixed experimenterW: probability distribution, case binomial distributionWLWWLWWLWdbinom(6, size = 9, prob = 0.5)p: prior probability distribution, case uniformed","code":""},{"path":"lecture-02.html","id":"joint-model-1","chapter":"11 Lecture 02","heading":"11.2.5 Joint model","text":"W ~ Binomial(N, p)p ~ Uniform(0, 1)(W distributed binomially probability p measure, p uniform 1)","code":""},{"path":"lecture-02.html","id":"posterior","chapter":"11 Lecture 02","heading":"11.2.6 Posterior","text":"Posterior = (probability observed variables * prior) / normalizing constant(priors uniform, don’t affect shape posterior. may influence shape though)","code":""},{"path":"lecture-02.html","id":"grid-approximation","chapter":"11 Lecture 02","heading":"11.3 Grid approximation","text":"Grid approximation: consider finite discrete set solutionsFor example, 1000 solutionsGenerate sequence solutions seq_sol <- seq(0, 1, length.= 1000)Prior = uniform 1 across sequence solutions prior <- rep(1, seq_sol)Probability data = binomial prob_data <- dbinom(6, size = 9, prob = seq_sol)Posterior numerator = posterior_num <- prior * prob_dataPosterior standardized = posterior_numerator / sum(posterior_num)","code":""},{"path":"lecture-02.html","id":"sampling-from-the-posterior","chapter":"11 Lecture 02","heading":"11.3.1 Sampling from the posterior","text":"Approximate posterior, can sample posteriors sample(p, prob = posterior, 1e4, replace = TRUE)Summarizeabove/valuePercentile intervalHighest posterior density interval…Predictive checksrbinorm(1e4, size = 0, prob = samples)…","code":""},{"path":"lecture-03.html","id":"lecture-03","chapter":"12 Lecture 03","heading":"12 Lecture 03","text":"","code":""},{"path":"lecture-03.html","id":"regressions","chapter":"12 Lecture 03","heading":"12.1 Regressions","text":"Model mean variance normally distributed measureMean additive combination weighted variablesTypical assumed constant variable (???)line returned mean - Bayesian want see distribution lines, ranked plausibilityThe model endorses line, line doesn’t necessarily fit dataIn regressions always certainty means bow tie towards limits dataRegression models don’t arrows like DAGs - just measure associations.","code":""},{"path":"lecture-03.html","id":"normal-distributions","chapter":"12 Lecture 03","heading":"12.2 Normal distributions","text":"Normal distributions arise repeated fluctuations tend cancel near 0The Gaussian distribution conservative distribution use prior, best option additional scientific information available","code":""},{"path":"lecture-03.html","id":"prior-predictive-distributions","chapter":"12 Lecture 03","heading":"12.3 Prior predictive distributions","text":"Simulate joint posterior distribution evaluateSetup model quapprior <- extract.prior(model)link(model, post = prior, data = seq)\nseq sequence x variable (eg standardize -2, 2\nseq sequence x variable (eg standardize -2, 2Plot linesThese possibility give prior, dataIf lines show limited relationship ’d expect true relationship outside , expand priors.alternatively widely implausible, tighten priors.","code":""},{"path":"lecture-03.html","id":"quadratic-approximate","chapter":"12 Lecture 03","heading":"12.4 Quadratic approximate","text":"multidimensional space, QUAP uses gradient climbing find peaksMaximum likelihood estimation = QUAP flat priorsFunction rethinking rethinking::quap","code":""},{"path":"lecture-03.html","id":"centering-variables","chapter":"12 Lecture 03","heading":"12.5 Centering variables","text":"x - mean(x)default behavior regression","code":""},{"path":"lecture-04.html","id":"lecture-04","chapter":"13 Lecture 04","heading":"13 Lecture 04","text":"","code":""},{"path":"lecture-04.html","id":"standardizing-variables","chapter":"13 Lecture 04","heading":"13.1 Standardizing variables","text":"(x - mean(x)) / sd(x) scale(x)Result = mean 0, sd 1Helps software fitValue = 1 equal 1 SD","code":""},{"path":"lecture-04.html","id":"plotting-uncertainty---sample-from-posterior","chapter":"13 Lecture 04","heading":"13.2 Plotting uncertainty - sample from posterior","text":"(multivariate normal)Approximate posterior mean, standard deviationSample multivariate normal distribution parametersUse samples generate predictions integrate uncertaintyextract_samples returns , b, sigma, … can plot ","code":""},{"path":"lecture-04.html","id":"polynomials","chapter":"13 Lecture 04","heading":"13.3 Polynomials","text":"Polynomials bad behavior especially boundaries dataThey don’t fit locally, actually flexibly.Eg. polynomial 3rd degree necessarily two turns - happen irrespective data","code":""},{"path":"lecture-04.html","id":"splines","chapter":"13 Lecture 04","heading":"13.4 Splines","text":"Locally wiggly functions, combined interpolationGeocentric - describing relationships - exploring ","code":""},{"path":"lecture-04.html","id":"basis-splines","chapter":"13 Lecture 04","heading":"13.5 Basis splines","text":"Bayesian B-splines = P-splinesSimilar linear models synthetic variables\\(\\mu = \\alpha + w_{1} \\beta_{1} + + w_{2} \\beta_{2} + + w_{3} \\beta_{3} + + w_{4} \\beta_{4} + ...\\)Knots often picked equal intervals data, though strategies varyAt knot, knot’s function 100%, moving away , neighboring functions turn onParameters always uncertainty predictionsCaution: fitting","code":""},{"path":"lecture-04.html","id":"recipe","chapter":"13 Lecture 04","heading":"13.5.1 Recipe","text":"Choose knots - points spline pivotsChoose degree basis functions - wiggly, polynomialFind posterior distribution weights","code":""},{"path":"lecture-05.html","id":"lecture-05","chapter":"14 Lecture 05","heading":"14 Lecture 05","text":"","code":""},{"path":"lecture-05.html","id":"multiple-regression-models","chapter":"14 Lecture 05","heading":"14.1 Multiple regression models","text":"?Spurious associationsDetermining value predictor given predictors\neg. divorce rate given marriage rate median age marriage. know marriage rate, value knowing median age?\neg. divorce rate given marriage rate median age marriage. know marriage rate, value knowing median age?","code":""},{"path":"lecture-05.html","id":"directed-acyclic-graphs-dag","chapter":"14 Lecture 05","heading":"14.2 Directed acyclic graphs (DAG)","text":"Directed: arrows, indicating causal implicationsAcyclic: loopsUnlike statistical models, DAGs causal implicationseg. Median age → marriage rate → divorce rate, Median age → divorce rate","code":""},{"path":"lecture-05.html","id":"example-age-marriage-divorce","chapter":"14 Lecture 05","heading":"14.3 Example: Age, marriage, divorce","text":"\\(D_{} \\sim \\text{Normal}(\\mu_{}, \\sigma)\\)\\(\\mu_{} = \\alpha + \\beta_{M}M_{} + \\beta_{}A_{}\\)(M)arriage rate()ge marriage(D)ivorce rate","code":""},{"path":"lecture-05.html","id":"priors-2","chapter":"14 Lecture 05","heading":"14.3.1 Priors","text":"Standardize z-scores\\(\\alpha\\) = expected value response values 0. since standardized response 0. Without peaking data, hard guess. standardization, much simpler.Slopes - use prior predictive simulation. Harder.","code":""},{"path":"lecture-05.html","id":"prior-predictive-simulation-2","chapter":"14 Lecture 05","heading":"14.3.2 Prior predictive simulation","text":"See Prior predictive distributions","code":""},{"path":"lecture-05.html","id":"interpretation-5","chapter":"14 Lecture 05","heading":"14.3.3 Interpretation","text":"know median age marriage, little additional value knowing marriage rate.know marriage rate, still value knowing median age marriage.don’t know median, still useful know marriage rate, since median age marriage related marriage rate. However, don’t want try influence eg. policy marriage rate, since isn’t causal divorce rate.","code":""},{"path":"lecture-05.html","id":"plotting-multivariate-posteriors","chapter":"14 Lecture 05","heading":"14.4 Plotting multivariate posteriors","text":"Regress predictor predictorsCompute predictor residualsRegress outcome residualsSide note: never analyze residuals.","code":""},{"path":"lecture-05.html","id":"posterior-predictive-checks","chapter":"14 Lecture 05","heading":"14.4.1 Posterior predictive checks","text":"Compute implied predictions observed casesAgain, regressions always well area around mean","code":""},{"path":"lecture-05.html","id":"reveal-masked-associations","chapter":"14 Lecture 05","heading":"14.5 Reveal masked associations","text":"Sometimes association outcome predictor masked another variableThis tends arise 2 predictors associated outcome opposite effects ","code":""},{"path":"lecture-05.html","id":"categorical-variables","chapter":"14 Lecture 05","heading":"14.6 Categorical variables","text":"Two approaches:Use dummy/indicator variablesUse index variablesIndex variables much better","code":""},{"path":"lecture-05.html","id":"dummy-variable","chapter":"14 Lecture 05","heading":"14.6.1 Dummy variable","text":"“Stand ” variableEg. male/female column, translated 0, 1, 0, 0, 1 0 female, 1 maleModel:\\(h_{} \\sim \\text{Normal}(\\mu_{}, \\sigma)\\)\\(\\mu_{} = \\alpha + \\beta_{M}M_{}\\)case dummy variables, alpha mean M = 0 (female) beta M change mean M = 1 (male).Result 2 intercepts = alpha alone female alpha + beta M intercept malesProblem: k categories, need k-1 dummy variables need priors . also, priors aren’t balanced alpha vs beta","code":""},{"path":"lecture-06.html","id":"lecture-06","chapter":"15 Lecture 06","heading":"15 Lecture 06","text":"","code":""},{"path":"lecture-06.html","id":"index-variable","chapter":"15 Lecture 06","heading":"15.1 Index variable","text":"(unordered, categorical variables)Starts 1, counts upSame prior can given allExtends easily > 2eg.[sex] sex prior . directly precis(m) .can directly calculate difference groups posteriors, need rerun model","code":"m <- quap(\n    alist(\n        height ~ dnorm(mu, sigma),\n        mu <- a[sex],\n        a[sex] ~ dnorm(178, 20),\n        sigma ~ dunif(0, 50)\n    ), \n    data = d\n)post <- extract.samples(m)\npost$diff <- post$a[, 1] - post$a[, 2]\nprecis(post)\n\n#       mean\n# sigma 27\n# a[1]  134\n# a[2]  142\n# diff  -7.7"},{"path":"lecture-06.html","id":"four-elemental-confounds","chapter":"15 Lecture 06","heading":"15.2 Four elemental confounds","text":"inferring relationships X Y…Confounds determined model selection, use DAGs.Arrows indicate causation, statistical information can flow either way.","code":""},{"path":"lecture-06.html","id":"notes","chapter":"15 Lecture 06","heading":"15.2.1 Notes","text":"Regression models don’t arrows like DAGs - just measure associations.can’t tell difference fork path given data alone.Remember DAGs small world constructs.","code":""},{"path":"lecture-06.html","id":"the-fork","chapter":"15 Lecture 06","heading":"15.2.2 The fork","text":"X ← Z → YZ common cause X Y. Including Z remove relationship X Y.","code":""},{"path":"lecture-06.html","id":"the-path","chapter":"15 Lecture 06","heading":"15.2.3 The path","text":"X → Z → YZ along path X Y, mediating relationship.example, influence treatment plant height, treatment influence fungus.T → F → HSince treatment influences fungus (post treatment measure), include treatment fungus, see relationship treatment height, fungus. (know fungus, treatment tell us - nothing). case, model treatment fungus tells us relationship , properly consider influence treatment need omit fungusTherefore, understanding relationship T F important, determining causality T H, need omit model.","code":""},{"path":"lecture-06.html","id":"the-collider","chapter":"15 Lecture 06","heading":"15.2.4 The collider","text":"X → Z ← YZ common result X Y. X Y independent, condition Z. Careful statistical correlations indicate causation .","code":""},{"path":"lecture-06.html","id":"steps","chapter":"15 Lecture 06","heading":"15.2.5 Steps","text":"List paths connecting X (treatment) Y (outcome)Classify path either open closed. paths open unless contain collider.Classify path backdoor/front door. Backdoor paths arrow entering X.Condition variables backdoor paths close .","code":""},{"path":"lecture-07.html","id":"lecture-07","chapter":"16 Lecture 07","heading":"16 Lecture 07","text":"","code":""},{"path":"lecture-07.html","id":"four-elemental-confounds-continued","chapter":"16 Lecture 07","heading":"16.1 Four elemental confounds (continued)","text":"","code":""},{"path":"lecture-07.html","id":"unobserved-variables","chapter":"16 Lecture 07","heading":"16.1.1 Unobserved variables","text":"Careful unmeasured variables. can create confounds, without directly measured.Eg. (Haunted DAG). G C. G → P → C, G → C. unobserved variable U creates collider: G → P ← U → C. including P allows collider distort influence G C.","code":""},{"path":"lecture-07.html","id":"over-fitting","chapter":"16 Lecture 07","heading":"16.2 Over fitting","text":"Ockham’s razor: “plurality never posited without necessity”isn’t sufficient, usually comparing models complicated fit data better, models less complicated fit worse.Two major hazards: simple, learning enough data (fitting) complex, learning much data (fitting)Goal = learn regular features sample, generalize samples","code":""},{"path":"lecture-07.html","id":"measuring-model-fit","chapter":"16 Lecture 07","heading":"16.3 Measuring model fit","text":"","code":""},{"path":"lecture-07.html","id":"r-squared","chapter":"16 Lecture 07","heading":"16.3.1 R squared","text":"Common, great\\(R_{2} = 1 - \\frac{var(residuals)}{var(outcome)}\\)“Proportion variance explained”can get R squared = 1 parameter data point - perfect fit. obviously nonsense.Therefore ’s trap picking models solely R squared increase parameters increase R squared.","code":""},{"path":"lecture-07.html","id":"obtaining-the-regular-features","chapter":"16 Lecture 07","heading":"16.4 Obtaining the regular features","text":"Regularizing priorsCross validationInformation criteria","code":""},{"path":"lecture-08.html","id":"lecture-08","chapter":"17 Lecture 08","heading":"17 Lecture 08","text":"","code":""},{"path":"lecture-08.html","id":"information-theory","chapter":"17 Lecture 08","heading":"17.1 Information theory","text":"Information: reduction uncertainty caused learning outcomeTherefore ’s scale uncertainty, information theory system deriving metric uncertaintyInformation entropy: uncertainty probability distribution average log probability event. Uncertainty distribution, “potential surprise”entropy(p) - entropy(q) trying minimize (p true, q model)","code":""},{"path":"lecture-08.html","id":"divergence","chapter":"17 Lecture 08","heading":"17.2 Divergence","text":"\\(D_{KL} = \\sum p_{} (log(p_{}) - log(q_{}))\\)Average difference log probability model q target pIt’s asymmetrical - recall W/L ratio Earth → Mars reverse. Expecting water events coming Mars reverse coming EarthSince don’t actually know “truth”, can’t use directly measure modelBut turns - don’t need truth compare two models, average log probability","code":""},{"path":"lecture-08.html","id":"estimating-divergence","chapter":"17 Lecture 08","heading":"17.3 Estimating divergence","text":"gold standard scoring models","code":""},{"path":"lecture-08.html","id":"log-pointwise-predictive-density","chapter":"17 Lecture 08","heading":"17.3.1 Log pointwise predictive density","text":"lppdPoint wise measure average probability model expects dataUsing entire posterior, measures log probabilitySumming vector lppd returns total log probability scoreLarger values better, indicating larger average accuracy","code":""},{"path":"lecture-08.html","id":"deviance","chapter":"17 Lecture 08","heading":"17.3.2 Deviance","text":"Deviance = lppd score * -2Smaller values betterNote: deviance decreases parameters, however sample deviance best model right number parameters (simulated example)","code":""},{"path":"lecture-08.html","id":"regularization","chapter":"17 Lecture 08","heading":"17.4 Regularization","text":"Must always skeptical sampleRegularization: use informative, conservative priors reduce fitting (models learn less sample).particularly important small sample sizes result, multilevel models.","code":""},{"path":"lecture-08.html","id":"cross-validation","chapter":"17 Lecture 08","heading":"17.5 Cross validation","text":"Without known sample measures, can estimate sample devianceModel samples left , average estimate samples","code":""},{"path":"lecture-08.html","id":"loo","chapter":"17 Lecture 08","heading":"17.5.1 LOO","text":"Leave one outPareto-smoothed importance sample (PSIS)loo package","code":""},{"path":"lecture-08.html","id":"information-criteria","chapter":"17 Lecture 08","heading":"17.6 Information criteria","text":"Historically: AIC, theoretical estimate KL distanceAssumptions AIC includepriors flat overwhelmed dataposterior essentially Gaussiansample size >> number parameters k","code":""},{"path":"lecture-08.html","id":"waic","chapter":"17 Lecture 08","heading":"17.6.1 WAIC","text":"Widely Applicable Information CriterionDoes assume Gaussian posterior","code":""},{"path":"lecture-08.html","id":"standard-error","chapter":"17 Lecture 08","heading":"17.6.2 Standard error","text":"Presented rethinking::compare available LOO AIC comparisons. standard error approximate standard error WAIC. Caution: small sample sizes, standard error reported underestimates uncertainty.determine two models can distinguished, use standard error difference (dSE). Using compare function, can get @dSE slot return matrix dSE pair models.","code":""},{"path":"lecture-08.html","id":"model-selection","chapter":"17 Lecture 08","heading":"17.7 Model selection","text":"Avoid model selectionScore models apply causal inference use compare competing models explain","code":""},{"path":"lecture-08.html","id":"model-comparison","chapter":"17 Lecture 08","heading":"17.8 Model comparison","text":"Model comparison causal inferenceAdd imagine unobserved confounds","code":""},{"path":"lecture-08.html","id":"example-1-model-mis-selection-using-waic","chapter":"17 Lecture 08","heading":"17.8.1 Example 1: model mis-selection using WAIC","text":"Height 0 → Height 1, Treatment → Fungus → Height 1F + T, dWAIC = 0T, dWAIC = 41intercept = 44Since f pipe T→F, including confounds modelAIC indicate causal inference, simply identifies best model according predicted sample devianceModel comparison causal inference important","code":""},{"path":"lecture-08.html","id":"example-2-primate-lifetime","chapter":"17 Lecture 08","heading":"17.8.2 Example 2: primate lifetime","text":"Body mass → lifespan, Body mass → brain size → lifespanRelationship interest: brain size lifespanM + B, WAIC = 217B, WAIC = 218M = 229Note: different parameters return similar WAIC, ’s invitation poke inside!Inspecting estimate posterior notice sign brain mass parameter flips negative positive across modelsAnother approach: since WAIC point wise can plot difference WAIC point across modelsComparing life span Y, point wise difference WAIC two models XWe see model M+B better species eg. Cebus, simple B model better species eg. GorillaIncredible","code":""},{"path":"lecture-09.html","id":"lecture-09","chapter":"18 Lecture 09","heading":"18 Lecture 09","text":"","code":""},{"path":"lecture-09.html","id":"conditioning","chapter":"18 Lecture 09","heading":"18.1 Conditioning","text":"Interaction variables otherDependence stateeg. Influence genes phenotype depends environmentApproachesUse interacting terms (simplest)Generalized linear modelsMultilevel modelsInteractions arise wherever boundary outcome space. GLMs interactions.DAG, interaction looks likegene → phenotype ← environmentBut DAGs can’t fully tell ’s interactionBefore interaction terms, variables simply independent additive terms.","code":""},{"path":"lecture-09.html","id":"example-ruggedness","chapter":"18 Lecture 09","heading":"18.1.1 Example: ruggedness","text":"“Ruggedness bad economy outside Africa, within Africa good”Reminder - constrain priors possible outcome spaceScale ruggedness 0, 1Constrain change GDP bc evidently eg GDP x 2 huge effectKeep reasonableOptionsSplit data?Run two linear regressions. means statistical criteria measure split.interested contrast slope, need use model.Add categorical variable Africa?Use alpha[id] different estimates eachThis means slope forced , difference intercepts.Relationship held constant across groups, want.Interaction\\(\\mu_{} = \\alpha_{CID[]} + \\beta_{CID[]}(r_{} - \\bar{r})\\)Slope intercept allowed vary ","code":""},{"path":"lecture-09.html","id":"example-tulips","chapter":"18 Lecture 09","heading":"18.1.2 Example: tulips","text":"Tulip blooms, varying Water Shade\\(\\mu_{} = \\alpha + \\beta_{W}W + \\beta_{S}S + \\beta_{W * S}W*S\\)beta W*S variable actually nested linear model","code":""},{"path":"lecture-09.html","id":"interpreting-interactions","chapter":"18 Lecture 09","heading":"18.2 Interpreting interactions","text":"Interpreting interactions hardThe influence predictors depends upon multiple parameters co variationInteractions symmetric within data.\nEg. effect continent depends ruggedness effect ruggedness continent\nStatistically \nneed apply outside knowledge causal information\nEg. effect continent depends ruggedness effect ruggedness continentStatistically sameWe need apply outside knowledge causal information","code":""},{"path":"lecture-09.html","id":"plotting-interactions","chapter":"18 Lecture 09","heading":"18.3 Plotting interactions","text":"Use triptychVary shade -1, 0, 1Plot bloom response water x","code":""},{"path":"lecture-09.html","id":"higher-order-interactions","chapter":"18 Lecture 09","heading":"18.3.1 Higher order interactions","text":"Caution: hard interpret, hard estimate","code":""},{"path":"lecture-10.html","id":"lecture-10","chapter":"19 Lecture 10","heading":"19 Lecture 10","text":"","code":""},{"path":"lecture-10.html","id":"markov-chain-monte-carlo","chapter":"19 Lecture 10","heading":"19.1 Markov Chain Monte Carlo","text":"Reminder: Bayesian inference calculating posterior. Bayesian ≠ Markov Chains4 ways compute posteriorAnalytical approach (mostly impossible)Grid approximation (intensive)Quadratic approximate (limited)MCMC (intensive)Advantages MCMCYou don’t know posterior yet can still visit part proportion ’s relative probability“Sample distribution don’t know”","code":""},{"path":"lecture-10.html","id":"metropolis-algorithm","chapter":"19 Lecture 10","heading":"19.1.1 Metropolis algorithm","text":"Loop iterationsRecord locationGenerate neighbor location proposalsMove based frequencyConverges long run, can used long proposals symmetric","code":""},{"path":"lecture-10.html","id":"metropolis-hastings","chapter":"19 Lecture 10","heading":"19.1.2 Metropolis Hastings","text":"Improvement Metropolis, require proposals symmetrical","code":""},{"path":"lecture-10.html","id":"gibbs-sampling","chapter":"19 Lecture 10","heading":"19.1.3 Gibbs sampling","text":"efficient version MH","code":""},{"path":"lecture-10.html","id":"hamiltonian-monte-carlo","chapter":"19 Lecture 10","heading":"19.1.4 Hamiltonian Monte Carlo","text":"Markov Chain: memory. Probability solely depends current state, past state. storage.Monte Carlo: Random simulation (eg Monaco casino)MCMC numerical technique solve posterior, several advantages Metropolis GibbsMetropolis Gibbs use optimization optimization good strategy high dimensions (see concentration measure)Hamiltonian Monte Carlo uses gradient avoid guess + check Metropolis GibbsEspecially high dimensional space, acceptance rate decreases methods take timeHamiltonian Monte Carlo:Uses physics simulation representing parameter state particleFlicks particle around friction less log-posterior surfaceFollows curvature surface, doesn’t get stuckUses random direction random speedSlows climbs, speeds dropsThis much computationally intensive, requires less steps, much fewer rejectionsIt’s also easier determine MCMC failed","code":""},{"path":"lecture-10.html","id":"tuning-mcmc","chapter":"19 Lecture 10","heading":"19.1.5 Tuning MCMC","text":"Step size: time simulation run. Increase step size = increase efficiency overestimates curvatureU Turn risk solved NUTS (U Turn Sampler)Warm phase - finding step size maximize acceptance rate. Default = good (half number samples)Runs directions gives uncorrelated samples. need pick leap frog steps","code":""},{"path":"lecture-10.html","id":"stan","chapter":"19 Lecture 10","heading":"19.1.6 Stan","text":"Stan uses NUTS","code":""},{"path":"lecture-10.html","id":"ulam","chapter":"19 Lecture 10","heading":"19.1.7 ulam","text":"Create list data needulam formulas quapulam translates formulas StanBuilds NUTS samplerSampler runsReturns posterior","code":""},{"path":"lecture-10.html","id":"diagnosis","chapter":"19 Lecture 10","heading":"19.1.8 Diagnosis","text":"Neff: number effective samples. Can greater number samples Markov Chan. Effective autocorrelationRhat: Convergence diagnostic. 1 good. Ratio variance within vs ratio variance across chains.“Typically computational problem, often ’s problem model”","code":""},{"path":"lecture-10.html","id":"checking-the-chain","chapter":"19 Lecture 10","heading":"19.1.9 Checking the chain","text":"TODO: p283","code":""},{"path":"lecture-11.html","id":"lecture-11","chapter":"20 Lecture 11","heading":"20 Lecture 11","text":"Flat distributions highest entropy many ways can realized","code":""},{"path":"lecture-11.html","id":"maximum-entropy","chapter":"20 Lecture 11","heading":"20.1 Maximum entropy","text":"Distribution largest entropy distribution consistent stated assumptionsFor parameters: helps understand priors. constraints make prior reasonable?observations: way understand likelihoodSolving posterior = getting distribution flat possible consistent data within constraintsHighest entropy answer = distance truth smaller","code":""},{"path":"lecture-11.html","id":"distributions","chapter":"20 Lecture 11","heading":"20.1.1 Distributions","text":"","code":""},{"path":"lecture-11.html","id":"generalized-linear-model","chapter":"20 Lecture 11","heading":"20.2 Generalized linear model","text":"Connect linear model outcome variablePick outcome distributionModel parameter using links linear modelsCompute posteriorExtends multivariate relationships non-linear responsesBuilding blocks multilevel modelsVery common widely applicable","code":""},{"path":"lecture-11.html","id":"picking-a-distribution","chapter":"20 Lecture 11","heading":"20.2.1 Picking a distribution","text":"Mostly exponential family maximum entropy interpretations arise natural processesDo pick looking histogram - way aggregate histogram outcomes unconditional something else going relevant distributionJust use principles.Exponential: non negative real. Lambda rate mean 1/lambdaBinomial: count events emerging exponential distributionPoisson: count events, low rateGamma: sum exponentialNormal: gamma large meanTide prediction machine - complex “parameters” bottom. “Can understand models resist urge understand parameters”","code":""},{"path":"lecture-11.html","id":"types-of-outcomes","chapter":"20 Lecture 11","heading":"20.2.2 Types of outcomes","text":"Distances durationsExponentialGammaCountsPoissonBinomialMultinomialGeometryMonstersRanks, ordered categoriesMixturesBeta binomialGamma-poissonEtc","code":""},{"path":"lecture-11.html","id":"model-parameters-with-a-link-function","chapter":"20 Lecture 11","heading":"20.2.3 Model parameters with a link function","text":"Yi ~ Normal(mu, sigma)mu ~ alpha + beta * XLinear regressions linear regressions scientific units outcome variable parameters meanAnother example - binomialCount: Y ~ Binomial(N, p) (unit count something)Probability: P ? alpha + beta * X (unit less)need functionf(p) = alpha + beta * X","code":""},{"path":"lecture-11.html","id":"binomial-distribution","chapter":"20 Lecture 11","heading":"20.3 Binomial distribution","text":"Counts specific event n possible trialsmin: 0, max: nConstant expected valueMaxent: binomialy ~ Binomial(n, p)count successes distribution binomially n trials p probability success","code":""},{"path":"lecture-11.html","id":"link","chapter":"20 Lecture 11","heading":"20.3.1 Link","text":"Goal map linear model [0, 1]y ~ Binomial(n, p)logit(p) = alpha + beta * xlogit log oddsGiven link function, priors logit scale shape priors probability scaleProsocial monkey exampley ~ Binomial(n, p)logit(p) = alpha[actor] + beta[treatment] * Treatmentprecis(m)[1] … [7] different chimps, posterior means logit scaleb[1] … b[4] b treatments, average log odd deviations chimp handedness consideredInvestigatingextract samplesinv_logit transform probability scoreprecisIt’s really hard understand just using precis output thereforePlot outcome scale link = posterior predictive samplingControlling handedness isn’t backdoor criterion. Handedness = noise, controlling gives us precise criteria","code":""},{"path":"lecture-12.html","id":"lecture-12","chapter":"21 Lecture 12","heading":"21 Lecture 12","text":"","code":""},{"path":"lecture-12.html","id":"relative-and-absolute-effects","chapter":"21 Lecture 12","heading":"21.1 Relative and absolute effects","text":"Effect sizes two waysRelative effect scale: parameters relative differences effectAbsolute effect scale: used predictionsProportional odds eg treatment 4 2post ← extract.samplesmean0.9 = 90% previous oddsTherefore 2→4 expects reduction odds 10%disregards base rateRisk relative effects like proportional odds don’t consider absolute likelihoodRelative shark vs absolute deer - need ","code":""},{"path":"lecture-12.html","id":"logistic-regression","chapter":"21 Lecture 12","heading":"21.2 Logistic regression","text":"0, 1 trials (Bernoulli trials)Aggregate binomial: aggregated 0, 1 counts categoryExample - UC Berkeley 1970sGender → Department → Acceptance, Gender → AcceptanceRecall: regressions literal exactly question askingModel 1Acceptance ~ Binomial(N, p)logit(p) = alpha [gender]Statistical question: average probabilities admission gender across departments?Causal question: total causal influence gender?’s asking total effect, discrimination effectTherefore, paths play (Gender → Department → Acceptance, Gender → Acceptance)Model 2Close backdoor.Acceptance ~ Binomial(N, p)logit(p) = alpha [gender] + beta departmentStatistical question: average difference probability admission genders within department?Causal question: direct influence gender?equally valid, different questions.","code":""},{"path":"lecture-12.html","id":"simpsons-paradox","chapter":"21 Lecture 12","heading":"21.3 Simpsons paradox","text":"Flip covariates sign adding/removing variable","code":""},{"path":"lecture-13.html","id":"lecture-13","chapter":"22 Lecture 13","heading":"22 Lecture 13","text":"","code":""},{"path":"lecture-13.html","id":"binomial","chapter":"22 Lecture 13","heading":"22.1 Binomial","text":"Outcome count zero known upper bound","code":""},{"path":"lecture-13.html","id":"poisson","chapter":"22 Lecture 13","heading":"22.2 Poisson","text":"Binomial events N trials large/unknown probability event smallPoisson varied exposure/offset","code":""},{"path":"lecture-13.html","id":"generalized-linear-madness","chapter":"22 Lecture 13","heading":"22.3 Generalized linear madness","text":"Example oceanic tool complexityModeled poisson linkThe model outcomes terrible - though fit data, intercepts don’t pass originWouldn’t expect zero population = zero tools?Solution scientific model","code":""},{"path":"lecture-13.html","id":"scientific-model","chapter":"22 Lecture 13","heading":"22.4 Scientific model","text":"relationship can thought change tools per unit timeChange time = alpha P ^ betaAlpha: innovation rate, P: population per person = person change inventing somethingBeta: diminishing returns, saturation effect, “someone else invent ”TODO: read , highlight , etcThe resulting model using function based scientific model perfect, meanings clearer, intercept actually goes 0This ad hoc function, link","code":""},{"path":"lecture-13.html","id":"survival-analysis","chapter":"22 Lecture 13","heading":"22.5 Survival analysis","text":"Estimate rates modeling time--eventCan’t ignore censored casesLeft censored: don’t time startedRight censored: something else cut endExample catsTime adoption observed adoptions simplest, exponential functionFor censored catsuse cumulative distributiontake complementcalculate probability event yet","code":""},{"path":"lecture-13.html","id":"monsters","chapter":"22 Lecture 13","heading":"22.6 Monsters","text":"Specialized complex distributionseg. ordered categories, ranks","code":""},{"path":"lecture-13.html","id":"mixtures","chapter":"22 Lecture 13","heading":"22.7 Mixtures","text":"Blends stochastic processeseg. varying means, probabilities, rateseg. zero-inflation, hurdlesExample monksNumber manuscripts per dayCan infer number days get drunk?Drunkenness hidden stateThere probability drink work, within work, probability produce 0 1+ manuscripts","code":""},{"path":"lecture-14.html","id":"lecture-14","chapter":"23 Lecture 14","heading":"23 Lecture 14","text":"","code":""},{"path":"lecture-14.html","id":"ordered-categories","chapter":"23 Lecture 14","heading":"23.1 Ordered categories","text":"Discrete outcomes defined order, defined min maxBut “distances” categories unknown metricHard model isn’t continuous bounded min-max count.Solution: use log-cumulative-odds link probability modelTODO fill","code":""},{"path":"lecture-15.html","id":"lecture-15","chapter":"24 Lecture 15","heading":"24 Lecture 15","text":"","code":""},{"path":"lecture-15.html","id":"multilevel-models","chapter":"24 Lecture 15","heading":"24.1 Multilevel models","text":"models forget things data move one case otherFixed effects: model forgets everything clusters. information passed clusters.Multilevel model: remember pool informationDefault multilevel modelingnearly every case improved multilevel modelingif , ’s just goodWhy use multilevel modeling?deal clustering data (eg. classroom within schools, students within classrooms, …)handles imbalance samplinghandles pseudo replications","code":""},{"path":"lecture-15.html","id":"varying-intercepts","chapter":"24 Lecture 15","heading":"24.2 Varying intercepts","text":"Example tadpoleOutcome: number survivingTadpoles tanks different densitiesModel 1 index tankModel 2 multilevel varying interceptsUntitledVarying intercepts = random intercepts“Random” “varying” unclearDistinction varying intercepts prior learns dataAdaptive regularizationFrom example, survival across tanks distribution. distribution prior tank. distribution needs prior","code":""},{"path":"lecture-15.html","id":"shrinkage","chapter":"24 Lecture 15","heading":"24.3 Shrinkage","text":"Model doesn’t retrodict samples exactlyShrinkage towards population mean caused regularizationLarger variation = shrinkageLess data per cluster = shrinkageIncreased difference mean = shrinkage","code":""},{"path":"lecture-15.html","id":"pooling","chapter":"24 Lecture 15","heading":"24.4 Pooling","text":"varying effects accurate fixed effects?Grand mean - maximum fitting - complete poolingFixed effects - maximum fitting - poolingVarying effects - adaptive regularization - partial pooling","code":""},{"path":"lecture-16.html","id":"lecture-16","chapter":"25 Lecture 16","heading":"25 Lecture 16","text":"","code":""},{"path":"lecture-16.html","id":"multiple-clusters","chapter":"25 Lecture 16","heading":"25.1 Multiple clusters","text":"Example: chimpanzees * block“cross classified”Add actor block intercepts - alpha actor, gamma block, beta treatmentThink offset\\(\\alpha_{j} \\sim \\text{Normal}(\\bar{\\alpha}, \\sigma_{alpha})\\)\\(\\gamma{j} \\sim \\text{Normal}(0, \\sigma_{gamma})\\)\\(\\bar{\\alpha} \\sim \\text{Normal}(0, 1.5)\\)example, sigma actors shows variation actors, sigma blocks quite small. include blocks ? sample predictions quite similar, block parameters aggressively regularized.Random effects many definitions. statistical things used regularize inference. Doesn’t necessarily things “fixed experimenter”.","code":""},{"path":"lecture-16.html","id":"divergent-transitions","chapter":"25 Lecture 16","heading":"25.2 Divergent transitions","text":"Tells something numerically inefficient model. Solution = switch different ways writing model.transition sample path. Since real physics energy conserved, energy isn’t conserved physics simulation within HMM - something went wrong.HMC thankfully reports warnings, whereas Gibbs, Metropolis, etc (since physics simulation)","code":""},{"path":"lecture-16.html","id":"solutions-for-divergent-transitions","chapter":"25 Lecture 16","heading":"25.2.1 Solutions for divergent transitions","text":"Increase adapt_delta argument. results better step size adaption , consequence, slower exploration.Reparameterize modelReparameterizing model takes “centered” model transforms “non-centered” model. Even though mathematically , helps sampling.\nEg. funnel → Gaussian hillResult z-score centered, “non-centered”, much easier sample yields effective samples. Chains also run faster.","code":""},{"path":"lecture-16.html","id":"posterior-predictions-with-multilevel-models","chapter":"25 Lecture 16","heading":"25.3 Posterior predictions with multilevel models","text":"","code":""},{"path":"lecture-16.html","id":"same-clusters","chapter":"25 Lecture 16","heading":"25.3.1 Same clusters","text":"Proceed usualPush samples back model (eg. link sim)","code":""},{"path":"lecture-16.html","id":"new-clusters","chapter":"25 Lecture 16","heading":"25.3.2 New clusters","text":"“New chimpanzees”new population, individual (alpha example) parameters irrelevantThree approaches:“Average” actor - average chimp statistically population mean (alpha bar).\nAlpha = alpha bar.\nReplace varying intercept samples 0s actors average intercept row.\nUse link directly simulate.\nAlpha = alpha bar.Replace varying intercept samples 0s actors average intercept row.Use link directly simulate.Marginal actor - sample number statistical actors average distribution.\nExtract samples sigma actor\nSimulate new varying intercepts\nUse simulated intercepts simulate predictions\nExtract samples sigma actorSimulate new varying interceptsUse simulated intercepts simulate predictionsShow samples actors posterior\nSample plot individuals simulated posterior\nSample plot individuals simulated posteriorNote effects limits reduced ceiling floor effects generalized linear models. Eg near 0-1 bounds, effects reduced.","code":""},{"path":"lecture-17.html","id":"lecture-17","chapter":"26 Lecture 17","heading":"26 Lecture 17","text":"","code":""},{"path":"lecture-17.html","id":"varying-slopes","chapter":"26 Lecture 17","heading":"26.1 Varying slopes","text":"Slopes another feature responseMaking parameter varying effectSplit vector parameters clusterDefine population clustersAny batch parameters exchangeable index values can (“probably ”) pooled. Exchangeable = unordered labels.treat slopes distinct varying effect, even better - relate intercepts slopes directly. Since intercepts slopes related population/math/geometry, features units correlation structure.","code":""},{"path":"lecture-17.html","id":"example---cafes","chapter":"26 Lecture 17","heading":"26.1.1 Example - cafes","text":"Cafe visits morning afternoon, intercepts: average morning wait, slopes: avg difference afternoon morning.slopes intercepts related? Yes. pooling across parameters.prior 2 dimensional Gaussian. vector means (average intercept, average slope) variance-covariance matrix.","code":""},{"path":"lecture-17.html","id":"variance-covariance-matrix","chapter":"26 Lecture 17","heading":"26.2 Variance covariance matrix","text":"[var covar]","code":""},{"path":"lecture-17.html","id":"varying-slopes-model","chapter":"26 Lecture 17","heading":"26.3 Varying slopes model","text":"\\(W_{} \\sim \\text{Normal}(\\mu_{}, \\sigma)\\)\\(\\mu_{} = \\alpha_{\\text{cafe}[]} + \\beta_{\\text{cafe}[}*A_{}\\)[alpha cafe] ~ MVNormal([alpha / beta, S])Mu represents varying intercepts + varying slopes. = afternoon/notMultivariate prior: cafe, ’s pair parameters alpha beta, distributed 2 dimensional normal averages alpha beta, S covariance matrix.R ~ LKJcorr(2)can’t assign priors independently. 1 dimensional correlations vary -1 1, increasing n dimensions, correlation remains restricted within limits. Therefore, 1 really big, necessarily smaller.LKJcorr one variable eta. eta defines concentrated identity matrix. density -1 1. eta = 1 represents pretty much uniform density. eta > 1 concentration around 0, skeptical extremes.","code":""},{"path":"lecture-17.html","id":"multidimensional-shrinkage","chapter":"26 Lecture 17","heading":"26.4 Multidimensional shrinkage","text":"Joint distribution varying effects pools information across slopes intercepts. Correlation induces shrinkages across dimensions, increasing accuracy.","code":""},{"path":"lecture-17.html","id":"example---prosocial-chimps-many-clusters","chapter":"26 Lecture 17","heading":"26.4.1 Example - prosocial chimps, many clusters","text":"4 treatments: partner present/absent, side table L/R. Can vary actor block.\\(L_{} \\sim \\text{Binomial}(1, p_{})\\)\\(\\text{logit}(p_{}) = \\gamma_{\\text{treatment}} + \\alpha_{\\text{actor, treatment}} + \\beta_{\\text{block, treatment}}\\)Mean effect treatments, actor treatment, block treatment.Alpha actor, treatment matrix alpha deviation mean actor treatmentBeta block, treatment matrix beta deviation mean block treatmentHow many parameters ? 7 individuals * 4 treatments + 6 blocks * 4 treatments + 6 correlations + 4 sigmas = 76 parametersWith shrinkage, number effective parameters much lower.","code":""},{"path":"lecture-17.html","id":"divergences","chapter":"26 Lecture 17","heading":"26.4.2 Divergences","text":"divergences (common models), need use non-centered versions.Simpler uni variate models, since need factor parameters prior linear model. factor correlation matrix?Cholesky factor","code":""},{"path":"lecture-18.html","id":"lecture-18","chapter":"27 Lecture 18","heading":"27 Lecture 18","text":"","code":""},{"path":"lecture-18.html","id":"centeringnon-centered","chapter":"27 Lecture 18","heading":"27.1 Centering/non-centered","text":"always one better situationsuse reparameterized version modeling inefficientcheck number effective parameters increases ","code":""},{"path":"lecture-18.html","id":"multilevel-horoscopes","chapter":"27 Lecture 18","heading":"27.2 Multilevel horoscopes","text":"Think causal model firstDraw DAGBegin “empty” model varying intercepts relevant clustersStandardize predictorsUse regularizing priors + prior predictive simulationAdd predictors vary slopesPossibly drop varying effects tiny sigmasConsider two kinds posterior prediction\nunits: “happened data?”\nnew units: “might expect new units?”\nunits: “happened data?”new units: “might expect new units?”","code":""},{"path":"lecture-18.html","id":"other-covariance-structures","chapter":"27 Lecture 18","heading":"27.3 Other covariance structures","text":"Instrumental variablesSocial relations model (covariance behaviour among nodes)Factor analysisAnimal model, heritability phenotypePhylogenetic regressionsSpatial autocorrelation","code":""},{"path":"lecture-18.html","id":"instrumental-variables","chapter":"27 Lecture 18","heading":"27.4 Instrumental variables","text":"Recall: adding variables can introduce confounds, use\nback door criterion determine include \nmay introduce confounds.Sometimes back door says way shut .example, E->W, U->E, U->W, U unobserved confoundU confound, U unmeasuredSolution: instrument influences exposure outcomeQ->E->W, U->E, U->WWhere Q case, quarter year someone born . E \neducation, W wage. Quarter influences 1) start school 2) \nallowed quit school, since Q calendar year biological age.example, confounds likely related quarter, therefore\nquarter valid instrument.makes E colliderU generates correlation E, WQ tells us something deviation E, separate E, W correlation UInstrumental variables also called “natural experiments” biology.can limited. depend greatly DAG, hard find\nplausible instrument. Instruments weak effects useful.","code":""},{"path":"lecture-18.html","id":"social-relationoso-models","chapter":"27 Lecture 18","heading":"27.5 Social relationoso models","text":"Example: giving receiving ratesHow disentangle dyadic offsets, general giving, general receiving rates, etc.Use multiple covariance matrices. 2x2 giving + receiving, 2x2 dij + dji (paired\ndyad offsets). dij != dji one may give .","code":""},{"path":"lecture-19.html","id":"lecture-19","chapter":"28 Lecture 19","heading":"28 Lecture 19","text":"","code":""},{"path":"lecture-19.html","id":"gaussian-process-regression-continuous-categories","chapter":"28 Lecture 19","heading":"28.1 Gaussian process regression: continuous categories","text":"Traditional clusters discrete, unordered. Every category equally different\nothers.take, example, income. reason every increase \ndecrease 1 dollar equivalently weighted. examples include\nage, phylogenetic distance, social network distance, etc. \nobvious cut points, know similar values similar.Gaussian process regression considers infinite numbers categories. (Need pooling)","code":""},{"path":"lecture-19.html","id":"example-spatial-autocorrelation-of-oceanic-tool-complexity-and-population-size","chapter":"28 Lecture 19","heading":"28.1.1 Example: spatial autocorrelation of oceanic tool complexity and population size","text":"Construct distance matrix distances islands. improve\nconsidering sailing distance example.Recall model: tool complexity distributed poisson. Innovation rate,\npopulation size, rate diminishing returns loss rate.add Gaussian process, add factor k front population\nk=0, exactly expected model\nk=-0.5, 60% (exp(-0.5) = 0.6)\nk=-0.5, 130% (exp(0.25) = 1.3)\nk=0, exactly expected modelk=-0.5, 60% (exp(-0.5) = 0.6)k=-0.5, 130% (exp(0.25) = 1.3)Gaussian process prior: multivariate Gaussian 10x10 covariance (distance case)Modelling covariance: covariance islands j max covariance\nmultiplied rate decline distance squared distance, added \nvariance within self (also called jitter function multiple observations \nisland different covariance). Gaussian, bell curve.Linear (Dij) vs squared (Dij^2). Linear assumes rate decline\nfastest start. Usually true assume squared term.,Result lower covariance predicted much slower decay.","code":""},{"path":"lecture-19.html","id":"phylogenetic-relationship","chapter":"28 Lecture 19","heading":"28.2 Phylogenetic relationship","text":"Phylogenetic relationships sort proxy unobserved confounds.ways incorporating phylogenetic information GLM:Brownian motion model (PGLS)Ornstein-Uhlenbeck (OU) processes…use covariance matrix represent phylogeny principle \nGaussian process regression","code":""},{"path":"lecture-19.html","id":"example-primates","chapter":"28 Lecture 19","heading":"28.2.1 Example: primates","text":"Body size -> Group size, Body size -> Brain size, Brain size -> Group sizeG ~ MVNormal(mu, sigma)mu = alpha + beta body + beta body\nS = sigma ^ 2 * II identity matrix, therefore simplest version (covariance), just diagonals sigma squaredBrownian motion oldest conservative assumption. decline covariance linear since date species diverged.\nAlternatively, use non-linear relationship covariance\ndistance Gaussian process consider infinite\ndifferent functions.","code":""},{"path":"lecture-20.html","id":"lecture-20","chapter":"29 Lecture 20","heading":"29 Lecture 20","text":"","code":""},{"path":"lecture-20.html","id":"measurement-error","chapter":"29 Lecture 20","heading":"29.1 Measurement error","text":"always error measurement. Sigma models captures\nerror outcomes. error predictors? error isn’t constant?","code":""},{"path":"lecture-20.html","id":"example-waffle-divorces","chapter":"29 Lecture 20","heading":"29.1.1 Example: waffle divorces","text":"Error outcome (divorce rate) heterogeneous smaller states\nlarger error.-> M -> D -> D_obs, -> D, N -> D_obsD true, unobserved divorce rate. D_obs observed divorce rate.Approach:Treat true divorce rate unknown parameterThe observed rate sampled Gaussian distributionD_obs ~ Normal (D true, D standard error)gives us shrinkage considering standard error well \nmodeled relationship divorce rate median age.","code":""},{"path":"lecture-20.html","id":"error-on-predictor","chapter":"29 Lecture 20","heading":"29.2 Error on predictor","text":"","code":""},{"path":"lecture-20.html","id":"example-marriage-rate","chapter":"29 Lecture 20","heading":"29.2.1 Example: marriage rate","text":"Consider error marriage rate:Likelihood observed rate, stateM_obs ~ Normal (M true, M standard error)M true ~ Normal (0, 1)Note: simplification misses M associated.","code":""},{"path":"lecture-20.html","id":"sources-of-error","chapter":"29 Lecture 20","heading":"29.2.2 Sources of error","text":"Data come uncertain procedure often discard uncertainty\nget analysis/model.example, taking series measurements modeling group-level\naverages instead raw value. toss away information variation,\nsample size, etc.","code":""},{"path":"lecture-20.html","id":"missing-data","chapter":"29 Lecture 20","heading":"29.3 Missing data","text":"Usual approach consider complete cases. either\nactively done user silently, software eg lm. discards lot\ninformation harmless.Options include multiple imputation (frequentist approach based ..),\nBayesian imputation. : replace missing values mean. model\ntakes true value.Missing values can confound.","code":""},{"path":"lecture-20.html","id":"types-of-missingness","chapter":"29 Lecture 20","heading":"29.3.1 Types of missingness","text":"","code":""},{"path":"lecture-20.html","id":"mcar-missing-completely-at-random","chapter":"29 Lecture 20","heading":"29.3.1.1 MCAR: Missing completely at random","text":"response completely independent missingness,\nmissingness variable confound need \nimputation, add precision.example, brain neocortex proportion. missing values,\ncan consider B obs = function(B, R_B) R_B missingness B.\ntwo paths B -> K, direct path B obs -> B -> K \nindirect path B obs -> B <- U -> M -> K. back door.ever really get completely random missingness?","code":""},{"path":"lecture-20.html","id":"mar-missing-at-random","chapter":"29 Lecture 20","heading":"29.3.1.2 MAR: Missing at random","text":"Missingness likely specific values another variable.example, brain neocortex proportion body mass M. Species \nlarger smaller likely missing values. Maybe ’s \nsize related observation bias. Now, M -> R_B creates backdoor. close\nbackdoor, can condition M. missingness R_B value ignorable,\nmust imputation.","code":""},{"path":"lecture-20.html","id":"mnar-missing-not-at-random","chapter":"29 Lecture 20","heading":"29.3.1.3 MNAR: Missing not at random","text":"Missingness likely specific values response (\nspecific values unobserved variable influences response\nmissingness).example, brain neocortex proportion higher monkeys similar\nhumans likely study monkeys. missing values\nB species lower B. Alternatively, unobserved variable\nmay influence missingness B. can’t close backdoor.\nthing can model error.","code":""},{"path":"lecture-20.html","id":"imputing","chapter":"29 Lecture 20","heading":"29.3.2 Imputing","text":"","code":""},{"path":"lecture-20.html","id":"example-milk-energy-mar","chapter":"29 Lecture 20","heading":"29.3.2.1 Example: milk energy MAR","text":"unobserved value becomes parameter. values imputed, estimating\nobserved values model structure.result increased precision , example, doesn’t consider \nrelationship B body mass. Solution use multivariate normal\nimpute given relationship B body mass.","code":""},{"path":"references.html","id":"references","chapter":"30 References","heading":"30 References","text":"","code":""}]
